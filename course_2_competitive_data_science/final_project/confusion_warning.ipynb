{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The scorer in `GridSearchCV` can be utterly confusing. \n",
    "\n",
    "The scores returned by `GridSearchCV` are negative for scores as `GridSearchCV` by convention tries to maximize its score. This means that loss functions like MSE have to be negated.\n",
    "\n",
    "See [here](https://stackoverflow.com/questions/21050110/sklearn-gridsearchcv-with-pipeline)\n",
    "and [here](https://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error).\n",
    "\n",
    "For clarity let's run an experiment.\n",
    "We will fit a linear regression classifier on data that intersects (0, 1) rather than origo.\n",
    "Thus we know that including the intercept is better than not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Returns the root mean squared error of the predictions\n",
    "    \n",
    "    The root mean squared error is defined by:\n",
    "    $\\sqrt {\\frac {\\sum _{t=1}^{T}({\\hat {y}}_{t}-y_{t})^{2}}{T}}$\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : array, shape (n_samples,)\n",
    "        The correct prediction\n",
    "    prediction : array, shape (n_samples,)\n",
    "        The predictions\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rmse : float\n",
    "        The root mean squared error\n",
    "    \"\"\"\n",
    "    return np.sqrt(metrics.mean_squared_error(ground_truth, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scorer = metrics.make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_train = pd.DataFrame(np.array(range(100)))\n",
    "example_target = pd.DataFrame(np.array(range(1, 101)))\n",
    "\n",
    "parameters = {'fit_intercept': (True, False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scorer_greater = metrics.make_scorer(rmse, greater_is_better=True)\n",
    "grid_lin_greater = model_selection.GridSearchCV(LinearRegression(), \n",
    "                                                parameters,\n",
    "                                                scoring=rmse_scorer_greater,\n",
    "                                                return_train_score=False)\n",
    "grid_lin_greater.fit(example_train, example_target)\n",
    "greater_best_model = grid_lin_greater.best_estimator_\n",
    "greater_best_score = grid_lin_greater.best_score_\n",
    "greater_mean_score = grid_lin_greater.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scorer_lesser = metrics.make_scorer(rmse, greater_is_better=False)\n",
    "grid_lin_lesser = model_selection.GridSearchCV(LinearRegression(), \n",
    "                                               parameters,\n",
    "                                               scoring=rmse_scorer_lesser,\n",
    "                                               return_train_score=False)\n",
    "grid_lin_lesser.fit(example_train, example_target)\n",
    "lesser_best_model = grid_lin_lesser.best_estimator_\n",
    "lesser_best_score = grid_lin_lesser.best_score_\n",
    "lesser_mean_score = grid_lin_lesser.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Greater is best returns best a score of {greater_best_score:.2f} '\n",
    "      f'of {greater_mean_score} '\n",
    "      f'with the model\\n{greater_best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lesser is best returns best a score of {lesser_best_score:.2f} '\n",
    "      f'of {lesser_mean_score} '\n",
    "      f'with the model\\n{lesser_best_model}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
