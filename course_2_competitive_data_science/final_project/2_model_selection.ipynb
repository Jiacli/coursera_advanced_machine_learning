{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will do the model selection and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "\n",
    "* [1 Loading the data](#1-Loading-the-data)\n",
    "* [2 Mean prediction](#2-Mean-prediction)\n",
    "* [3 Make loss function](#3-Make-loss-function)\n",
    "* [4 Validation generation](#4-Validation-generation)\n",
    "* [5 Hyperparameter optimization](#5-Hyperparameter-optimization)\n",
    "    * [5.1 Linear Regression](#5.1-Linear-Regression)\n",
    "    * [5.2 Lasso](#5.2-Lasso)\n",
    "    * [5.3 Ridge](#5.3-Ridge)\n",
    "    * [5.4 KNN](#5.4-KNN)\n",
    "    * [5.5 Extremely Randomized Trees](#5.5-Extremely-Randomized-Trees)\n",
    "    * [5.6 Gradient Boosting Decision Tree](#5.6-Gradient-Boosting-Decision-Tree)\n",
    "    * [5.7 Neural Network](#5.7-Neural-Network)\n",
    "* [6 Ensembling](#6-Ensembling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Possible more fixes for non-determinism\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-366542480\n",
    "import os\n",
    "from keras import backend as k\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "# Limit operation to 1 thread for deterministic results.\n",
    "# NOTE: This will slow down the operation\n",
    "# session_conf = tf.ConfigProto(\n",
    "#     intra_op_parallelism_threads=1,\n",
    "#     inter_op_parallelism_threads=1)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "\n",
    "k.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = Path('.').absolute().joinpath('generated_data')\n",
    "\n",
    "target_name = 'item_cnt_month'\n",
    "dt_data = pd.read_hdf(generated_data.joinpath('dt_data.hdf'), key='dt_data')\n",
    "non_dt_data = pd.read_hdf(generated_data.joinpath('non_dt_data.hdf'), key='non_dt_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dt = dt_data.loc[dt_data.loc[:, 'date_block_num'] < dt_data.loc[:, 'date_block_num'].max()].copy()\n",
    "target = train_dt.loc[:, [target_name]]\n",
    "train_dt.drop(target_name, axis=1, inplace=True)\n",
    "test_dt = dt_data.loc[dt_data.loc[:, 'date_block_num'] == dt_data.loc[:, 'date_block_num'].max()].copy()\n",
    "test_dt.drop(target_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We have normalized 'date_block_num' in non_dt_data, so we should be careful to use this as a filter\n",
    "#       as this is now a float value\n",
    "#       Instead we use the index values\n",
    "train_non_dt = non_dt_data.iloc[:train_dt.shape[0]].copy()\n",
    "train_non_dt.drop(target_name, axis=1, inplace=True)\n",
    "test_non_dt = non_dt_data.iloc[train_dt.shape[0]:].copy()\n",
    "test_non_dt.drop(target_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dt_data\n",
    "del non_dt_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Mean prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start we should do some rudimental model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `sample_submission.csv.gz` contains the constant prediction `0.5`, and gives the score `1.23646` against the kaggle site.\n",
    "\n",
    "Furthermore we know that the optimal prediction for a constant is a target mean (of the ground truth).\n",
    "Nevertheless, we can probe the leaderboard with target mean of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction = test_dt.loc[:, ['ID']]\n",
    "mean_prediction.loc[:, 'item_cnt_month'] = target.loc[:, 'item_cnt_month'].mean()\n",
    "\n",
    "# Set ID as index\n",
    "mean_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "mean_prediction.to_csv(generated_data.joinpath('mean_prediction.csv'))\n",
    "mean_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean prediction gave a score of `1.21744`, which is worse than our initial submission.\n",
    "This means that on average, predictions with lower values are preferred over predicitions with higher values.\n",
    "\n",
    "We can in fact use this to probe the leaderboard. As we know that the constant target mean of the ground thruth gives the lowest score, we can check whether `0.2` is a minimum (at least of the public test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_02 = mean_prediction.copy()\n",
    "prediction_02.loc[:, 'item_cnt_month'] = 0.2\n",
    "prediction_02.to_csv(generated_data.joinpath('prediction_02.csv'))\n",
    "prediction_02.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This degraded the score to `1.22032`.\n",
    "\n",
    "We could continue to probe the leader board like this to find the minimum to get a direction on what our prediction mean should be close to. However, we must bear in mind that we are only probing the public part of the test set, so we must use this technique with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Make loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the RSME is not available as a loss function out of the box, so we define it ourselves.\n",
    "This also give us the oppurtunity to clip the predictions to $[0,20]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_clip(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Returns the root mean squared error of the predictions\n",
    "    \n",
    "    The root mean squared error is defined by:\n",
    "    $\\sqrt {\\frac {\\sum _{t=1}^{T}({\\hat {y}}_{t}-y_{t})^{2}}{T}}$\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This version clips the predictions to [0, 20]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : array, shape (n_samples,)\n",
    "        The correct prediction\n",
    "    prediction : array, shape (n_samples,)\n",
    "        The predictions\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rmse : float\n",
    "        The root mean squared error\n",
    "    \"\"\"\n",
    "    return np.sqrt(metrics.mean_squared_error(ground_truth, predictions.clip(0, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Validation generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to predict for the next month, we know that train-test is split by time (we would like to predict for month $34$).\n",
    "\n",
    "In addition, from [0_EDA_raw_data.ipynb](0_EDA_raw_data.ipynb), we saw that different band of item ids were removed (i.e. non-random row numbers were removed in the training set). \n",
    "\n",
    "As a rule of thumb we should mimic the validation in the similar manner. The time component is fairly straigth forward. The question is whether it makes sense to take out bands of item id in addition. This is of course testable, and due to time constraints we will just split by time here.\n",
    "\n",
    "**NOTE**: We could have used `sklearn.model_selectionTimeSeriesSplit(n_splits=n)` to split the dataset. However, as we have quite big training set and since training of the models takes time, we will use only one split. Furthermore, we will do a simple parameter tuning where one parameter is tuned at the time. Thus, we make the split and the tuning ourselves instead of using `sklearn.model_selection.GridSearchCV` (it turns out that non-native `sklearn`-models needs some pathcing to work anyway). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = train_dt.shape[0]\n",
    "n_test = test_dt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test/n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the test-set is quite small in comparison to the train set. \n",
    "\n",
    "There are several ways to make a proper split for the validation set. We could for example take into account what `item_id` and `shop_id` which are present, where we are in a seasonal trend and so forth. In order to simplify the process, we just want to take care that there are enough samples in the validation set. Therefore, we make a $70-30$ split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_train = 0.7\n",
    "train_split = int(n_train*pct_train)\n",
    "indices_from_original_train = np.array(range(n_train))\n",
    "train_indices = indices_from_original_train[:train_split]\n",
    "val_indices = indices_from_original_train[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_unscaled = train_dt.iloc[train_indices]\n",
    "x_val_unscaled = train_dt.iloc[val_indices]\n",
    "\n",
    "x_train_scaled = train_non_dt.iloc[train_indices]\n",
    "x_val_scaled = train_non_dt.iloc[val_indices]\n",
    "\n",
    "y_train = target.iloc[train_indices]\n",
    "y_val = target.iloc[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order not to run the same fittings several times we make a dictionary to take care of the book-keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = generated_data.joinpath('models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_df_path = generated_data.joinpath('hyper_df.hdf')\n",
    "if hyper_df_path.is_file():\n",
    "    hyper_df = pd.read_hdf(hyper_df_path, key='hyper_df')\n",
    "else:\n",
    "    hyper_df = pd.DataFrame(columns=['name', 'train_score', 'val_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_object(save_dir, file_name, obj):\n",
    "    \"\"\"\n",
    "    Pickles a model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    save_dir : Path\n",
    "        Path to save to\n",
    "    file_name : str\n",
    "        The name of the file\n",
    "    obj : object\n",
    "        The object to store\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = save_dir.joinpath(f'{file_name}.pkl')\n",
    "    with file_path.open('wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f'Saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_object(save_dir, file_name):\n",
    "    \"\"\"\n",
    "    Pickles a model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_dir : Path\n",
    "        Path to the models\n",
    "    model_name : str\n",
    "        The name of the model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model : model object\n",
    "        The model to load\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = Path(save_dir).joinpath(f'{file_name}.pkl')\n",
    "    with file_path.open('rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "        print(f'Loaded from {file_path}')\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_hyper_df(hyper_df, hyper_df_path, model_name, train_score, val_score):\n",
    "    \"\"\"\n",
    "    Add the results of a model to the hyper parameter data frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hyper_df : DataFrame\n",
    "        The data frame to append\n",
    "    hyper_df_path : Path\n",
    "        Path to the stored data frame\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    train_score : float\n",
    "        The training score\n",
    "    val_score : float\n",
    "        The validation score\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    hyper_df : DataFrame\n",
    "        The appended data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    hyper_df = hyper_df.append({'name': model_name, \n",
    "                                'train_score': train_score,\n",
    "                                'val_score': val_score},\n",
    "                               ignore_index=True)\n",
    "    hyper_df.to_hdf(hyper_df_path, key='hyper_df')\n",
    "    \n",
    "    return hyper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(model_base_name, hyper_df, val_score):\n",
    "    \"\"\"\n",
    "    Check if the model has the best score among the model base\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_base_name : str\n",
    "        Name of the model base\n",
    "    hyper_df : DataFrame\n",
    "        The dataframe containing all the scores\n",
    "    val_score : float\n",
    "        The validation score\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "        Whether the validation score is the best \n",
    "    \"\"\"\n",
    "    \n",
    "    rows = [row for row in hyper_df.loc[:, 'name'] if model_base_name in row]\n",
    "    \n",
    "    if len(rows) == 0:\n",
    "        return True\n",
    "    \n",
    "    best_score = sorted(hyper_df.loc[hyper_df.loc[:, 'name'].isin(rows), 'val_score'].values)[0]\n",
    "    \n",
    "    if val_score < best_score:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val(hyper_df, model_base_name, param, exclude=[]):\n",
    "    \"\"\"\n",
    "    Plot the train-validation curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_base_name : str\n",
    "        Name of the model base\n",
    "    hyper_df : DataFrame\n",
    "        The dataframe containing all the scores\n",
    "    param : str\n",
    "        Parameter to check\n",
    "    exclude : list\n",
    "        Parameter to exclude\n",
    "    \"\"\"\n",
    "   \n",
    "    rows = [row for row in hyper_df.loc[:, 'name'] \n",
    "            if model_base_name in row \n",
    "            and param in row]\n",
    "    \n",
    "    if len(exclude) != 0:\n",
    "        for excl in exclude:\n",
    "            rows = [row for row in rows if excl not in row]\n",
    "\n",
    "    train_scores = hyper_df.loc[hyper_df.loc[:, 'name'].isin(rows), 'train_score'].values\n",
    "    val_scores = hyper_df.loc[hyper_df.loc[:, 'name'].isin(rows), 'val_score'].values\n",
    "    parameter_vals = [float(val.split(f'_{param}_')[-1].split('_')[0]) for val in rows]\n",
    "    \n",
    "    parameter_vals, train_scores, val_scores = zip(*sorted(zip(parameter_vals, train_scores, val_scores)))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "        \n",
    "    ax.plot(parameter_vals, train_scores, label='Train')\n",
    "    ax.plot(parameter_vals, val_scores, label='Validation')\n",
    "    ax.set_xlabel(param)\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='best', fancybox=True, framealpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no real hyperparameters to tune in linear regression (other than choosing wheter we should include the intersect or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lin_reg'\n",
    "\n",
    "if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(x_train_scaled, y_train)\n",
    "    lin_reg_train_score = rmse_clip(y_train, lin_reg.predict(x_train_scaled))\n",
    "    lin_reg_val_score = rmse_clip(y_val, lin_reg.predict(x_val_scaled))\n",
    "    pickle_object(model_dir, model_name, lin_reg)\n",
    "    hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, lin_reg_train_score, lin_reg_val_score)\n",
    "else:\n",
    "    lin_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "    lin_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "    lin_reg = unpickle_object(model_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train score: {lin_reg_train_score:.3f}')\n",
    "print(f'Validation score: {lin_reg_val_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the strength of the `L1` regularizer through the `alpha` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'lasso_reg'\n",
    "model_name_template = f'{model_base_name}_alpha_{{}}'\n",
    "\n",
    "for alpha in [1, 2, 3, 4, 8]:\n",
    "    print(f'Processing alpha={alpha}')\n",
    "    model_name = model_name_template.format(alpha)\n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        lasso_reg = Lasso(alpha=alpha)\n",
    "        # NOTE: We ravel in order to avoid warnings\n",
    "        lasso_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        lasso_reg_train_score = rmse_clip(y_train, lasso_reg.predict(x_train_unscaled))\n",
    "        lasso_reg_val_score = rmse_clip(y_val, lasso_reg.predict(x_val_unscaled))\n",
    "        if best_model(model_base_name, hyper_df, lasso_reg_val_score):\n",
    "            pickle_object(model_dir, model_base_name, lasso_reg)\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, lasso_reg_train_score, lasso_reg_val_score)\n",
    "    else:\n",
    "        lasso_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        lasso_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "        lasso_reg = unpickle_object(model_dir, model_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the strength of the `L2` regularizer through the `alpha` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'ridge_reg'\n",
    "model_name_template = f'{model_base_name}_alpha_{{}}'\n",
    "\n",
    "for alpha in [1, 2, 3, 4, 8]:\n",
    "    print(f'Processing alpha={alpha}')\n",
    "    model_name = model_name_template.format(alpha)\n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        ridge_reg = Ridge(alpha=alpha)\n",
    "        # NOTE: We ravel in order to avoid warnings\n",
    "        ridge_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        ridge_reg_train_score = rmse_clip(y_train, ridge_reg.predict(x_train_unscaled))\n",
    "        ridge_reg_val_score = rmse_clip(y_val, ridge_reg.predict(x_val_unscaled))\n",
    "        if best_model(model_base_name, hyper_df, ridge_reg_val_score):\n",
    "            pickle_object(model_dir, model_base_name, ridge_reg)\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, ridge_reg_train_score, ridge_reg_val_score)\n",
    "    else:\n",
    "        ridge_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        ridge_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "        ridge_reg = unpickle_object(model_dir, model_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge classifier appers to be in a bad health at the moment (given the ill-condition warnings). This is further discussed [here](https://github.com/scikit-learn/scikit-learn/issues/10517). Therefore, we will drop using the ridge classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The KNN uses forever to run, and it appears that it only uses one processor (even for `njobs=-1`). Note that KNN scales rather badly with number of points as seen [here](https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/) and [here](https://cs.stanford.edu/people/ang/papers/nips06-mapreducemulticore.pdf). To overcome the problem, we could use PCA analysis or t-SNE for dimensionality reduction. Similar arguments also holds for SVM as indicated [here](https://stackoverflow.com/questions/18165213/how-much-time-does-take-train-svm-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_knn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'knn_reg_n_neighbors_1'\n",
    "\n",
    "if run_knn:\n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        knn_reg = KNeighborsRegressor(n_neighbors = 1)\n",
    "        knn_reg.fit(x_train_scaled, y_train)\n",
    "        knn_reg_train_score = rmse_clip(y_train, knn_reg.predict(x_train_scaled))\n",
    "        knn_reg_val_score = rmse_clip(y_val, knn_reg.predict(x_val_scaled))\n",
    "        pickle_object(model_dir, model_name, knn_reg)\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, knn_reg_train_score, knn_reg_val_score)\n",
    "    else:\n",
    "        knn_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        knn_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "        knn_reg = unpickle_object(model_dir, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to benchmark our results with [this notebook](https://www.kaggle.com/the1owl/playing-in-the-sandbox/notebook), which achieves a RMSE around $0.27$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_name = 'et_reg'\n",
    "model_name = f'{model_base_name}_n_estimators_25_max_depth_15'\n",
    "\n",
    "if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "    et_reg = ExtraTreesRegressor(n_estimators=25,\n",
    "                                 n_jobs=-1,\n",
    "                                 max_depth=15,\n",
    "                                 random_state=18)\n",
    "    # NOTE: We ravel in order to avoid warnings\n",
    "    et_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "    et_reg_train_score = rmse_clip(y_train, et_reg.predict(x_train_unscaled))\n",
    "    et_reg_val_score = rmse_clip(y_val, et_reg.predict(x_val_unscaled))\n",
    "    pickle_object(model_dir, model_base_name, et_reg)\n",
    "    hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, et_reg_train_score, et_reg_val_score)\n",
    "else:\n",
    "    et_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "    et_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "    et_reg = unpickle_object(model_dir, model_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train score: {et_reg_train_score:.3f}')\n",
    "print(f'Validation score: {et_reg_val_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can increase the score by first tuning the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'et_reg'\n",
    "model_name_template = f'{model_base_name}_n_estimators_{{}}'\n",
    "\n",
    "# Sklearn documentation:\n",
    "# https://etboost.readthedocs.io/en/latest/python/python_api.html#module-etboost.sklearn\n",
    "for n_estimators in [12, 25, 50, 75]:\n",
    "    print(f'Processing n_estimators={n_estimators}')\n",
    "    model_name = model_name_template.format(n_estimators)\n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        et_reg = ExtraTreesRegressor(n_estimators=n_estimators,\n",
    "                                     n_jobs=-1,\n",
    "                                     max_depth=15,\n",
    "                                     random_state=18)\n",
    "        # NOTE: We ravel in order to avoid warnings\n",
    "        et_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        et_reg_train_score = rmse_clip(y_train, et_reg.predict(x_train_unscaled))\n",
    "        et_reg_val_score = rmse_clip(y_val, et_reg.predict(x_val_unscaled))\n",
    "        if best_model(model_base_name, hyper_df, et_reg_val_score):\n",
    "            pickle_object(model_dir, model_base_name, et_reg)\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, et_reg_train_score, et_reg_val_score)\n",
    "    else:\n",
    "        et_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        et_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "        et_reg = unpickle_object(model_dir, model_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'n_estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gained a bit to have $75$ estimators, but it also costed time. Let's finally tune the `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'et_reg'\n",
    "model_name_template = f'{model_base_name}_max_depth_{{}}'\n",
    "\n",
    "# Sklearn documentation:\n",
    "# https://etboost.readthedocs.io/en/latest/python/python_api.html#module-etboost.sklearn\n",
    "for max_depth in [6, 8, 15, 20]:\n",
    "    print(f'Processing max_depth={max_depth}')\n",
    "    model_name = model_name_template.format(max_depth)\n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        et_reg = ExtraTreesRegressor(n_estimators=75,\n",
    "                                     max_depth=max_depth,\n",
    "                                     n_jobs=-1,\n",
    "                                     random_state=18)\n",
    "        # NOTE: We ravel in order to avoid warnings\n",
    "        et_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        et_reg_train_score = rmse_clip(y_train, et_reg.predict(x_train_unscaled))\n",
    "        et_reg_val_score = rmse_clip(y_val, et_reg.predict(x_val_unscaled))\n",
    "        if best_model(model_base_name, hyper_df, et_reg_val_score):\n",
    "            pickle_object(model_dir, model_base_name, et_reg)\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, et_reg_train_score, et_reg_val_score)\n",
    "    else:\n",
    "        et_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        et_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "        et_reg = unpickle_object(model_dir, model_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'max_depth', ['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Gradient Boosting Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other estimators, the `xgboost` estimator has several knobs to turn which can be used to find the optimal estimator.\n",
    "\n",
    "To start with, we have:\n",
    "\n",
    "Better fitting (increase for reducing underfit)\n",
    "* max_depth\n",
    "* subsample\n",
    "* colsample_bytree\n",
    "* colsample_bylevel\n",
    "* eta \n",
    "* num_round\n",
    "\n",
    "Impeeds fitting (increase for reducing overfitting)\n",
    "* min_child_weight\n",
    "* lambda\n",
    "* alpha\n",
    "\n",
    "We will start with the `max_depth` parameter to investigate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'xg_reg'\n",
    "model_name_template = f'{model_base_name}_max_depth_{{}}'\n",
    "\n",
    "# Sklearn documentation:\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "for max_depth in [0, 1, 2, 3, 4, 5, 6, 8, 10, 14]:\n",
    "    print(f'Processing max_depth={max_depth}')\n",
    "    model_name = model_name_template.format(max_depth)\n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        xg_reg = XGBRegressor(max_depth=max_depth,\n",
    "                              n_jobs=-1,\n",
    "                              seed=seed)\n",
    "        # NOTE: We ravel in order to avoid warnings\n",
    "        xg_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        xg_reg_train_score = rmse_clip(y_train, xg_reg.predict(x_train_unscaled))\n",
    "        xg_reg_val_score = rmse_clip(y_val, xg_reg.predict(x_val_unscaled))\n",
    "        if best_model(model_base_name, hyper_df, xg_reg_val_score):\n",
    "            pickle_object(model_dir, model_base_name, xg_reg)\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, xg_reg_train_score, xg_reg_val_score)\n",
    "    else:\n",
    "        xg_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        xg_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "        xg_reg = unpickle_object(model_dir, model_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'max_depth', ['min_child_weigh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to make a submission with this, to see if we are doing any better than our mean prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_prediction = test_dt.loc[:, ['ID']]\n",
    "xg_prediction.loc[:, 'item_cnt_month'] = xg_reg.predict(test_non_dt).clip(0, 20)\n",
    "\n",
    "# Set ID as index\n",
    "xg_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "xg_prediction.to_csv(generated_data.joinpath('xg_prediction.csv'))\n",
    "xg_prediction.loc[:, 'item_cnt_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gave a leader board score of $1.07932$, which is better than the mean predictions. Let us improve the validation score even further. From the graph above, it appears that we are [overfitting](http://scikit-learn.org/stable/modules/learning_curve.html#validation-curve) as the training error improves, whereas the validation error is flatting out. Let's increase `min_child_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'xg_reg'\n",
    "model_name_template = f'{model_base_name}_max_depth_14_min_child_weight_{{}}'\n",
    "\n",
    "# Sklearn documentation:\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "for min_child_weight in [1, 2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "    print(f'Processing min_child_weight={min_child_weight}')\n",
    "    model_name = model_name_template.format(min_child_weight)\n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        xg_reg = XGBRegressor(max_depth=14,\n",
    "                              min_child_weight=min_child_weight,\n",
    "                              n_jobs=-1,\n",
    "                              seed=seed)\n",
    "        # NOTE: We ravel in order to avoid warnings\n",
    "        xg_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        xg_reg_train_score = rmse_clip(y_train, xg_reg.predict(x_train_unscaled))\n",
    "        xg_reg_val_score = rmse_clip(y_val, xg_reg.predict(x_val_unscaled))\n",
    "        if best_model(model_base_name, hyper_df, xg_reg_val_score):\n",
    "            pickle_object(model_dir, model_base_name, xg_reg)\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, xg_reg_train_score, xg_reg_val_score)\n",
    "    else:\n",
    "        xg_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        xg_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]\n",
    "        xg_reg = unpickle_object(model_dir, model_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'min_child_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a minimum around $128$. We could continue to tune the hyperparameters, but we stop here due to time constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We also make a costum RMSE for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_keras_clip(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the root mean squared error of the predictions\n",
    "    \n",
    "    The root mean squared error is defined by:\n",
    "    $\\sqrt {\\frac {\\sum _{t=1}^{T}({\\hat {y}}_{t}-y_{t})^{2}}{T}}$\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This version clips the predictions to [0, 20]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape (n_samples,)\n",
    "        The correct prediction (the ground truth)\n",
    "    y_pred : array, shape (n_samples,)\n",
    "        The predictions\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rmse : float\n",
    "        The root mean squared error\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(tf.clip_by_value(y_pred, 0, 20) - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: RNNs would probably be the best fit for this task, we will for simplicity use plain old multilayer perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, optimizer, rmse_keras_clip, hidden_layers=1, nodes=32, dropout=0, silent=False):\n",
    "    \"\"\"\n",
    "    Returns a keras model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        The input dimension\n",
    "    hidden_layers : int\n",
    "        The number of hidden layers\n",
    "    optimizer : str\n",
    "        The optimizer to use\n",
    "    rmse_keras_clip : function\n",
    "        Function which gives the rmse loss function\n",
    "    nodes : int or array-like, shape (hidden_layers)\n",
    "        Nodes for all the layers.\n",
    "        If array-like, each element corresponds to the nodes in the hidden layer\n",
    "        If int, all hidden layers will have the same number of nodes\n",
    "    dropout : float or array-like, shape (hidden_layers)\n",
    "        Dropout for all the layers.\n",
    "        If array-like, each element corresponds to the dropout values after each hidden layer\n",
    "        If int, all hidden layers will have the same dropout value\n",
    "    silent : bool\n",
    "        If False the summary will be printed\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(nodes) == int:\n",
    "        nodes = [nodes] * hidden_layers\n",
    "    if type(dropout) == float or type(dropout) == int:\n",
    "        dropout = [dropout] * hidden_layers\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(nodes[0], input_dim=input_dim))\n",
    "    \n",
    "    if len(nodes) > 1:\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    for node, drop in zip(nodes[1:], dropout[:-1]):\n",
    "        model.add(Dropout(drop))\n",
    "        model.add(Dense(node))\n",
    "        \n",
    "        if node != nodes[-1]:\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "    # Add the final layer\n",
    "    model.add(Dropout(dropout[-1]))\n",
    "    model.add(Dense(1))\n",
    "    # NOTE: We use identity as we are dealing with a regression problem\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss=rmse_keras_clip,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[rmse_keras_clip])\n",
    "    \n",
    "    if not silent:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural networks, it makes sense to investigate\n",
    "\n",
    "Better fitting (increase for reducing underfit)\n",
    "* Number of neurons per layer\n",
    "* Number of layers\n",
    "* Adam/Adadelta/Adagrad/... (observed to lead to more overfitting)\n",
    "* Batch size\n",
    "\n",
    "Impeeds fitting (increase for reducing overfitting)\n",
    "* L2/L1 for weights\n",
    "* Dropout/Dropconnect\n",
    "* Static dropconnect\n",
    "\n",
    "We start by optimizing one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'nn_reg'\n",
    "model_name_template = f'{model_base_name}_nodes_{{}}'\n",
    "\n",
    "for node in (8, 16, 32, 64, 128, 256):\n",
    "    print(f'Processing nodes={node}')\n",
    "    model_name = model_name_template.format(node)\n",
    "    model_path = model_dir.joinpath(f'{model_name}.hdf')\n",
    "    checkpointer = ModelCheckpoint(filepath=str(model_path),\n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True)\n",
    "    \n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        nn_reg = build_mlp(train_non_dt.shape[1], \n",
    "                           'adadelta',\n",
    "                           rmse_keras_clip,\n",
    "                           hidden_layers=1,\n",
    "                           nodes=node,\n",
    "                           dropout=0.32)\n",
    "        \n",
    "        nn_reg.fit(x_train_scaled, \n",
    "                   y_train, \n",
    "                   validation_data=(x_val_scaled, y_val),\n",
    "                   batch_size=4096,\n",
    "                   epochs=10,\n",
    "                   callbacks=[checkpointer, stopper])\n",
    "        history = nn_reg.history.history\n",
    "        pickle_object(model_dir, f'{model_name}', history)\n",
    "        \n",
    "        # NOTE: Find the best scores\n",
    "        nn_reg_val_score = min(nn_reg.history.history['val_loss'])\n",
    "        index = np.where(np.isclose(history['val_loss'], nn_reg_val_score))[0][0]\n",
    "        nn_reg_train_score = history['loss'][index]\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, nn_reg_train_score, nn_reg_val_score)\n",
    "    else:\n",
    "        nn_reg = load_model(model_path, custom_objects={'rmse_keras_clip': rmse_keras_clip})\n",
    "        history = unpickle_object(model_dir, f'{model_name}')\n",
    "        nn_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        nn_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is somewhat surprising as the validation error is lower than the training error.\n",
    "One hypothesis could be that we should have trained for more epochs. However, the model with $256$ nodes ended with early stopping. \n",
    "\n",
    "As the score is quite low as compared to the other models we have fitted, we can probe the submit with the last fitted network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_prediction = test_dt.loc[:, ['ID']]\n",
    "nn_prediction.loc[:, 'item_cnt_month'] = nn_reg.predict(test_non_dt).clip(0, 20)\n",
    "\n",
    "# Set ID as index\n",
    "nn_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "nn_prediction.to_csv(generated_data.joinpath('nn_prediction.csv'))\n",
    "nn_prediction.loc[:, 'item_cnt_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gave a submission score of $1.05710$, which is actually not that bad. Although $256$ did not had the lowest score, lets add more layers with $128$ nodes to see if we can improve things a bit. Notice that we increase the number of epochs as more complex networks usually needs more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base_name = 'nn_reg'\n",
    "model_name_template = f'{model_base_name}_layer_{{}}'\n",
    "\n",
    "for layer in (2, 3, 4):\n",
    "    nodes = [256] + [128] * (layer - 1)\n",
    "    print(f'Processing layers={layer}')\n",
    "    model_name = model_name_template.format(layer)\n",
    "    model_path = model_dir.joinpath(f'{model_name}.hdf')\n",
    "    checkpointer = ModelCheckpoint(filepath=str(model_path),\n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True)\n",
    "    \n",
    "    if not model_name in hyper_df.loc[:, 'name'].values:\n",
    "        nn_reg = build_mlp(train_non_dt.shape[1], \n",
    "                           'adadelta',\n",
    "                           rmse_keras_clip,\n",
    "                           hidden_layers=1,\n",
    "                           nodes=nodes,\n",
    "                           dropout=0.32)\n",
    "        \n",
    "        nn_reg.fit(x_train_scaled, \n",
    "                   y_train, \n",
    "                   validation_data=(x_val_scaled, y_val),\n",
    "                   batch_size=4096,\n",
    "                   epochs=100,\n",
    "                   callbacks=[checkpointer, stopper])\n",
    "        history = nn_reg.history.history\n",
    "        pickle_object(model_dir, f'{model_name}', history)\n",
    "        \n",
    "        # NOTE: Find the best scores\n",
    "        nn_reg_val_score = min(nn_reg.history.history['val_loss'])\n",
    "        index = np.where(np.isclose(history['val_loss'], nn_reg_val_score))[0][0]\n",
    "        nn_reg_train_score = history['loss'][index]\n",
    "        hyper_df = add_to_hyper_df(hyper_df, hyper_df_path, model_name, nn_reg_train_score, nn_reg_val_score)\n",
    "    else:\n",
    "        nn_reg = load_model(model_path, custom_objects={'rmse_keras_clip': rmse_keras_clip})\n",
    "        history = unpickle_object(model_dir, f'{model_name}')\n",
    "        nn_reg_train_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'train_score'].values[0]\n",
    "        nn_reg_val_score = hyper_df.loc[hyper_df.loc[:, 'name'] == model_name, 'val_score'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(hyper_df, model_base_name, 'layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now investigate the performances of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_df.sort_values('name', axis=0, inplace=True)\n",
    "hyper_df.reset_index(drop=True, inplace=True)\n",
    "hyper_df.to_hdf(hyper_df_path, key='hyper_df')\n",
    "hyper_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we create the best regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best lasso regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_df = hyper_df.loc[hyper_df.loc[:, 'name'].str.contains('lasso')]\n",
    "lasso_df.loc[np.isclose(lasso_df.loc[:, 'val_score'], lasso_df.loc[:, 'val_score'].min())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lasso_reg = Lasso(alpha=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best extra tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_df = hyper_df.loc[hyper_df.loc[:, 'name'].str.contains('et')]\n",
    "et_df.loc[np.isclose(et_df.loc[:, 'val_score'], et_df.loc[:, 'val_score'].min())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that we did this tuning with `n_estimators=75`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_et_reg = ExtraTreesRegressor(n_estimators=75,\n",
    "                                  max_depth=max_depth,\n",
    "                                  n_jobs=-1,\n",
    "                                  random_state=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best extreme gradient regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_df = hyper_df.loc[hyper_df.loc[:, 'name'].str.contains('xg')]\n",
    "xg_df.loc[np.isclose(xg_df.loc[:, 'val_score'], xg_df.loc[:, 'val_score'].min())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xg_reg = XGBRegressor(max_depth=14,\n",
    "                           min_child_weight=128,\n",
    "                           n_jobs=-1,\n",
    "                           seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best neural net regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_df = hyper_df.loc[hyper_df.loc[:, 'name'].str.contains('nn')]\n",
    "nn_df.loc[np.isclose(nn_df.loc[:, 'val_score'], nn_df.loc[:, 'val_score'].min())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that this network was built with $256$ initial nodes, then subsequent $128$ nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nn_reg = build_mlp(train_non_dt.shape[1], \n",
    "                        'adadelta',\n",
    "                        rmse_keras_clip,\n",
    "                        hidden_layers=3,\n",
    "                        nodes=[256, 128, 128],\n",
    "                        dropout=0.32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first level we predict with our optimized classifiers, just as we would've done if we were to use only one of the models for the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = 'best_lin_reg'\n",
    "if not model_dir.joinpath(best_name + '.pkl').is_file():\n",
    "    best_lin_reg.fit(train_non_dt, target)\n",
    "    pickle_object(model_dir, best_name, best_lin_reg)\n",
    "else:\n",
    "    best_lin_reg = unpickle_object(model_dir, best_name)\n",
    "    \n",
    "lin_lvl_1_pred = best_lin_reg.predict(test_non_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = 'best_lasso_reg'\n",
    "if not model_dir.joinpath(best_name + '.pkl').is_file():\n",
    "    best_lasso_reg.fit(train_non_dt, target)\n",
    "    pickle_object(model_dir, best_name, best_lasso_reg)\n",
    "else:\n",
    "    best_lasso_reg = unpickle_object(model_dir, best_name)\n",
    "    \n",
    "lasso_lvl_1_pred = best_lasso_reg.predict(test_non_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = 'best_et_reg'\n",
    "if not model_dir.joinpath(best_name + '.pkl').is_file():\n",
    "    best_et_reg.fit(train_dt, target.values.ravel())\n",
    "    pickle_object(model_dir, best_name, best_et_reg)\n",
    "else:\n",
    "    best_et_reg = unpickle_object(model_dir, best_name)\n",
    "    \n",
    "et_lvl_1_pred = best_et_reg.predict(test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = 'best_xg_reg'\n",
    "if not model_dir.joinpath(best_name + '.pkl').is_file():\n",
    "    best_xg_reg.fit(train_dt, target.values.ravel())\n",
    "    pickle_object(model_dir, best_name, best_xg_reg)\n",
    "else:\n",
    "    best_xg_reg = unpickle_object(model_dir, best_name)\n",
    "    \n",
    "xg_lvl_1_pred = best_xg_reg.predict(test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = 'best_nn_reg'\n",
    "model_path = model_dir.joinpath(f'{best_name}.hdf')\n",
    "stopper = EarlyStopping(monitor='loss', patience=2, verbose=1, mode='auto', baseline=None)\n",
    "\n",
    "if not model_path.is_file():\n",
    "    # NOTE: We use a lower patience as we would not like to overfit the network\n",
    "    checkpointer = ModelCheckpoint(filepath=str(model_path),\n",
    "                                   monitor='loss',\n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True)\n",
    "    best_nn_reg.fit(train_non_dt, target, batch_size=4096, epochs=100, callbacks=[checkpointer, stopper])\n",
    "else:\n",
    "    best_nn_reg = load_model(model_path, custom_objects={'rmse_keras_clip': rmse_keras_clip})\n",
    "    print(f'Loaded from {model_path}')\n",
    "    \n",
    "nn_lvl_1_pred = best_nn_reg.predict(test_non_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we can do with the prediction of the model with the best loss. As there are some discrepancies between the neural net values and the other, we submit both the best neural network, and the best extreme gradient regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nn_prediction = test_dt.loc[:, ['ID']]\n",
    "best_nn_prediction.loc[:, 'item_cnt_month'] = nn_lvl_1_pred.clip(0, 20)\n",
    "\n",
    "# Set ID as index\n",
    "best_nn_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "best_nn_prediction.to_csv(generated_data.joinpath('best_nn_prediction.csv'))\n",
    "best_nn_prediction.loc[:, 'item_cnt_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a score of $1.06557$, which is sligthly worse then the previous neural network submission. It is likely that overfitting is the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xg_prediction = test_dt.loc[:, ['ID']]\n",
    "best_xg_prediction.loc[:, 'item_cnt_month'] = xg_lvl_1_pred.clip(0, 20)\n",
    "\n",
    "# Set ID as index\n",
    "best_xg_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "best_xg_prediction.to_csv(generated_data.joinpath('best_xg_prediction.csv'))\n",
    "best_xg_prediction.loc[:, 'item_cnt_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gave the score of $1.00418$, which is so far the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the prediction to the new \"unseen\" level $2$ test set.\n",
    "\n",
    "First, we will make a test set we can predict the meta model(s) on level $2$ on.\n",
    "\n",
    "**NOTE**: We will stop after level $2$, so we will only have one meta model on level $2$, but in principle we could have several models on level $2$ to make up the test set of level $3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_lvl_2 = np.c_[lin_lvl_1_pred.clip(0, 20), \n",
    "                     lasso_lvl_1_pred.clip(0, 20),\n",
    "                     et_lvl_1_pred.clip(0, 20),\n",
    "                     xg_lvl_1_pred.clip(0, 20),\n",
    "                     nn_lvl_1_pred.clip(0, 20)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we declare the target for the meta model on level $2$.\n",
    "\n",
    "As we only have one meta model on level $2$, we will use the $5$ last months to create for the k-fold validation on level $2$. We start by getting the first index of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_last_month = 6\n",
    "max_month = train_dt.loc[:, 'date_block_num'].max()\n",
    "# NOTE: Add one as range has non-inclusive endpoint\n",
    "month_range = np.array(range(max_month - n_last_month, max_month + 1))\n",
    "first_index = train_dt.shape[0] - train_dt.loc[train_dt.loc[:, 'date_block_num'].isin(month_range)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lvl_2 = target.iloc[first_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the training set, which we will fill with the k-fold procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lvl_2 = np.zeros([y_train_lvl_2.shape[0], x_test_lvl_2.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lvl_2_path = generated_data.joinpath('x_train_lvl_2.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_offset = 0\n",
    "if not x_train_lvl_2_path.is_file():\n",
    "    for month in month_range:\n",
    "        print(f'Processing month: {month}')\n",
    "        # Find indices\n",
    "        first_index = train_dt.shape[0] - train_dt.loc[train_dt.loc[:, 'date_block_num'] < month].shape[0]\n",
    "        last_index = first_index + train_dt.loc[train_dt.loc[:, 'date_block_num'] == month].shape[0]\n",
    "        \n",
    "        # Split to train and validate\n",
    "        x_train_scaled = train_non_dt.iloc[:first_index]\n",
    "        x_train_unscaled = train_dt.iloc[:first_index]\n",
    "        y_train = target.iloc[:first_index]\n",
    "        x_val_scaled = train_non_dt.iloc[first_index:last_index]\n",
    "        x_val_unscaled = train_dt.iloc[first_index:last_index]    \n",
    "\n",
    "        # Make new models (where necessary)\n",
    "        nn_reg_lvl_2 = build_mlp(x_train_unscaled.shape[1], \n",
    "                                 'adadelta',\n",
    "                                 rmse_keras_clip,\n",
    "                                 hidden_layers=3,\n",
    "                                 nodes=[256, 128, 128],\n",
    "                                 dropout=0.32,\n",
    "                                 silent=True)          \n",
    "        \n",
    "        # Fit\n",
    "        print(f'    - fitting linear model')\n",
    "        best_lin_reg.fit(x_train_scaled, y_train)\n",
    "        print(f'    - fitting lasso model')\n",
    "        best_lasso_reg.fit(x_train_scaled, y_train)\n",
    "        print(f'    - fitting extra tree model')\n",
    "        best_et_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        print(f'    - fitting extra gradient dt model')\n",
    "        best_xg_reg.fit(x_train_unscaled, y_train.values.ravel())\n",
    "        print(f'    - fitting nn model')\n",
    "        nn_reg_lvl_2.fit(x_train_scaled, y_train, batch_size=4096, epochs=100, callbacks=[stopper])\n",
    "        \n",
    "        # Predict\n",
    "        lin_lvl_2_pred = best_lin_reg.predict(x_val_scaled)\n",
    "        lasso_lvl_2_pred = best_lasso_reg.predict(x_val_scaled)\n",
    "        et_lvl_2_pred = best_et_reg.predict(x_val_unscaled)\n",
    "        xg_lvl_2_pred = best_xg_reg.predict(x_val_unscaled)\n",
    "        nn_lvl_2_pred = nn_reg_lvl_2.predict(x_val_scaled)\n",
    "        \n",
    "        # Store the predictions in the training set\n",
    "        row_inds = np.array(range(index_offset, last_index - first_index + index_offset))\n",
    "        x_train_lvl_2[row_inds, 0] = lin_lvl_2_pred.clip(0, 20).ravel()\n",
    "        x_train_lvl_2[row_inds, 1] = lasso_lvl_2_pred.clip(0, 20).ravel()\n",
    "        x_train_lvl_2[row_inds, 2] = et_lvl_2_pred.clip(0, 20).ravel()\n",
    "        x_train_lvl_2[row_inds, 3] = xg_lvl_2_pred.clip(0, 20).ravel()\n",
    "        x_train_lvl_2[row_inds, 4] = nn_lvl_2_pred.clip(0, 20).ravel()\n",
    "        \n",
    "        # Update index_offset\n",
    "        index_offset = last_index - first_index\n",
    "    x_train_lvl_2_df = pd.DataFrame(x_train_lvl_2, \n",
    "                                    columns=['lin_pred', 'lasso_pred', 'et_pred', 'xg_pred', 'nn_pred'])\n",
    "    x_train_lvl_2_df.to_hdf(generated_data.joinpath('x_train_lvl_2.hdf'), key='x_train_lvl_2')\n",
    "else:\n",
    "    x_train_lvl_2_df = pd.read_hdf(x_train_lvl_2_path, key='x_train_lvl_2')\n",
    "    print(f'x_train_lvl_2_df loaded from {x_train_lvl_2_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lvl_2_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big question is now what model to use as the final model.\n",
    "\n",
    "The rule of thumb is to have \"simpler\" models. Let's try with a linear model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train on the validation months, and predict using the test set for level $2$. \n",
    "\n",
    "**NOTE**: We are fitting on only the last months, and predicting for the prediction month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lin = LinearRegression()\n",
    "final_lin.fit(x_train_lvl_2_df, y_train_lvl_2)\n",
    "final_lin_prediction = final_lin.predict(x_test_lvl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_stack_prediction = test_dt.loc[:, ['ID']]\n",
    "lin_stack_prediction.loc[:, 'item_cnt_month'] = final_lin_prediction.clip(0, 20)\n",
    "\n",
    "# Set ID as index\n",
    "lin_stack_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "lin_stack_prediction.to_csv(generated_data.joinpath('lin_stack_prediction.csv'))\n",
    "lin_stack_prediction.loc[:, 'item_cnt_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears this is close to a mean submission as numbers cluster around $0.244$ items, however, if we look at the distributions for the best predictors, the $75$-percentile is still around $0.0$. Let's see if we can get more diversity from a shallow xgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_xg = XGBRegressor(max_depth=10,\n",
    "                        min_child_weight=10,\n",
    "                        n_jobs=-1,\n",
    "                        seed=seed)\n",
    "final_xg.fit(x_train_lvl_2_df.values, y_train_lvl_2.values.ravel())\n",
    "final_xg_prediction = final_xg.predict(x_test_lvl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_stack_prediction = test_dt.loc[:, ['ID']]\n",
    "xg_stack_prediction.loc[:, 'item_cnt_month'] = final_xg_prediction.clip(0, 20)\n",
    "\n",
    "# Set ID as index\n",
    "xg_stack_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "xg_stack_prediction.to_csv(generated_data.joinpath('xg_stack_prediction.csv'))\n",
    "xg_stack_prediction.loc[:, 'item_cnt_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the max only reaches $2.74$, we can expect high penalties from large numbers. In fact, this prediction gives a loss of $1.22040$ on the leaderboard, which is rather bad. Probably we've been too harsh on the `min_child_weight`. Let's relax it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_xg_min_child_1 = XGBRegressor(max_depth=10,\n",
    "                                    min_child_weight=1,\n",
    "                                    n_jobs=-1,\n",
    "                                    seed=seed)\n",
    "final_xg_min_child_1.fit(x_train_lvl_2_df.values, y_train_lvl_2.values.ravel())\n",
    "final_xg_min_child_1_prediction = final_xg_min_child_1.predict(x_test_lvl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_min_child_1_stack_prediction = test_dt.loc[:, ['ID']]\n",
    "xg_min_child_1_stack_prediction.loc[:, 'item_cnt_month'] = final_xg_min_child_1_prediction.clip(0, 20)\n",
    "\n",
    "# Set ID as index\n",
    "xg_min_child_1_stack_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "xg_min_child_1_stack_prediction.to_csv(generated_data.joinpath('xg_min_child_1_stack_prediction.csv'))\n",
    "xg_min_child_1_stack_prediction.loc[:, 'item_cnt_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution improves a bit. The prediction gives a loss of $1.22864$ on the leaderboard, which is even worse.\n",
    "Therefore, our official submission will our best gradient boost model with a score of $1.00418$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
