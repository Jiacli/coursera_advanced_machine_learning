{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy learning and SARSA\n",
    "\n",
    "This notebook builds on `qlearning.ipynb` to implement Expected Value SARSA.\n",
    "\n",
    "The policy we're gonna use is epsilon-greedy policy, where agent takes optimal action with probability $(1-\\epsilon)$, otherwise samples action at random. Note that agent __can__ occasionally sample optimal action during random sampling by pure chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1\n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "\n",
    "class EVSarsaAgent(QLearningAgent):\n",
    "    \"\"\" \n",
    "    An agent that changes some of q-learning functions to implement Expected Value SARSA.\n",
    "    \n",
    "    Notes\n",
    "    ------\n",
    "    This implementation assumes that QLearningAgent.update uses get_value(next_state).\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\" \n",
    "        Returns Vpi for current state under epsilon-greedy policy:\n",
    "          V_{pi}(s) = sum_{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : object\n",
    "            The state to get the value function value from\n",
    "            Must be a valid key in Q-value\n",
    "        \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        len_possible_actions = len(possible_actions)\n",
    "        \n",
    "        #If there are no legal actions, return 0.0\n",
    "        if len_possible_actions == 0:\n",
    "            return 0.0\n",
    "\n",
    "        q_values_state = np.empty(len(possible_actions))\n",
    "        for i, action in enumerate(possible_actions):\n",
    "            q_values_state[i] =  self.get_qvalue(state, action)\n",
    "        \n",
    "        # Create the policy\n",
    "        # We start by making an array where the probability of any action is epsilon \n",
    "        pi_state = epsilon*np.ones(len_possible_actions)/len_possible_actions\n",
    "        # We pick the index with highest Q-value...\n",
    "        best_action_index = q_values_state.argmax()\n",
    "        # ...and add 1 - epsilon to that action\n",
    "        pi_state[best_action_index] += (1-epsilon)\n",
    "        # NOTE: In order to convince ourselves that the total probability is 1, note that\n",
    "        #       n*(epsilon/n) + (1-epsilon) = 1 \n",
    "        \n",
    "        # Calculate the state value\n",
    "        state_value = (pi_state*q_values_state).sum()\n",
    "        \n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliff World\n",
    "\n",
    "Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time.\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/cliffworld.png width=600>\n",
    "<center><i>image by cs188</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.envs.toy_text\n",
    "env = gym.envs.toy_text.CliffWalkingEnv()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our cliffworld has one difference from what's on the image: there is no wall. \n",
    "# Agent can choose to go as close to the cliff as it wishes. x:start, T:exit, C:cliff, o: flat ground\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, \n",
    "                   agent,\n",
    "                   t_max=10**4):\n",
    "    \"\"\"\n",
    "    Run the full game, with actions given by the agent's policy\n",
    "    and updates the policy whenever possible\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym-object\n",
    "        The environment to play with\n",
    "    agent : QLearningAgent\n",
    "        The agent to play and train with\n",
    "    t_max : int\n",
    "        The maximum number of steps to take\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    total_reward : float\n",
    "        The accumulated reward\n",
    "    \"\"\"\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "        \n",
    "        next_s,r,done,_ = env.step(a)\n",
    "        agent.update(s, a, r, next_s)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "\n",
    "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
    "                       get_legal_actions = lambda s: range(n_actions))\n",
    "\n",
    "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
    "                       get_legal_actions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "rewards_sarsa, rewards_ql = [], []\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
    "    rewards_ql.append(play_and_train(env, agent_ql))\n",
    "    #Note: agent.epsilon stays constant\n",
    "    \n",
    "    if i %100 ==0:\n",
    "        clear_output(True)\n",
    "        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
    "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
    "        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n",
    "        plt.plot(moving_average(rewards_ql), label='qlearning')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.ylim(-500, 0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what did the algorithms learn by visualizing their actions at every state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_policy(env, agent):\n",
    "    \"\"\"\n",
    "    Prints CliffWalkingEnv policy with arrows. \n",
    "    Note that this is done in a hard-coded fashion.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym-object\n",
    "        The environment to play with\n",
    "    agent : QLearningAgent\n",
    "        The agent to play and train with\n",
    "    \"\"\"\n",
    "    \n",
    "    n_rows, n_cols = env._cliff.shape\n",
    "    actions = '^>v<'\n",
    "    \n",
    "    for yi in range(n_rows):\n",
    "        for xi in range(n_cols):\n",
    "            if env._cliff[yi, xi]:\n",
    "                print(\" C \", end='')\n",
    "            elif (yi * n_cols + xi) == env.start_state_index:\n",
    "                print(\" X \", end='')\n",
    "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
    "                print(\" T \", end='')\n",
    "            else:\n",
    "                print(\" %s \" % actions[agent.get_best_action(yi * n_cols + xi)], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q-Learning\")\n",
    "draw_policy(env, agent_ql)\n",
    "\n",
    "print(\"SARSA\")\n",
    "draw_policy(env, agent_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ''\n",
    "TOKEN = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_sarsa\n",
    "submit_sarsa(rewards_ql, rewards_sarsa, EMAIL, TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### More\n",
    "\n",
    "Here are some of the things you can do if you feel like it:\n",
    "\n",
    "* Play with epsilon. See learned how policies change if you set epsilon to higher/lower values (e.g. 0.75).\n",
    "* Expected Value SASRSA for softmax policy:\n",
    "$$ \\pi(a_i|s) = softmax({Q(s,a_i) \\over \\tau}) = {e ^ {Q(s,a_i)/ \\tau}  \\over {\\sum_{a_j}  e ^{Q(s,a_j) / \\tau }}} $$\n",
    "* Implement N-step algorithms and TD($\\lambda$): see [Sutton's book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) chapter 7 and chapter 12.\n",
    "* Use those algorithms to train on CartPole in previous / next assignment for this week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
