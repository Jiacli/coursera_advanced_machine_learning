{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digging deeper: approximate crossentropy with neural nets\n",
    "\n",
    "In this section we will train a neural network policy for continuous state space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cart_env = gym.make(\"CartPole-v0\").env  #if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "cart_env.reset()\n",
    "cart_n_actions = cart_env.action_space.n\n",
    "\n",
    "plt.imshow(cart_env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "cart_agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                           activation='tanh',\n",
    "                           warm_start=True, #keep progress between .fit(...) calls\n",
    "                           max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                          )\n",
    "\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "cart_initial_observation = cart_env.reset()\n",
    "cart_agent.fit(X=[cart_initial_observation]*cart_n_actions, \n",
    "               y=list(range(cart_n_actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, env, n_actions, t_max=1000):\n",
    "    \"\"\"\n",
    "    Generates sessions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    agent : MLPClassifier\n",
    "        The agent to use\n",
    "    env : gym.envs\n",
    "        The environment to use\n",
    "    n_actions : int\n",
    "        The number of actions\n",
    "    t_max : int\n",
    "        Maximum number of steps to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    states : list\n",
    "        The list of states\n",
    "    actions : list\n",
    "        Thes list of actions\n",
    "    total_reward : float\n",
    "        The sum of the reward\n",
    "    \"\"\"\n",
    "    \n",
    "    states = list()\n",
    "    actions = list()\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # a vector of action probabilities in current state\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = np.random.choice(n_actions, p=probs)\n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEM steps\n",
    "Deep CEM uses exactly the same strategy as the regular CEM, so you can copy your function code from previous notebook.\n",
    "\n",
    "The only difference is that now each observation is not a number but a float32 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch,\n",
    "                  actions_batch,\n",
    "                  rewards_batch,\n",
    "                  percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    It is not assumed that states are integers (they'll get different later)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    states_batch : list\n",
    "        List of list of states given as\n",
    "        >>> states_batch[session_i][t]\n",
    "        Where session_i is the session and t is the step\n",
    "    action_batch : list\n",
    "        List of list of actions given as\n",
    "        >>> actions_batch[session_i][t]\n",
    "        Where session_i is the session and t is the step\n",
    "    rewards_batch : list\n",
    "        List of rewards given in the sessions\n",
    "    percentile : float\n",
    "        The percentile to select the elites from\n",
    "        We are selecting states from games that have rewards >= percentile\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    elite_states : list\n",
    "        A list of the states where the elite actions took place\n",
    "        Sorted by  session number and timestep within session\n",
    "    elite_actions : list\n",
    "        A list of the elite actions\n",
    "        Sorted by  session number and timestep within session\n",
    "    \"\"\"\n",
    "    \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "    # NOTE: [0] as rewards_batch is a 1-d list\n",
    "    indices = np.where(rewards_batch >= reward_threshold)[0]\n",
    "    \n",
    "    elite_states  = [state for session, state_session in enumerate(states_batch) \n",
    "                     for state in state_session if session in indices]\n",
    "    elite_actions  = [action for session, action_session in enumerate(actions_batch) \n",
    "                      for action in action_session if session in indices]\n",
    "    \n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(batch_rewards, log, percentile, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward, threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(batch_rewards, range=reward_range);\n",
    "    plt.vlines([np.percentile(batch_rewards, percentile)], [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_n_sessions = 100\n",
    "cart_percentile = 70\n",
    "cart_log = []\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    cart_sessions = [generate_session(cart_agent, cart_env, cart_n_actions) for i in range(cart_n_sessions)]\n",
    "\n",
    "    cart_batch_states, cart_batch_actions, cart_batch_rewards = map(np.array, zip(*cart_sessions))\n",
    "\n",
    "    cart_elite_states, cart_elite_actions = select_elites(cart_batch_states,\n",
    "                                                          cart_batch_actions,\n",
    "                                                          cart_batch_rewards)\n",
    "    \n",
    "    # Train the network one epoch with the new information\n",
    "    cart_agent.fit(X=cart_elite_states, y=cart_elite_actions)    \n",
    "    show_progress(cart_batch_rewards, cart_log, cart_percentile, reward_range=[0, np.max(cart_batch_rewards)])\n",
    "    \n",
    "    if np.mean(cart_batch_rewards)> 190:\n",
    "        print(\"You Win! I'll stop for you.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "cart_v_env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True)\n",
    "cart_v_sessions = [generate_session(cart_agent, cart_v_env, cart_n_actions) for _ in range(100)]\n",
    "cart_v_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "_if you have any trouble with CartPole-v0 and feel stuck, take a look at the forums_\n",
    "\n",
    "* Pick one of environments: MountainCar-v0 or LunarLander-v2.\n",
    "  * For MountainCar, get average reward of __at least -150__\n",
    "  * For LunarLander, get average reward of __at least +50__\n",
    "\n",
    "See the tips section below, it's kinda important.\n",
    "__Note:__ If your agent is below the target score, you'll still get most of the points depending on the result, so don't be afraid to submit it.\n",
    "  \n",
    "  \n",
    "* Bonus quest: Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips & tricks\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ while R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the mountain car problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mountain_env = gym.make(\"MountainCar-v0\").env  #if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "mountain_env.reset()\n",
    "mountain_n_actions = mountain_env.action_space.n\n",
    "\n",
    "plt.imshow(mountain_env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_agent = MLPClassifier(hidden_layer_sizes=(20, 20),\n",
    "                               activation='tanh',\n",
    "                               warm_start=True, #keep progress between .fit(...) calls\n",
    "                               max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                              )\n",
    "\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "mountain_initial_observation = mountain_env.reset()\n",
    "mountain_agent.fit(X=[mountain_initial_observation]*mountain_n_actions, \n",
    "                   y=list(range(mountain_n_actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mountain_generate_session(t_max=10000):\n",
    "    \"\"\"\n",
    "    Generates sessions\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Although this is a programaticly bad function (due to the use of global variables),\n",
    "    this is what the grader accepts\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t_max : int\n",
    "        Maximum number of steps to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    states : list\n",
    "        The list of states\n",
    "    actions : list\n",
    "        Thes list of actions\n",
    "    total_reward : float\n",
    "        The sum of the reward\n",
    "    \"\"\"\n",
    "    \n",
    "    states = list()\n",
    "    actions = list()\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = mountain_env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # a vector of action probabilities in current state\n",
    "        probs = mountain_agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = np.random.choice(mountain_n_actions, p=probs)\n",
    "        \n",
    "        # NOTE: For each step taken, the reward is -1\n",
    "        new_s, r, done, info = mountain_env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_n_sessions = 100\n",
    "mountain_percentile = 70\n",
    "mountain_log = []\n",
    "mountain_t_max = 10000\n",
    "\n",
    "for i in range(350):\n",
    "    #generate new sessions\n",
    "    mountain_sessions = [mountain_generate_session(t_max=mountain_t_max)\n",
    "                         for i in range(mountain_n_sessions)]\n",
    "\n",
    "    mountain_batch_states, mountain_batch_actions, mountain_batch_rewards = map(np.array, zip(*mountain_sessions))\n",
    "\n",
    "    mountain_elite_states, mountain_elite_actions = select_elites(mountain_batch_states,\n",
    "                                                                  mountain_batch_actions,\n",
    "                                                                  mountain_batch_rewards,\n",
    "                                                                  mountain_percentile)\n",
    "    \n",
    "    threshold = np.percentile(mountain_batch_rewards, mountain_percentile)\n",
    "    reward_mean = np.mean(mountain_batch_rewards)\n",
    "    \n",
    "    if np.isclose(threshold, -mountain_t_max):\n",
    "        print('Not enough samples reached the destination, will not train')\n",
    "    elif threshold > -140 and reward_mean > -140:\n",
    "        print('You successfully trained the algorithm')\n",
    "        show_progress(mountain_batch_rewards, \n",
    "                      mountain_log, \n",
    "                      mountain_percentile, \n",
    "                      reward_range=[np.min(mountain_batch_rewards), np.max(mountain_batch_rewards)])\n",
    "        break\n",
    "    else:\n",
    "        # Train the network one epoch with the new information\n",
    "        mountain_agent.fit(X=mountain_elite_states, y=mountain_elite_actions)    \n",
    "    show_progress(mountain_batch_rewards, \n",
    "                  mountain_log, \n",
    "                  mountain_percentile, \n",
    "                  reward_range=[np.min(mountain_batch_rewards), np.max(mountain_batch_rewards)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "mountain_env = gym.wrappers.Monitor(gym.make(\"MountainCar-v0\"), directory=\"videos\", force=True)\n",
    "mountain_sessions = [mountain_generate_session() for _ in range(100)]\n",
    "mountain_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_mountain_car\n",
    "from functools import partial\n",
    "\n",
    "EMAIL = 'michael.l.magnussen@gmail.com'\n",
    "TOKEN = 'BDtRmu3qWqHjRkOU'\n",
    "\n",
    "submit_mountain_car(mountain_generate_session, EMAIL, TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
