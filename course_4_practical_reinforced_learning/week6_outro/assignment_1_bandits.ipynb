{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta \n",
    "from abc import abstractmethod\n",
    "from abc import abstractproperty\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Bandit\n",
    "\n",
    "We are going to implement several exploration strategies for simplest problem - bernoulli bandit.\n",
    "\n",
    "The bandit has $K$ actions. Action produce 1.0 reward $r$ with probability $0 \\le \\theta_k \\le 1$ which is unknown to agent, but fixed over time. Agent's objective is to minimize regret over fixed number $T$ of action selections:\n",
    "\n",
    "$$\\rho = T\\theta^* - \\sum_{t=1}^T r_t$$\n",
    "\n",
    "Where $\\theta^* = \\max_k\\{\\theta_k\\}$\n",
    "\n",
    "**Real-world analogy:**\n",
    "\n",
    "Clinical trials - we have $K$ pills and $T$ ill patient. After taking pill, patient is cured with probability $\\theta_k$. Task is to find most efficient pill.\n",
    "\n",
    "A research on clinical trials - https://arxiv.org/pdf/1507.08025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit:\n",
    "    \"\"\"\n",
    "    Implementation of the Bernoulli Bandit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=5):\n",
    "        \"\"\"\n",
    "        Constructor which sets the probability for reward\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_actions : int\n",
    "            How many action we are allowed to take\n",
    "        \"\"\"\n",
    "        self._probs = np.random.random(n_actions)\n",
    "    \n",
    "    # NOTE: The property decorator makes this function accessible as an attribute\n",
    "    #       https://www.python-course.eu/python3_properties.php\n",
    "    @property\n",
    "    def action_count(self):\n",
    "        \"\"\"\n",
    "        Counts the number of actions\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of actions\n",
    "        \"\"\"\n",
    "        return len(self._probs)\n",
    "    \n",
    "    def pull(self, action):\n",
    "        \"\"\"\n",
    "        Function for \"pulling the lever\"\n",
    "        \n",
    "        If the action probability is larger tha a random drawn number,\n",
    "        a reward of 1 is returned\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            The action to take (aka the lever to pull)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The reward (either 0.0 or 1.0)\n",
    "        \"\"\"\n",
    "        if np.random.random() > self._probs[action]:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    \n",
    "    def optimal_reward(self):\n",
    "        \"\"\"\n",
    "        Returns the highest probability of all the actions\n",
    "        \n",
    "        This is used for regret calculation\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The maximum probability of all the actions\n",
    "        \"\"\"\n",
    "        return np.max(self._probs)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\" \n",
    "        Used in nonstationary version\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "        Used in nonstationary version\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAgent(metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    The abstraction of the agents\n",
    "    \"\"\"\n",
    "    \n",
    "    def init_actions(self, n_actions):\n",
    "        \"\"\"\n",
    "        Initialization of the actions\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_actions : int\n",
    "            The number of possible actions\n",
    "        \"\"\"\n",
    "        \n",
    "        self._successes = np.zeros(n_actions)\n",
    "        self._failures = np.zeros(n_actions)\n",
    "        self._total_pulls = 0\n",
    "    \n",
    "    # NOTE: The abstractmethod decorator tells the child classes that this method must be defined\n",
    "    #       https://www.python-course.eu/python3_abstract_classes.php\n",
    "    @abstractmethod\n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get current best action\n",
    "        \n",
    "        The implementations should return an integer corresponding to the best action\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Observe reward from action and update agent's internal parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            The action taken\n",
    "        reward : int\n",
    "            The obtained return from the action\n",
    "        \"\"\"\n",
    "        self._total_pulls += 1\n",
    "        if reward == 1:\n",
    "            self._successes[action] += 1\n",
    "        else:\n",
    "            self._failures[action] += 1\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns the class name\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The class name\n",
    "        \"\"\"\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(AbstractAgent):\n",
    "    \"\"\"\n",
    "    The implementation of a random agent, which always chooses a random action\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Returns a random action\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        int\n",
    "            The chosen action\n",
    "        \"\"\"\n",
    "        return np.random.randint(0, len(self._successes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-greedy agent\n",
    "\n",
    "> **for** $t = 1,2,...$ **do**\n",
    ">> **for** $k = 1,...,K$ **do**\n",
    ">>> $\\hat\\theta_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
    "\n",
    ">> **end for** \n",
    "\n",
    ">> $x_t \\leftarrow argmax_{k}\\hat\\theta$ with probability $1 - \\epsilon$ or random action with probability $\\epsilon$\n",
    "\n",
    ">> Apply $x_t$ and observe $r_t$\n",
    "\n",
    ">> $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "> **end for**\n",
    "\n",
    "Implement the algorithm above in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(AbstractAgent):\n",
    "    \"\"\"\n",
    "    The implementation of the epsilon greedy agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Sets the epsilon\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "            The chance of taking a random action\n",
    "        \"\"\"\n",
    "        \n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get the action to take in an epsilon-greedy fashion\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The chosen action\n",
    "        \"\"\"\n",
    "        \n",
    "        epsilon = self._epsilon\n",
    "        n_actions = len(self._successes)\n",
    "        \n",
    "        # NOTE: Recall that self._success and self._failures are\n",
    "        #       arrays that counts how many times an action resulted in\n",
    "        #       a success and when it resulted in a failure\n",
    "        alpha = self._successes\n",
    "        beta = self._failures\n",
    "        \n",
    "        # NOTE: Add machine epsilon to avoid division by 0\n",
    "        theta_hat = alpha/(alpha + beta + np.finfo(float).eps)\n",
    "        \n",
    "        x = np.argmax(theta_hat)\n",
    "        \n",
    "        # Choose action according to the epsilon-greedy strategy\n",
    "        possibilities = [x] + list(range(n_actions))\n",
    "        probabilities = [1-epsilon] + [epsilon/n_actions]*n_actions\n",
    "        action = np.random.choice(possibilities, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns the class name with the epsilon appended\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The class name with the value of epsilon appended\n",
    "        \"\"\"\n",
    "        return self.__class__.__name__ + \"(epsilon={})\".format(self._epsilon) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB Agent\n",
    "Epsilon-greedy strategy heve no preference for actions. It would be better to select among actions that are uncertain or have potential to be optimal. One can come up with idea of index for each action that represents otimality and uncertainty at the same time. One efficient way to do it is to use UCB1 algorithm:\n",
    "\n",
    "> **for** $t = 1,2,...$ **do**\n",
    ">> **for** $k = 1,...,K$ **do**\n",
    ">>> $w_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k) + \\sqrt{2log\\ t \\ / \\ (\\alpha_k + \\beta_k)}$\n",
    "\n",
    ">> **end for** \n",
    "\n",
    ">> $x_t \\leftarrow argmax_{k}w$\n",
    "\n",
    ">> Apply $x_t$ and observe $r_t$\n",
    "\n",
    ">> $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "> **end for**\n",
    "\n",
    "\n",
    "__Note:__ in practice, one can multiply $\\sqrt{2log\\ t \\ / \\ (\\alpha_k + \\beta_k)}$ by some tunable parameter to regulate agent's optimism and wilingness to abandon non-promising actions.\n",
    "\n",
    "More versions and optimality analysis - https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent(AbstractAgent):\n",
    "    \"\"\"\n",
    "    The implementation of the UCB-1 agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get the action to take in an UCB-1 fashion\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The chosen action\n",
    "        \"\"\"\n",
    "                \n",
    "        # NOTE: Recall that self._success and self._failures are\n",
    "        #       arrays that counts how many times an action resulted in\n",
    "        #       a success and when it resulted in a failure\n",
    "        alpha = self._successes\n",
    "        beta = self._failures\n",
    "        \n",
    "        # NOTE: t is a scalar\n",
    "        t = self._total_pulls\n",
    "        \n",
    "        # NOTE: Adding machine epsilons to avoid division by zero\n",
    "        # NOTE: Adding 1 in the log in order to not take sqrt of a negative number\n",
    "        w = alpha/(alpha + beta + np.finfo(float).eps) +\\\n",
    "            np.sqrt(2*np.log(t+1)/(alpha + beta + np.finfo(float).eps))\n",
    "        \n",
    "        x = np.argmax(w)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns the class name\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The class name\n",
    "        \"\"\"\n",
    "            \n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson sampling\n",
    "\n",
    "UCB1 algorithm does not take into account actual distribution of rewards. If we know the distribution - we can do much better by using Thompson sampling:\n",
    "\n",
    "> **for** $t = 1,2,...$ **do**\n",
    ">> **for** $k = 1,...,K$ **do**\n",
    ">>> Sample $\\hat\\theta_k \\sim beta(\\alpha_k, \\beta_k)$\n",
    "\n",
    ">> **end for** \n",
    "\n",
    ">> $x_t \\leftarrow argmax_{k}\\hat\\theta$\n",
    "\n",
    ">> Apply $x_t$ and observe $r_t$\n",
    "\n",
    ">> $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "> **end for**\n",
    " \n",
    "\n",
    "More on Tompson Sampling:\n",
    "https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent(AbstractAgent):\n",
    "    \"\"\"\n",
    "    The implementation of the Thompson sampling agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get the action to take in a Thompson sampling fashion\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The chosen action\n",
    "        \"\"\"\n",
    "        \n",
    "                        \n",
    "        # NOTE: Recall that self._success and self._failures are\n",
    "        #       arrays that counts how many times an action resulted in\n",
    "        #       a success and when it resulted in a failure\n",
    "        alpha = self._successes\n",
    "        beta = self._failures\n",
    "        \n",
    "        # NOTE: We add 1 to all elements as alpha and beta < 0 in the beta distribution\n",
    "        p = np.random.beta(alpha+1, beta+1)\n",
    "        \n",
    "        return np.argmax(p)\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns the class name\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The class name\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def get_regret(env, agents, n_steps=5000, n_trials=50):\n",
    "    \"\"\"\n",
    "    Retruns the regret after n_trails\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : BernoulliBandit-like\n",
    "        The environment to play with\n",
    "    agent : AbstractAgent-like\n",
    "        An agent following the AbstractAgent\n",
    "    n_steps : int\n",
    "        How many times to pull the levels per trial\n",
    "    n_trials : int\n",
    "        How many trials to run\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    scores : OrderedDict\n",
    "        An ordered dict with the agent name as a key and the \n",
    "        normalized cumulative sum of the regret\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = OrderedDict({\n",
    "        agent.name : [0.0 for step in range(n_steps)] for agent in agents\n",
    "    })\n",
    "\n",
    "    for trial in tqdm_notebook(range(n_trials), desc='Trial'):\n",
    "        env.reset()\n",
    "        \n",
    "        for a in agents:\n",
    "            a.init_actions(env.action_count)\n",
    "\n",
    "        for i in tqdm_notebook(range(n_steps), desc='Step', leave=False):\n",
    "            optimal_reward = env.optimal_reward()\n",
    "            \n",
    "            for agent in agents:\n",
    "                action = agent.get_action()\n",
    "                reward = env.pull(action)\n",
    "                agent.update(action, reward)\n",
    "                scores[agent.name][i] += optimal_reward - reward\n",
    "                \n",
    "            env.step()  # change bandit's state if it is unstationary\n",
    "\n",
    "    for agent in agents:\n",
    "        scores[agent.name] = np.cumsum(scores[agent.name]) / n_trials\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regret(scores):\n",
    "    \"\"\"\n",
    "    Plots the regret\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scores : OrderedDict\n",
    "        An ordered dict with the agent name as a key and the \n",
    "        normalized cumulative sum of the regret    \n",
    "    \"\"\"\n",
    "    \n",
    "    for agent in agents:\n",
    "        plt.plot(scores[agent.name])\n",
    "\n",
    "    plt.legend([agent for agent in scores])\n",
    "    \n",
    "    plt.ylabel(\"regret\")\n",
    "    plt.xlabel(\"steps\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment agents\n",
    "agents = [\n",
    "     EpsilonGreedyAgent(),\n",
    "     UCBAgent(),\n",
    "     ThompsonSamplingAgent()\n",
    "]\n",
    "\n",
    "regret = get_regret(BernoulliBandit(), agents, n_steps=10000, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regret(regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ''\n",
    "TOKEN = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_bandits\n",
    "\n",
    "# NOTE: Due to randomness, it could be that you need to run get_regret a couple of times to get the scores to pass\n",
    "submit_bandits(regret, EMAIL, TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
