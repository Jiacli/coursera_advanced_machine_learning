{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from ops import BatchNorm\n",
    "from ops import conv2d\n",
    "from ops import lrelu\n",
    "from ops import deconv2d\n",
    "from ops import linear\n",
    "\n",
    "from utils import get_image\n",
    "from utils import save_images\n",
    "from utils import inverse_transform\n",
    "from utils import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'pkill -f .*tensorboard'\n",
    "subprocess.run(cmd.split(' '))\n",
    "cmd = 'tensorboard --logdir=logs --port=6060'\n",
    "process = subprocess.Popen(cmd.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to train GAN for generating faces and then we will make fun playing with it. Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the “adversarial”). One neural network, called the generator, generates new faces, while the other, the discriminator,  decides whether each instance of face it reviews belongs to the actual training dataset or not.\n",
    "\n",
    "Firstly download aligned faces of celebrities from [here](https://docs.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM) (or clone [this](https://github.com/carpedm20/DCGAN-tensorflow) repo and type `python download.py celebA`) and copy the files to `celebA` in the directory of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant variables below depends on your dataset and choosing of architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('.').joinpath('celebA') # Path to the dataset with celebA faces\n",
    "Z_DIM = 100 # Dimension of face's manifold\n",
    "GENERATOR_DENSE_SIZE = 64*8 # Length of first tensor in generator\n",
    "\n",
    "IMAGE_SIZE = 64 # Shapes of input image\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "N_CHANNELS = 3 # Number channels of input image\n",
    "\n",
    "MERGE_X = 8 # Number images in merged image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(DATA_PATH.is_dir()), 'Please, download aligned celebA to DATA_PATH folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define GAN. To do it, we need to define generator, discriminator and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tips on the architecture of the generator:\n",
    "1. The deeper is convolution, the less filters is using.\n",
    "2. Apply deconvolutions-relu layers to achieve input image shape.\n",
    "3. Use batch normalization before nonlinearity for speed and stability of learning.\n",
    "4. Use tanh activation at the end of network (in this case images should be scaled to [-1, 1])\n",
    "5. To force generator not to collapse and produce different outputs initialize bias with zero (see linear layer).\n",
    "\n",
    "Other useful tips: https://github.com/soumith/ganhacks. Example of architecture see below. You may also use defined layers from ops.py. <b> Please, use names for layers started with \"g\\_\" for generator and \"d_\" for discriminator.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/carpedm20/DCGAN-tensorflow/master/DCGAN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing generator function (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, is_training):\n",
    "    \"\"\"\n",
    "    Creates the generator which generates images from noise\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array-like, shape (100,)\n",
    "        The random input to generate images from\n",
    "    is_training : bool\n",
    "        Whether we are training or predicting\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tensor, shape (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "        The resulting image from the generator\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    https://arxiv.org/pdf/1511.06434.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Firstly let's reshape input vector into 3-d tensor.\n",
    "    # NOTE: linear is defined in ops.py, and is equivalent to a Dense layer\n",
    "    z_ = linear(z, GENERATOR_DENSE_SIZE * 4 * 4, 'g_h0_lin')\n",
    "    # Reshape the output from the dense layer\n",
    "    # NOTE: The -1 denotes that the rest of the shape should be infered\n",
    "    # NOTE: This is a rank 4 tensor\n",
    "    h_in = tf.reshape(z_, [-1, 4, 4, GENERATOR_DENSE_SIZE])\n",
    "    # NOTE: ReLU before batch normalization is disputed in general\n",
    "    # https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/\n",
    "    h_in_b = BatchNorm(epsilon=1e-3, momentum = 0.99, name='g_h0_b')(h_in, is_training)\n",
    "    h_in_a = tf.nn.relu(h_in_b, name='g_h0_a')\n",
    "    \n",
    "    h1 = deconv2d(h_in_a, \n",
    "                  [BATCH_SIZE, 8, 8, 512],\n",
    "                  k_h=5, \n",
    "                  k_w=5, \n",
    "                  d_h=2, \n",
    "                  d_w=2, \n",
    "                  name='g_h1_a_deconv2d')\n",
    "    h1_b = BatchNorm(epsilon=1e-5, momentum = 0.9, name='g_h1_b')(h1, is_training)\n",
    "    h1_a = tf.nn.relu(h1_b, name='g_h1_a')\n",
    "    \n",
    "    h2 = deconv2d(h1_a, \n",
    "                  [BATCH_SIZE, 16, 16, 256],\n",
    "                  k_h=5, \n",
    "                  k_w=5, \n",
    "                  d_h=2, \n",
    "                  d_w=2,\n",
    "                  name='g_h2_deconv2d')\n",
    "    h2_b = BatchNorm(epsilon=1e-5, momentum = 0.9, name='g_h2_b')(h2, is_training)\n",
    "    h2_a = tf.nn.relu(h2_b, name='g_h2_a')\n",
    "    \n",
    "    h3 = deconv2d(h2_a, \n",
    "                  [BATCH_SIZE, 32, 32, 128],\n",
    "                  k_h=5, \n",
    "                  k_w=5, \n",
    "                  d_h=2, \n",
    "                  d_w=2,\n",
    "                  name='g_h3_deconv2d')\n",
    "    h3_b = BatchNorm(epsilon=1e-5, momentum = 0.9, name='g_h3_b')(h3, is_training)\n",
    "    h3_a = tf.nn.relu(h3_b, name='g_h3_a')\n",
    "    \n",
    "    # Last layer of G and first of D are not Batch normalised, \n",
    "    # so that model can learn correct mean and scale of data distribution\n",
    "    h_out = deconv2d(h3_a,\n",
    "                     [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS],\n",
    "                     k_h=5, \n",
    "                     k_w=5, \n",
    "                     d_h=2, \n",
    "                     d_w=2,\n",
    "                     name='g_out')\n",
    "\n",
    "    return tf.nn.tanh(h_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard can be accessed at http://localhost:6060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tensorboard\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "    with tf.variable_scope(\"G\") as scope:\n",
    "        z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n",
    "        G = generator(z, is_training)\n",
    "\n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define discriminator. Discriminator takes 3d tensor as input and outputs one number - probability that this is an image.\n",
    "\n",
    "Some advice for discriminator's architecture:\n",
    "1. Use batch normalization between convolutions and nonlinearities.\n",
    "2. Use leaky relu with the leak about 0.2.\n",
    "3. The deeper the layer, the more filters you can use.\n",
    "\n",
    "If you use batch normalization, please define every layer in their own scope and pass is_training parameter there. Or you may use class of batch normalization from ops.py. Do not forget to flatten tensor after the convolution blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing discriminator function (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(image, is_training):\n",
    "    \"\"\"\n",
    "    Creates the generator which generates images from noise\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : Tensor, shape (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "        Either a real image, or an image from a generator\n",
    "    is_training : bool\n",
    "        Whether we are training or predicting\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    probabilities : Tensor, shape (2,)\n",
    "        The probabilities that the the image is real or fake\n",
    "    linear_out : Tensor, shape (2,)\n",
    "        The linear_output before applying the sigmoid funtion\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    https://arxiv.org/abs/1609.04468 \n",
    "    \"\"\"\n",
    "\n",
    "    # Last layer of G and first of D are not Batch normalised, \n",
    "    # so that model can learn correct mean and scale of data distribution\n",
    "    h0 = conv2d(image, 64, k_h=5, k_w=5, d_h=2, d_w=2, name='d_h0_conv2d')\n",
    "    h0_a = lrelu(h0, leak=0.2, name='d_h0_a')\n",
    "    \n",
    "    h1 = conv2d(h0_a, 128, k_h=5, k_w=5, d_h=2, d_w=2, name='d_h1_conv2d')\n",
    "    h1_b = BatchNorm(epsilon=1e-5, momentum = 0.9, name='d_h1_b')(h1, is_training)\n",
    "    h1_a = lrelu(h1_b, leak=0.2, name='d_h1_a')\n",
    "\n",
    "    h2 = conv2d(h1_a, 256, k_h=5, k_w=5, d_h=2, d_w=2, name='d_h2_conv2d')\n",
    "    h2_b = BatchNorm(epsilon=1e-5, momentum = 0.9, name='d_h2_b')(h2, is_training)\n",
    "    h2_a = lrelu(h2_b, leak=0.2, name='d_h2_a')\n",
    "    \n",
    "    h3 = conv2d(h2_a, 512, k_h=5, k_w=5, d_h=2, d_w=2, name='d_h3_conv2d')\n",
    "    h3_b = BatchNorm(epsilon=1e-5, momentum = 0.9, name='d_h3_b')(h3, is_training)\n",
    "    h3_a = lrelu(h3_b, leak=0.2, name='d_h3_a')    \n",
    "    \n",
    "    h2_f = tf.contrib.layers.flatten(h3_a)\n",
    "    \n",
    "    linear_out = linear(h2_f, 1, 'linear_out') \n",
    "    \n",
    "    return tf.nn.sigmoid(linear_out), linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tensorboard\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    with tf.variable_scope(\"G\") as scope:\n",
    "        z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n",
    "        G = generator(z, is_training)\n",
    "    \n",
    "    with tf.variable_scope('D') as scope:\n",
    "        images = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS])\n",
    "        D_real, D_real_logits = discriminator(images, is_training)\n",
    "        \n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "with tf.variable_scope(\"G\") as scope:\n",
    "    z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n",
    "    G = generator(z, is_training)\n",
    "\n",
    "with tf.variable_scope('D') as scope:\n",
    "    images = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS])\n",
    "    D_real, D_real_logits = discriminator(images, is_training)\n",
    "    scope.reuse_variables()\n",
    "    D_fake, D_fake_logits = discriminator(G, is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write definition of loss funstions according to formulas:\n",
    "$$ D\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m}[\\log{D(x_i)} + \\log{(1 - D(G(z_i)))}]$$\n",
    "$$ G\\_loss = \\frac{1}{m} \\sum_{i=1}^{m} \\log{(1 - D(G(z_i)))}$$\n",
    "\n",
    "Or for better learning you may try other loss for generator:\n",
    "$$ G\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m} \\log{(D(G(z_i)))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing loss functions (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: tf.reduce_mean is the average\n",
    "# NOTE: The following loss has been found to give better stability\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, \n",
    "                                                                     labels=tf.ones_like(D_real)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, \n",
    "                                                                     labels=tf.zeros_like(D_fake)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, \n",
    "                                                                labels=tf.ones_like(D_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizers. We use different optimizers for discriminator and generator, so we needed a separate prefix for the discriminator and generator variables (g_ for generator, d_ for disciminator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "## All variables of discriminator\n",
    "d_vars = [v for v in tvars if 'd_' in v.name]\n",
    "\n",
    "## All variables of generator\n",
    "g_vars = [v for v in tvars if 'g_' in v.name]\n",
    "\n",
    "LEARNING_RATE = 0.0002 # Learning rate for adam optimizer\n",
    "BETA = 0.5 # Beta paramater in adam optimizer\n",
    "\n",
    "##Optimizers - ypu may use your favourite instead.\n",
    "d_optim = tf.train.AdamOptimizer(LEARNING_RATE, beta1=BETA) \\\n",
    "                  .minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(LEARNING_RATE, beta1=BETA) \\\n",
    "                  .minimize(g_loss, var_list=g_vars) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sorted(DATA_PATH.glob('*.jpg'))\n",
    "assert(len(data) > 0), 'Length of training data should be more than zero'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training and evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(sess, load_dir):\n",
    "    \"\"\"\n",
    "    Loads the network's paramaters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sess : Session\n",
    "        The session to load for\n",
    "    load_dir : Path or str\n",
    "        Path to the load directory\n",
    "    \"\"\"\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(str(load_dir))\n",
    "    \n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN (1 point + 2 for good results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = inverse_transform(get_image(data[np.random.randint(len(data))], IMAGE_SIZE, IMAGE_SIZE))\n",
    "fig, ax = plt.subplots()\n",
    "# NOTE: [..., ::-1] converts from BGR to RGB\n",
    "ax.imshow(sample[...,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_and_batch(sample_dir):\n",
    "    \"\"\"\n",
    "    Obtains the current epoch and batch from the sample directory\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    save_frequency must be equal to sample_frequency in `train` for this to work\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    save_dir: Path or str\n",
    "        Path where to save parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cur_epoch : int\n",
    "        Integer of the epoch last saved\n",
    "    cur_batch : int\n",
    "        Integer of the last batch saved\n",
    "    \"\"\"\n",
    "    \n",
    "    images = sorted(sample_dir.glob('*.png'))\n",
    "    if len(images) == 0:\n",
    "        return 0, 0\n",
    "    else:\n",
    "        last_save = images[-1]\n",
    "        _, cur_epoch, cur_batch = last_save.name.split('.png')[0].split('_')\n",
    "        return int(cur_epoch), int(cur_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess,\n",
    "          load_dir=None, \n",
    "          save_frequency=100, \n",
    "          sample_frequency=100, \n",
    "          sample_dir='sample_faces',\n",
    "          save_dir='checkpoint', \n",
    "          max_to_keep=3, \n",
    "          model_name='dcgan.model',\n",
    "          n_epochs=25,\n",
    "          n_generator_update=2):\n",
    "    \"\"\"\n",
    "    Trains the GAN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sess : Session\n",
    "        The session to train for\n",
    "    load_dir : None or Path or str\n",
    "        Path to directory with the parameters\n",
    "    save_frequency: int\n",
    "        How often save parameters\n",
    "    sample_frequency: int\n",
    "        How often sample faces\n",
    "    sample_dir: Path or str\n",
    "        Directory for generated images\n",
    "    save_dir: Path or str\n",
    "        Path where to save parameters\n",
    "    max_to_keep: int\n",
    "        How many last checkpoints to store\n",
    "    model_name: str\n",
    "        Name of model\n",
    "    n_epochs: int\n",
    "        Number epochs to train\n",
    "    n_generator_update: int\n",
    "        How many times run generator updates per one discriminator update\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_frequency is not None:\n",
    "        saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "        \n",
    "    if load_dir is not None and load_dir.joinpath('checkpoint').is_file():\n",
    "        print('Reading checkpoints...')\n",
    "        try:\n",
    "            load(sess, str(load_dir))\n",
    "            add_to_epoch, add_to_batch = get_epoch_and_batch(sample_dir)\n",
    "            print('Loaded checkpoints')\n",
    "        except:\n",
    "            print('WARNING! Reading checkpoints failed')\n",
    "            add_to_epoch = 0\n",
    "            add_to_batch = 0\n",
    "            try:\n",
    "                tf.global_variables_initializer().run()\n",
    "            except:\n",
    "                tf.initialize_all_variables().run()\n",
    "    else:\n",
    "        add_to_epoch = 0\n",
    "        add_to_batch = 0\n",
    "        try:\n",
    "            tf.global_variables_initializer().run()\n",
    "        except:\n",
    "            tf.initialize_all_variables().run()\n",
    "\n",
    "    counter=1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(n_epochs), desc='Epoch'):\n",
    "        if epoch < add_to_epoch:\n",
    "            continue\n",
    "        batch_idxs = min(len(data), np.inf) // BATCH_SIZE\n",
    "        # Store last epoch in own directory\n",
    "        new_dir = Path('.').joinpath(f'epoch_{epoch-1}_{time.strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "        new_dir.mkdir()\n",
    "        old_files = Path(save_dir).glob('*')\n",
    "        for f in old_files:\n",
    "            shutil.copy(f, new_dir.joinpath(f.name))\n",
    "            print(f'Copied {f} to {new_dir.joinpath(f.name)}')\n",
    "        for idx in tqdm_notebook(range(0, batch_idxs), desc='Batch', leave=False):\n",
    "            if idx <= add_to_batch:\n",
    "                continue\n",
    "            # Reset add to batch\n",
    "            add_to_batch = -1\n",
    "            batch_files = data[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE]\n",
    "            batch = [get_image(batch_file, IMAGE_SIZE) for batch_file in batch_files]\n",
    "            batch_images = np.array(batch).astype(np.float32)\n",
    "            batch_z = np.random.uniform(-1, 1, [BATCH_SIZE, Z_DIM]).astype(np.float32)\n",
    "\n",
    "            # Update D network\n",
    "            sess.run(d_optim, feed_dict={images: batch_images, z: batch_z, is_training: True})\n",
    "\n",
    "            # Update G network\n",
    "            for _ in range(n_generator_update):\n",
    "                sess.run(g_optim, feed_dict={z: batch_z, is_training: True})\n",
    "\n",
    "            errD_fake = d_loss_fake.eval({z: batch_z, is_training: False})\n",
    "            errD_real = d_loss_real.eval({images: batch_images, is_training: False})\n",
    "            errG = g_loss.eval({z: batch_z, is_training: False})\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            print('Epoch: [{:2d}] [{:4d}/{:4d}] time: {:4.4f}, d_loss: {:.8f}, g_loss: {:.8f}'.format(\n",
    "                  epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG))\n",
    "\n",
    "            if np.mod(counter, save_frequency) == 1:\n",
    "                print('Saved model')\n",
    "                saver.save(sess, str(save_dir.joinpath(model_name)))\n",
    "\n",
    "            if np.mod(counter, sample_frequency) == 1:\n",
    "                samples = sess.run(G, feed_dict={z: batch_z, is_training: False} )\n",
    "                save_images(samples,\n",
    "                            [MERGE_X, BATCH_SIZE//MERGE_X],\n",
    "                            str(sample_dir.joinpath('train_{:02d}_{:04d}.png'.format(epoch, idx))))\n",
    "                print('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path('.').absolute().joinpath('checkpoint')\n",
    "sample_dir = Path('.').absolute().joinpath('sample_faces')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train(sess, load_dir=save_dir, save_dir=save_dir, sample_dir=sample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you generated something that looks like a face - it's cool! Add 2 points to your mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face interpolation (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's interpolate between faces: generate two vectors $z_1$ and $z_2$ and get a batch of vectors of the form $\\alpha\\cdot z_1 + (1- \\alpha)\\cdot  z_2, \\alpha \\in [0,1].$ Generate faces on them and look at results. The generator displays pictures in the range from -1 to 1, so use the inverse transform function from the file utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(path):\n",
    "    \"\"\"\n",
    "    Displays the image located at the path\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path or str\n",
    "        Path to image\n",
    "    \"\"\"\n",
    "    \n",
    "    images = imread(path)\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    # NOTE: [..., ::-1] converts from BGR to RGB\n",
    "    ax.imshow(images[..., ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_dir = Path('.').absolute().joinpath('sample_faces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_interpolation_image = faces_dir.joinpath('face_interpolation.png')\n",
    "face_interpolation_data = faces_dir.joinpath('face_interpolation.npy')\n",
    "\n",
    "if not face_interpolation_image.is_file():\n",
    "    # Reseed\n",
    "    np.random.seed(20)\n",
    "    z_1 = np.random.uniform(-1, 1, Z_DIM).astype(np.float32)\n",
    "    z_2 = np.random.uniform(-1, 1, Z_DIM).astype(np.float32)\n",
    "    # NOTE: As Batch size is 64, we will vary alpha from 0 to 1 in 64 steps\n",
    "    batchz = [alpha*z_1 + (1-alpha)*z_2 for alpha in np.linspace(0, 1, 64)]\n",
    "    \n",
    "    # Feed batchz to obtain images\n",
    "    with tf.Session() as sess:\n",
    "        load(sess, './checkpoint')\n",
    "        generated_image = sess.run(G, feed_dict={z: batchz, is_training: False})\n",
    "    np.save(face_interpolation_data, generated_image)\n",
    "\n",
    "generated_image = np.load(face_interpolation_data)        \n",
    "# Utils are made so that it's easiest to use save_images to save the image and restore it again\n",
    "save_images(generated_image, [MERGE_X, BATCH_SIZE//MERGE_X], face_interpolation_image)   \n",
    "display(face_interpolation_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a smile (1 point + 1 point for good results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's make face smiling. Find several vectors z, such that the generator generates smiling faces and not. Five vectors in every group should be enough (but the more, the better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate \"smile vector\" as mean of vectors z with generated smile on it minus mean of vectors z with generated not smile on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the result of applying the smile vector: compare the results of generation before and after the addition of the smile vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_faces_image = faces_dir.joinpath('random_faces.png')\n",
    "random_faces_data = faces_dir.joinpath('random_faces.npy')\n",
    "\n",
    "if not random_faces_data.is_file():\n",
    "    # Reseed\n",
    "    np.random.seed(0)\n",
    "    random_batch = np.random.uniform(-1, 1, [BATCH_SIZE, Z_DIM]).astype(np.float32)\n",
    "    with tf.Session() as sess:\n",
    "        load(sess, './checkpoint')\n",
    "        random_images = sess.run(G, feed_dict={z: random_batch, is_training: False})\n",
    "    np.save(random_faces_data, random_images)\n",
    "\n",
    "random_images = np.load(random_faces_data)\n",
    "save_images(random_images, [MERGE_X, BATCH_SIZE//MERGE_X], random_faces_image)\n",
    "display(random_faces_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the indexes of similing and non-smiling faces manually. Counting starts from top left corner and runs towards right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_image = faces_dir.joinpath('smile.png')\n",
    "smile_data = faces_dir.joinpath('smile.npy')\n",
    "\n",
    "if not smile_data.is_file():\n",
    "    smiling_indices = [0, 1, 6, 7, \n",
    "                       8, 10, 13, 14,\n",
    "                       17, 18, 20,\n",
    "                       30,\n",
    "                       33, 34, 38,\n",
    "                       40, 47,\n",
    "                       49, 51, 52, 53, 54,\n",
    "                       60, 61, 62, 63\n",
    "                      ]\n",
    "    non_smiling_indices = [3, 4, 5,\n",
    "                           11, 12,\n",
    "                           19, 21,\n",
    "                           24, 26, 27, 28,\n",
    "                           32, 35, 39,\n",
    "                           44, 45, 46,\n",
    "                           55,\n",
    "                           56, 59\n",
    "                          ]\n",
    "    \n",
    "    avg_smile = np.mean(random_batch[smiling_indices, :], axis=0)\n",
    "    avg_non_smile = np.mean(random_batch[non_smiling_indices, :], axis=0)\n",
    "    \n",
    "    # We use the same interpolation strategy as before\n",
    "    smile_batch = [alpha*avg_smile + (1-alpha)*avg_non_smile for alpha in np.linspace(0, 1, 64)]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        load(sess, './checkpoint')\n",
    "        smile_images = sess.run(G, feed_dict={z: smile_batch, is_training: False})\n",
    "    np.save(smile_data, smile_images)\n",
    "\n",
    "smile_images = np.load(smile_data)\n",
    "save_images(smile_images, [MERGE_X, BATCH_SIZE//MERGE_X], smile_image)\n",
    "display(smile_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If faces looks really cool, add bonus 1 point to your score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
