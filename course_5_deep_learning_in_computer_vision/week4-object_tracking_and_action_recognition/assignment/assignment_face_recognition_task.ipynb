{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition using neural network features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you have to construct face recognizer based on features extracted from the neural network. The task consists of two parts: image classification and video classification. In the first one you should classify distinct images and in the second one you will deal with short video sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import dlib\n",
    "import cv2\n",
    "import requests\n",
    "import bz2\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from skimage.transform import resize\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you have you have to read the data. Run the cell below to unpack data. The data can be obtained from\n",
    "this [link](https://d3c33hcgiwev3.cloudfront.net/blRYxTKUEeiISxJZ7npQ3g_6eb16bf0329411e8905a51c51e77c61e_face-recognition-task.zip?Expires=1539820800&Signature=WC-OWrQOumffTroH5U-QimCCAhMEXRx~u6Eit8uDcPXGUK2c4Rp7i9QSkzTwZK9kVyVYu6IEzoVT3BgyU5zr6NhBPRHUwDASA4lq6momTDrbpg6FKqWc9TEo2-cXZfbejVGgjkEwihe4gbx2pPiDDysj2OGxHJWftqCppyPUfJk_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dir = Path('.').joinpath('Face_Recognition_data')\n",
    "if not face_dir.is_dir():\n",
    "    raise RuntimeError('Please download the dataset and expand the zip to the current directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data for image and video classification (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function $\\tt{load}$\\_$\\tt{image}$\\_$\\tt{data}$. It should return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing ones. The keys of the dictionaries are the names of the image files and the values are 3-dimensional numpy arrays (for images) or strings with the names (for labels).\n",
    "\n",
    "$\\tt{dir}$\\_$\\tt{name}$ is the name of directory with data for image classification. If 'Face_Recofnition_data' directory is located in the same directory as this notebook, then the default value can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(dir_name = face_dir.joinpath('image_classification')):\n",
    "    \"\"\"\n",
    "    Loads the image data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_name : Path or str\n",
    "        Path to the images\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_train : dict\n",
    "        The training data.\n",
    "        A dictionary where the keys are the image names and the \n",
    "        values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_train : dict\n",
    "        The training labels.\n",
    "        A dictionary where the keys are image names and the values are the class_is as strings.\n",
    "    x_test : dict\n",
    "        The test data.\n",
    "        A dictionary where the keys are the image names and the \n",
    "        values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_test : dict\n",
    "        The test labels.\n",
    "        A dictionary where the keys are image names and the values are the class_ids as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train = dict()\n",
    "    x_test = dict()\n",
    "\n",
    "    train_dir = Path(dir_name).joinpath('train')\n",
    "    test_dir = Path(dir_name).joinpath('test')\n",
    "    \n",
    "    train_files = train_dir.joinpath('images').glob('*.jpg')\n",
    "    for f in tqdm_notebook(train_files, desc='train'):\n",
    "        # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "        x_train[f.name] = cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "        \n",
    "    y_train = dict(pd.read_csv(train_dir.joinpath('y_train.csv')).values)\n",
    "    \n",
    "    test_files = test_dir.joinpath('images').glob('*.jpg')\n",
    "    for f in tqdm_notebook(test_files, desc='test'):\n",
    "        # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "        x_test[f.name] = cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "        \n",
    "    y_test = dict(pd.read_csv(test_dir.joinpath('y_test.csv')).values)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_image_data()\n",
    "print(f'{len(x_train)} \\ttraining images')\n",
    "print(f'{len(x_test)} \\ttesting images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data, labels, function = lambda x:x[0], n_cols = 5, n_rows=1):\n",
    "    figure(figsize = (3*n_cols,3*n_rows))\n",
    "    for n, i in enumerate(np.random.choice(list(data.keys()), size = n_cols*n_rows)):\n",
    "        plt.subplot(n_rows, n_cols, n+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(function([data[i]]))\n",
    "        plt.title(labels[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x_train, y_train)\n",
    "visualize(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now read the video classification data, as well. You have to implement function to load video data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function $\\tt{load}$\\_$\\tt{video}$\\_$\\tt{data}$ should also return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing videos and labels. The training data is in the same format as in image classification task. The keys of testing data and labels are video ids and the values are the lists of frames and the strings with names, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_data(dir_name = face_dir.joinpath('video_classification')):\n",
    "    \"\"\"\n",
    "    Loads the video data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_name : Path or str\n",
    "        Path to the images\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_train : dict\n",
    "        The training data.\n",
    "        A dictionary where the keys are the image names and the \n",
    "        values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_train : dict\n",
    "        The training labels.\n",
    "        A dictionary where the keys are image names and the values are the class_is as strings.\n",
    "    x_test : dict\n",
    "        The test data.\n",
    "        A dictionary where the keys are the video ids and the values are lists of \n",
    "        numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_test : dict\n",
    "        The test labels.\n",
    "        A dictionary where the keys are image names and the values are the class_ids as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train = dict()\n",
    "    x_test = dict()\n",
    "\n",
    "    train_dir = Path(dir_name).joinpath('train')\n",
    "    test_dir = Path(dir_name).joinpath('test')\n",
    "    \n",
    "    train_files = train_dir.joinpath('images').glob('*.jpg')\n",
    "    for f in tqdm_notebook(train_files, desc='train'):\n",
    "        # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "        x_train[f.name] = cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "        \n",
    "    y_train = dict(pd.read_csv(train_dir.joinpath('y_train.csv')).values)\n",
    "    \n",
    "    # NOTE: The first directory is 'this' dir\n",
    "    test_dirs = sorted(test_dir.joinpath('videos').glob('**'))[1:]\n",
    "    for d in tqdm_notebook(test_dirs, desc='test_dir'):\n",
    "        images = []\n",
    "        image_paths = sorted(d.glob('*.jpg'))\n",
    "        for f in tqdm_notebook(image_paths, desc='test_img', leave=False):\n",
    "            # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "            images.append(cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1])\n",
    "        \n",
    "        x_test[d.name] = images\n",
    "        \n",
    "    y_test = dict(pd.read_csv(test_dir.joinpath('y_test.csv')).values.astype(str))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "video_train, train_labels, video_test, test_labels = load_video_data()\n",
    "print(f'{len(video_train)} \\ttraining images')\n",
    "print(f'{len(video_test)} \\ttesting videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(video_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize({i:video_test[i][1] for i in video_test}, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (3 points)\n",
    "You have to implement preprocessing function in the cell below.\n",
    "Getting a list of images as an input the this function should detect the face on each image, find the facial keypoints () and then crop and normalize the image according to these keypoints. The output of this function is the list of images which contain only the aligned face and should be converted to the tensor of the shape $(N, 224, 224, 3)\\ $ where $N$ is the length of the initial list. You can add extra arguments to the preprocess function if necessary (i.e. flag $\\tt{is}$\\_$\\tt{video}$ to determine if the list of images is video sequence or not).\n",
    "\n",
    "For face detection and facial keypoint regression you can use your models from the previous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** In order to ensure good quality, we use the dlib toolkit to get the facial landmarks. See http://dlib.net/face_landmark_detection.py.html and https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/ for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
    "landmark_file = Path('.').joinpath('shape_predictor_68_face_landmarks.dat')\n",
    "landmark_compressed_file = Path('.').joinpath('shape_predictor_68_face_landmarks.dat.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not landmark_compressed_file.is_file():\n",
    "    print(f'Downloading {landmark_compressed_file}')\n",
    "    response = requests.get(url)\n",
    "    with landmark_compressed_file.open('wb') as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print(f'{landmark_compressed_file} found') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not landmark_file.is_file():\n",
    "    print(f'Decompressing {landmark_compressed_file}')\n",
    "    with landmark_compressed_file.open('rb') as src_f, landmark_file.open('wb') as dst_f:\n",
    "        dst_f.write(bz2.decompress(src_f.read()))\n",
    "else:\n",
    "    print(f'{landmark_file} found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_to_np(landmarks):\n",
    "    \"\"\"\n",
    "    Convert landmarks to a numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    landmarks : dlib.full_object_detection\n",
    "        Detected landmarks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np_landmarks : np.array, shape (68, 2)\n",
    "        The landmarks converted to numpy\n",
    "        There are 68 landmarks, each with an (x, y) coordinate\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    https://github.com/jrosebr1/imutils/blob/master/imutils/face_utils/helpers.py\n",
    "    \"\"\"\n",
    "    \n",
    "    np_landmarks = np.zeros((landmarks.num_parts, 2), dtype=int)\n",
    "\n",
    "    for i in range(0, landmarks.num_parts):\n",
    "        np_landmarks[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    "\n",
    "    return np_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eyes(landmarks):\n",
    "    \"\"\"\n",
    "    Return a tuple of the eyes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    landmarks : np.array, shape (68, 2)\n",
    "        The landmarks converted to numpy\n",
    "        There are 68 landmarks, each with an (x, y) coordinate \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eyes : tuple, len (2,)\n",
    "        A tuple consisting of the eyes on the form\n",
    "        >>> (eye_1, eye_2)\n",
    "        Both eye_1 and eye_2 are np.arrays with len 2, \n",
    "        consisting of the x-coordinate and the y-coordinate of the eye\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: The indexing of the keypoints can be found here:\n",
    "    # https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg\n",
    "    # NOTE: Recall that python starts counting from 0, hence off-by-one with respect to facial landmarks\n",
    "    # Find the midpoints of the eyes\n",
    "    left_eye_center = np.round(landmarks[[36, 37, 38, 39, 40, 41], :].mean(axis=0)).astype(int)\n",
    "    right_eye_center = np.round(landmarks[[42, 43, 44, 45, 46, 47], :].mean(axis=0)).astype(int)\n",
    "\n",
    "    eyes = (left_eye_center, right_eye_center)\n",
    "    \n",
    "    return eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eyes_angle(eyes):\n",
    "    \"\"\"\n",
    "    Returns the angle between a one of the eyes and a horizontal line crossing the second eye\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eyes : tuple, len (2,)\n",
    "        A tuple consisting of the eyes on the form\n",
    "        >>> (eye_1, eye_2)\n",
    "        Both eye_1 and eye_2 are np.arrays with len 2, \n",
    "        consisting of the x-coordinate and the y-coordinate of the eye\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eyes_angle_deg : float\n",
    "        The angle given in degrees\n",
    "    \"\"\"\n",
    "    # Find the angle between the segment connecting two eyes and horizontal line\n",
    "    eye_1, eye_2 = eyes\n",
    "    eye_1_x_pos = eye_1[0]\n",
    "    eye_2_x_pos = eye_2[0]\n",
    "    \n",
    "    # Identify the left and the rigth eye\n",
    "    if eye_1_x_pos < eye_2_x_pos:\n",
    "        left_eye = np.array(eye_1)\n",
    "        right_eye = np.array(eye_2)\n",
    "    else:\n",
    "        left_eye = np.array(eye_2)\n",
    "        right_eye = np.array(eye_1)\n",
    "        \n",
    "    # Construct the horizontal point\n",
    "    horizontal_form_left_eye = np.array((right_eye[0], left_eye[1]))\n",
    "    \n",
    "    # Calculate the vectors to find the angle between\n",
    "    l_to_h = horizontal_form_left_eye - left_eye\n",
    "    l_to_r = right_eye - left_eye\n",
    "    \n",
    "    # Calculate the angle in radians\n",
    "    eyes_angle_rad = \\\n",
    "        np.arccos(np.dot(l_to_h, l_to_r)/\\\n",
    "                  (np.linalg.norm(l_to_h)*np.linalg.norm(l_to_r)))\n",
    "    \n",
    "    # The angle will depend of which eye is on top\n",
    "    # We define the positive angle between the horizontal line and the rigth eye \n",
    "    if left_eye[1] > right_eye[1]:\n",
    "        eyes_angle_rad = - eyes_angle_rad\n",
    "    \n",
    "    # Convert to degrees\n",
    "    eyes_angle_deg = eyes_angle_rad * 180/np.pi\n",
    "    \n",
    "    return eyes_angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_rectangle_to_np(rectangle):\n",
    "    \"\"\"\n",
    "    Convert the rectangle to a numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rectangle : rectangle\n",
    "        Rectangle prediction from dlib\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np_rectangle : np.array, shape (4,)\n",
    "        The rectangle on the form (top, bottom, left, right)\n",
    "    \"\"\"\n",
    "    \n",
    "    np_rectangle = np.array([rectangle.top(), rectangle.bottom(), rectangle.left(), rectangle.right()])\n",
    "    return np_rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_indices(img_shape, eyes):\n",
    "    \"\"\"\n",
    "    Returns the cropping indices\n",
    "    \n",
    "    The algorithms tries to center the eyes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_shape : tuple, len (3,)\n",
    "        Image to be processed\n",
    "    bbox : np.array, shape(4,)\n",
    "        The bounding box on the form (top, bottom, left, right)\n",
    "    eyes : tuple, len (2,)\n",
    "        A tuple consisting of the eyes on the form\n",
    "        >>> (eye_1, eye_2)\n",
    "        Both eye_1 and eye_2 are np.arrays with len 2, \n",
    "        consisting of the x-coordinate and the y-coordinate of the eye\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    crop_indices : np.array, shape (4,)\n",
    "        The cropping indices on the form (top, bottom, left, right) \n",
    "    \"\"\"\n",
    "    \n",
    "    # In order to not stretch the image, we use the largest distance from start to end to decide the crop indices\n",
    "    w_max, h_max, _ = img_shape\n",
    "\n",
    "    # Based on the findings from\n",
    "    # https://upload.wikimedia.org/wikipedia/commons/0/06/AvgHeadSizes.png\n",
    "    # We have that head to chin is at the 99 percentile for men is 10 cm\n",
    "    # and that center of eye distance at the 99 percentile for men is 7.4 cm\n",
    "    # This means that the box size should include most of the heads if we multiply the\n",
    "    # center of eye distance of eyes in pixels with the head_eye_ratio\n",
    "    # However, we add some pad as people are usually not in the 99 percentile\n",
    "    pad = 3\n",
    "    head_eye_ratio = pad+10/7.4\n",
    "    eye_distance = np.linalg.norm(eyes[0] - eyes[1])\n",
    "    box_size = np.floor(eye_distance*head_eye_ratio).astype(int)\n",
    "    half_size = (box_size/2).astype(int)\n",
    "\n",
    "    # Find the center of the eyes\n",
    "    x_center, y_center = np.round(np.vstack(eyes).mean(axis=0)).astype(int)\n",
    "    \n",
    "    new_top = y_center - half_size\n",
    "    new_bottom = y_center + half_size\n",
    "    new_left = x_center - half_size\n",
    "    new_right = x_center + half_size\n",
    "    \n",
    "    # Deal with corner cases\n",
    "    # NOTE: This list of corner cases is not exhaustive\n",
    "    y_shift = 0\n",
    "    x_shift = 0\n",
    "    \n",
    "    if new_top < 0:\n",
    "        y_shift = -new_top\n",
    "    if new_bottom > h_max:\n",
    "        y_shift = h_max - new_bottom\n",
    "    if new_left < 0:\n",
    "        x_shift = -new_left\n",
    "    if new_right > w_max:\n",
    "        x_shift = w_max - new_right\n",
    "            \n",
    "    new_top = (new_top + y_shift).clip(0, h_max)\n",
    "    new_bottom = (new_bottom + y_shift).clip(0, h_max)\n",
    "    new_left = (new_left + x_shift).clip(0, w_max)\n",
    "    new_right = (new_right + x_shift).clip(0, w_max)\n",
    "       \n",
    "    crop_indices = np.array([new_top, new_bottom, new_left, new_right])\n",
    "    \n",
    "    return crop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_with_largest_box(bounding_rectangles):\n",
    "    \"\"\"\n",
    "    Returns the index for the box with the largest area\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bounding_rectangles : list\n",
    "        List of rectangle prediction from dlib\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    index_max_area : int\n",
    "        The index with the highest area\n",
    "    \"\"\"\n",
    "    \n",
    "    index_max_area = 0\n",
    "    max_area = 0\n",
    "    for index, rectangle in enumerate(bounding_rectangles):\n",
    "        top, bottom, left, right = bounding_rectangle_to_np(rectangle)\n",
    "        area = (bottom-top)*(right-left)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            index_max_area = index\n",
    "    \n",
    "    return index_max_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_eyes(eyes, rot_mat):\n",
    "    \"\"\"\n",
    "    Rotates the eyes accoring to the rotation matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eyes : tuple, len (2,)\n",
    "        A tuple consisting of the eyes on the form\n",
    "        >>> (eye_1, eye_2)\n",
    "        Both eye_1 and eye_2 are np.arrays with len 2, \n",
    "        consisting of the x-coordinate and the y-coordinate of the eye\n",
    "    rot_mat : np.array, shape (2, 3)\n",
    "        The rotation matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rot_eyes : tuple, len (2,)\n",
    "        The rotated eyes on the same format as the input `eyes`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rotate the eyes\n",
    "    stacked_eyes = np.vstack((eyes[0], eyes[1]))\n",
    "    eyes_w_ones = np.hstack((stacked_eyes, np.ones((stacked_eyes.shape[0], 1))))\n",
    "    rot_eyes = np.round(rot_mat.dot(eyes_w_ones.T)).T.astype(int)\n",
    "    \n",
    "    rot_left_eye = rot_eyes[0, :]\n",
    "    rot_right_eye = rot_eyes[1, :]\n",
    "    rot_eyes = (rot_left_eye, rot_right_eye)\n",
    "    \n",
    "    return rot_eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(img, crop_indices, size=224):\n",
    "    \"\"\"\n",
    "    Cropping and resizing the image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (width, height, 3)\n",
    "        Image to be processed\n",
    "    crop_indices : np.array, shape (4,)\n",
    "        The rectangle on the form (top, bottom, left, right)\n",
    "    size : int\n",
    "        The width and heigth of the output image\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    processed_img : np.array, shape (*size, 3)\n",
    "        The cropped and resized image\n",
    "    \"\"\"\n",
    "    \n",
    "    top, bottom, left, right = crop_indices\n",
    "    cropped_img = img[top:bottom, left:right]\n",
    "    processed_img = resize(cropped_img, (size, size), mode='reflect', anti_aliasing=True)\n",
    "    \n",
    "    return processed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imgs(imgs, verbose=False):\n",
    "    \"\"\"\n",
    "    Preprocesses images by aligning and resize faces\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    If the face detection algorithm fails, no preprocessing will be done\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : list, shape (n, )\n",
    "        A list of images as numpy arrays on with the shape (width, height, 3)\n",
    "        NOTE: The shapes of the images varies\n",
    "    verbose : bool\n",
    "        Whether or not to prin warnings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    processed_imgs : np.arrsy, shape (m, 224, 224, 3)\n",
    "        The preprocessed images containing the resized, aligned faces\n",
    "    \"\"\"\n",
    "    \n",
    "    # HOG based face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    \n",
    "    landmark_predictor = dlib.shape_predictor(str(landmark_file))\n",
    "    \n",
    "    processed_imgs = []\n",
    "    for img in tqdm_notebook(imgs, desc='preprocessing', leave=False):\n",
    "        # Detect the faces, we know apriori that there is one person per image\n",
    "        # NOTE: The 1 in the second argument indicates that we should upsample the image 1 time. \n",
    "        #       This will make everything bigger and allow us to detect more faces.\n",
    "        bounding_rectangles = face_detector(img, 1)\n",
    "        \n",
    "        if len(bounding_rectangles) != 0:\n",
    "            index = 0\n",
    "            if len(bounding_rectangles) > 1:\n",
    "                if verbose:\n",
    "                    print('Warning: Several faces found, choosing the largest')\n",
    "                index = get_index_with_largest_box(bounding_rectangles)\n",
    "                \n",
    "            bounding_rectangle = bounding_rectangles[index]\n",
    "            landmarks = landmark_predictor(img, bounding_rectangle)\n",
    "            \n",
    "            # Cast to numpy\n",
    "            bounding_rectangle = bounding_rectangle_to_np(bounding_rectangle)\n",
    "            landmarks = landmarks_to_np(landmarks)\n",
    "            \n",
    "            eyes = get_eyes(landmarks)\n",
    "            eyes_angle_deg = get_eyes_angle(eyes)\n",
    "        \n",
    "            # Rotate the image\n",
    "            image_center = tuple((np.array(img.shape[1::-1]) / 2).astype(int))\n",
    "            rot_mat = cv2.getRotationMatrix2D(image_center, eyes_angle_deg, 1.0)\n",
    "            rot_img = cv2.warpAffine(img, rot_mat, img.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "            rot_eyes = get_rot_eyes(eyes, rot_mat) \n",
    "            \n",
    "            crop_indices = get_crop_indices(img.shape, rot_eyes)\n",
    "            \n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Warning: No face found, only cropping will be done')\n",
    "            w, h, _ = img.shape\n",
    "            size = w if w < h else h\n",
    "            rot_img = img\n",
    "            crop_indices = np.array([0, size, 0, size])\n",
    "        \n",
    "        # Crop the and rescale image\n",
    "        processed_img = crop_and_resize(rot_img, crop_indices)\n",
    "        processed_imgs.append(processed_img)\n",
    "        \n",
    "    processed_imgs = np.array(processed_imgs)\n",
    "    \n",
    "    return processed_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize(x_train, y_train, function = lambda x:preprocess_imgs(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is already trained on the other face dataset. You should use this network as feature extractor to get descriptors of the faces. You can choose any hidden layer you need (or several layers) to extract features and any classification method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('face_recognition_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of using the network as feature extractor. The shape of input tensor has to be (n_images, 224, 224, 3), so you can input several images simultaneously and get their face descriptors of shape (n_images, n_components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output(images, layer = 'fc6'):\n",
    "    assert len(images.shape)==4, 'Wrong input dimentionality!'\n",
    "    assert images.shape[1:]==(224,224,3), 'Wrong input shape!'\n",
    "    \n",
    "    network_output = model.get_layer(layer).output\n",
    "    feature_extraction_model = Model(model.input, network_output)\n",
    "    \n",
    "    output = feature_extraction_model.predict(images)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(x_train['0.jpg'], (224,224)).reshape(1,224,224,3)\n",
    "out = get_layer_output(img)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier (2 points)\n",
    "\n",
    "\n",
    "You have to implement class $\\tt{Classifier}$ with methods $\\tt{fit}$, $\\tt{classify}$\\_$\\tt{images}$ and $\\tt{classify}$\\_$\\tt{videos}$ in the cell below. \n",
    "The method $\\tt{Classifier.fit}$ gets two dictionaries as input: train images and labels, and trains the classifier to predict the person shown on the image.\n",
    "$\\tt{Classifier.classify}$\\_$\\tt{images}$ gets the dictionary of test images (with filenames as keys) as input and should return the dictionary of the predicted labels.\n",
    "$\\tt{Classifier.classify}$\\_$\\tt{videos}$ is similar to the previous one, but gets the dictionary of test videos (with video as keys) as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify video you can combine the predictions for its frames any way you want (averaging, voting, etc.).\n",
    "If video classification takes too long you can use face detector not in all the frames but every few frames while preprocessing video frames. \n",
    "Besides, sometimes the face is hardly detected on the image and the frame in which the detector works wrong can add noise to the prediction. Hence, the result of the prediction without using such frames may be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    \"Class for classifying a person based on features from a pretrained network and nearest neighbors\"\n",
    "    \n",
    "    def __init__(self, nn_model, layer='fc7', neighbors=1, weights='uniform'):\n",
    "        \"\"\"\n",
    "        Constructor for the classifier\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        nn_model : Sequential\n",
    "            The neural network to use\n",
    "        layer : str\n",
    "            Name of an existing layer in the model\n",
    "        neighbors : int\n",
    "            Number of neighbors to use in the kNN classifier\n",
    "        weights : str\n",
    "            The weights of the kNN classifier\n",
    "        \"\"\"        \n",
    "        network_output = nn_model.get_layer(layer).output\n",
    "        self.feature_extractor = Model(model.input, network_output)\n",
    "        self.kNN = kNN(n_neighbors=neighbors, weights=weights)\n",
    "\n",
    "    def fit(self, train_imgs, train_labels):\n",
    "        \"\"\"\n",
    "        Fits the classifier\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_imgs : dict\n",
    "            The training data.\n",
    "            A dictionary where the keys are the image names and the \n",
    "            values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "            NOTE: The width and height varies\n",
    "        train_labels : dict\n",
    "            The training labels.\n",
    "            A dictionary where the keys are image names and the values are the class_is as strings.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make the data and the labels aligned\n",
    "        imgs = []\n",
    "        y = []\n",
    "        \n",
    "        names = train_imgs.keys()\n",
    "        for name in names:\n",
    "            imgs.append(train_imgs[name])\n",
    "            y.append(train_labels[name])\n",
    "        \n",
    "        print('Preprocessing...')\n",
    "        imgs = preprocess_imgs(imgs)\n",
    "        print('done')\n",
    "        print('Extracting features...', end='')\n",
    "        x = self.feature_extractor.predict(imgs, verbose=1)\n",
    "        print('done')\n",
    "        print('Fitting...', end='', flush=True)\n",
    "        self.kNN.fit(x, y)\n",
    "        print('done')\n",
    "\n",
    "    def classify_images(self, test_imgs):\n",
    "        \"\"\"\n",
    "        Makes a prediction for the test images\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        test_imgs : dict\n",
    "            The test data.\n",
    "            A dictionary where the keys are the image names and the \n",
    "            values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "            NOTE: The width and height varies\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        predicted : dict\n",
    "            The prediction for the input images.\n",
    "            A dictionary where the keys are the image names and the values are predicted labels\n",
    "        \"\"\"\n",
    "        \n",
    "        names = test_imgs.keys()\n",
    "        imgs = test_imgs.values()\n",
    "        print('Preprocessing...')\n",
    "        imgs = preprocess_imgs(imgs)\n",
    "        print('done')\n",
    "        print('Extracting features...')\n",
    "        x = self.feature_extractor.predict(imgs, verbose=1)\n",
    "        print('done')\n",
    "        print('Predicting...', end='', flush=True)\n",
    "        pred = self.kNN.predict(x)\n",
    "        print('done')\n",
    "        \n",
    "        predicted = {n: p for n, p in zip(names, pred)}\n",
    "        \n",
    "        return predicted\n",
    "   \n",
    "    def classify_videos(self, test_video):\n",
    "        \"\"\"\n",
    "        Makes a prediction for the videos\n",
    "        \n",
    "        The prediciton will be the most common prediction in the frames\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        test_video : dict\n",
    "            The test data.\n",
    "            A dictionary where the keys are the video ids and the values are lists of \n",
    "            numpy arrays containing the images with the shape (width, height, 3).\n",
    "            NOTE: The width and height varies\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted : dict\n",
    "            The prediction for the input images.\n",
    "            A dictionary where the keys are the video ids and the values are predicted labels\n",
    "        \"\"\"\n",
    "        \n",
    "        names = test_video.keys()\n",
    "        videos = test_video.values()\n",
    "        preds = []\n",
    "        for video in tqdm_notebook(videos, desc='videos'):\n",
    "            imgs = preprocess_imgs(video)\n",
    "            x = self.feature_extractor.predict(imgs, verbose=1)\n",
    "            video_preds = self.kNN.predict(x)\n",
    "            preds.append(Counter(video_preds).most_common(n=1)[0][0])\n",
    "        \n",
    "        predicted = {n: p for n, p in zip(names, preds)}\n",
    "        \n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the classifier, fit it and use to predict the labels of testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier = Classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = img_classifier.classify_images(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification quality (2 points)\n",
    "\n",
    "Let us check the accuracy of your classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.90, to obtain 2 points — at least 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(output, gt):    \n",
    "    correct = 0.\n",
    "    total = len(gt)\n",
    "    for k, v in gt.items():\n",
    "        if output[k] == v:\n",
    "            correct += 1\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return f'Classification accuracy is {accuracy:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_test(y_out, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video classification quality (2 points)\n",
    "\n",
    "Let us check the quality of video classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.80, to obtain 2 points — at least 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_classifier = Classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_classifier.fit(video_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_video_out = video_classifier.classify_videos(video_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_test(y_video_out, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
