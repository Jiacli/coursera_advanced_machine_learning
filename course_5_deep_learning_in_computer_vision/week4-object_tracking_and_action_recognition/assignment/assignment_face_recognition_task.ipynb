{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition using neural network features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you have to construct face recognizer based on features extracted from the neural network. The task consists of two parts: image classification and video classification. In the first one you should classify distinct images and in the second one you will deal with short video sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import dlib\n",
    "import cv2\n",
    "import requests\n",
    "import bz2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "from collections import Counter\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you have you have to read the data. Run the cell below to unpack data. The data can be obtained from\n",
    "this [link](https://d3c33hcgiwev3.cloudfront.net/blRYxTKUEeiISxJZ7npQ3g_6eb16bf0329411e8905a51c51e77c61e_face-recognition-task.zip?Expires=1539820800&Signature=WC-OWrQOumffTroH5U-QimCCAhMEXRx~u6Eit8uDcPXGUK2c4Rp7i9QSkzTwZK9kVyVYu6IEzoVT3BgyU5zr6NhBPRHUwDASA4lq6momTDrbpg6FKqWc9TEo2-cXZfbejVGgjkEwihe4gbx2pPiDDysj2OGxHJWftqCppyPUfJk_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dir = Path('.').joinpath('Face_Recognition_data')\n",
    "if not face_dir.is_dir():\n",
    "    raise RuntimeError('Please download the dataset and expand the zip to the current directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data for image and video classification (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function $\\tt{load}$\\_$\\tt{image}$\\_$\\tt{data}$. It should return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing ones. The keys of the dictionaries are the names of the image files and the values are 3-dimensional numpy arrays (for images) or strings with the names (for labels).\n",
    "\n",
    "$\\tt{dir}$\\_$\\tt{name}$ is the name of directory with data for image classification. If 'Face_Recofnition_data' directory is located in the same directory as this notebook, then the default value can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(dir_name = face_dir.joinpath('image_classification')):\n",
    "    \"\"\"\n",
    "    Loads the image data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_name : Path or str\n",
    "        Path to the images\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_train : dict\n",
    "        The training data.\n",
    "        A dictionary where the keys are the image names and the \n",
    "        values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_train : dict\n",
    "        The training labels.\n",
    "        A dictionary where the keys are image names and the values are the class_is as strings.\n",
    "    x_test : dict\n",
    "        The test data.\n",
    "        A dictionary where the keys are the image names and the \n",
    "        values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_test : dict\n",
    "        The test labels.\n",
    "        A dictionary where the keys are image names and the values are the class_ids as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train = dict()\n",
    "    x_test = dict()\n",
    "\n",
    "    train_dir = Path(dir_name).joinpath('train')\n",
    "    test_dir = Path(dir_name).joinpath('test')\n",
    "    \n",
    "    train_files = train_dir.joinpath('images').glob('*.jpg')\n",
    "    for f in tqdm_notebook(train_files, desc='train'):\n",
    "        # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "        x_train[f.name] = cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "        \n",
    "    y_train = dict(pd.read_csv(train_dir.joinpath('y_train.csv')).values)\n",
    "    \n",
    "    test_files = test_dir.joinpath('images').glob('*.jpg')\n",
    "    for f in tqdm_notebook(test_files, desc='test'):\n",
    "        # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "        x_test[f.name] = cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "        \n",
    "    y_test = dict(pd.read_csv(test_dir.joinpath('y_test.csv')).values)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_image_data()\n",
    "print(f'{len(x_train)} \\ttraining images')\n",
    "print(f'{len(x_test)} \\ttesting images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data, labels, function = lambda x:x[0], n_cols = 5, n_rows=1):\n",
    "    figure(figsize = (3*n_cols,3*n_rows))\n",
    "    for n, i in enumerate(np.random.choice(list(data.keys()), size = n_cols*n_rows)):\n",
    "        plt.subplot(n_rows, n_cols, n+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(function([data[i]]))\n",
    "        plt.title(labels[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x_train, y_train)\n",
    "visualize(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now read the video classification data, as well. You have to implement function to load video data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function $\\tt{load}$\\_$\\tt{video}$\\_$\\tt{data}$ should also return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing videos and labels. The training data is in the same format as in image classification task. The keys of testing data and labels are video ids and the values are the lists of frames and the strings with names, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_data(dir_name = face_dir.joinpath('video_classification')):\n",
    "    \"\"\"\n",
    "    Loads the video data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_name : Path or str\n",
    "        Path to the images\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_train : dict\n",
    "        The training data.\n",
    "        A dictionary where the keys are the image names and the \n",
    "        values are numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_train : dict\n",
    "        The training labels.\n",
    "        A dictionary where the keys are image names and the values are the class_is as strings.\n",
    "    x_test : dict\n",
    "        The test data.\n",
    "        A dictionary where the keys are the video ids and the values are lists of \n",
    "        numpy arrays containing the images with the shape (width, height, 3).\n",
    "        NOTE: The width and height varies\n",
    "    y_test : dict\n",
    "        The test labels.\n",
    "        A dictionary where the keys are image names and the values are the class_ids as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train = dict()\n",
    "    x_test = dict()\n",
    "\n",
    "    train_dir = Path(dir_name).joinpath('train')\n",
    "    test_dir = Path(dir_name).joinpath('test')\n",
    "    \n",
    "    train_files = train_dir.joinpath('images').glob('*.jpg')\n",
    "    for f in tqdm_notebook(train_files, desc='train'):\n",
    "        # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "        x_train[f.name] = cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "        \n",
    "    y_train = dict(pd.read_csv(train_dir.joinpath('y_train.csv')).values)\n",
    "    \n",
    "    # NOTE: The first directory is 'this' dir\n",
    "    test_dirs = sorted(test_dir.joinpath('videos').glob('**'))[1:]\n",
    "    for d in tqdm_notebook(test_dirs, desc='test_dir'):\n",
    "        images = []\n",
    "        image_paths = sorted(d.glob('*.jpg'))\n",
    "        for f in tqdm_notebook(image_paths, desc='test_img', leave=False):\n",
    "            # NOTE: We swap from bgr to rgb with [:,:,::-1]\n",
    "            images.append(cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1])\n",
    "        \n",
    "        x_test[d.name] = images\n",
    "        \n",
    "    y_test = dict(pd.read_csv(test_dir.joinpath('y_test.csv')).values.astype(str))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "video_train, train_labels, video_test, test_labels = load_video_data()\n",
    "print(f'{len(video_train)} \\ttraining images')\n",
    "print(f'{len(video_test)} \\ttesting videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(video_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize({i:video_test[i][1] for i in video_test}, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (3 points)\n",
    "You have to implement preprocessing function in the cell below.\n",
    "Getting a list of images as an input the this function should detect the face on each image, find the facial keypoints () and then crop and normalize the image according to these keypoints. The output of this function is the list of images which contain only the aligned face and should be converted to the tensor of the shape $(N, 224, 224, 3)\\ $ where $N$ is the length of the initial list. You can add extra arguments to the preprocess function if necessary (i.e. flag $\\tt{is}$\\_$\\tt{video}$ to determine if the list of images is video sequence or not).\n",
    "\n",
    "For face detection and facial keypoint regression you can use your models from the previous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** In order to ensure good quality, we use the dlib toolkit to get the facial landmarks. See http://dlib.net/face_landmark_detection.py.html and https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/ for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
    "landmark_file = Path('.').joinpath('shape_predictor_68_face_landmarks.dat')\n",
    "landmark_compressed_file = Path('.').joinpath('shape_predictor_68_face_landmarks.dat.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not landmark_compressed_file.is_file():\n",
    "    print(f'Downloading {landmark_compressed_file}')\n",
    "    response = requests.get(url)\n",
    "    with landmark_compressed_file.open('wb') as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print(f'{landmark_compressed_file} found') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not landmark_file.is_file():\n",
    "    print(f'Decompressing {landmark_compressed_file}')\n",
    "    with landmark_compressed_file.open('rb') as src_f, landmark_file.open('wb') as dst_f:\n",
    "        dst_f.write(bz2.decompress(src_f.read()))\n",
    "else:\n",
    "    print(f'{landmark_file} found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_to_np(landmarks):\n",
    "    \"\"\"\n",
    "    Convert landmarks to a numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    landmarks : dlib.full_object_detection\n",
    "        Detected landmarks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np_landmarks : np.array, shape (68, 2)\n",
    "        The landmarks converted to numpy\n",
    "        There are 68 landmarks, each with an (x, y) coordinate\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    https://github.com/jrosebr1/imutils/blob/master/imutils/face_utils/helpers.py\n",
    "    \"\"\"\n",
    "    \n",
    "    np_landmarks = np.zeros((landmarks.num_parts, 2), dtype=int)\n",
    "\n",
    "    for i in range(0, landmarks.num_parts):\n",
    "        np_landmarks[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    "\n",
    "    return np_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eyes(landmarks):\n",
    "    \"\"\"\n",
    "    Return a tuple of the eyes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    np_landmarks : np.array, shape (68, 2)\n",
    "        The landmarks converted to numpy\n",
    "        There are 68 landmarks, each with an (x, y) coordinate \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eyes : tuple, len (2,)\n",
    "        A tuple consisting of the eyes on the form\n",
    "        >>> (eye_1, eye_2)\n",
    "        Both eye_1 and eye_2 are np.arrays with len 2, \n",
    "        consisting of the x-coordinate and the y-coordinate of the eye\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: The indexing of the keypoints can be found here:\n",
    "    # https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg\n",
    "    # NOTE: Recall that python starts counting from 0, hence off-by-one with respect to facial landmarks\n",
    "    # Find the midpoints of the eyes\n",
    "    landmarks = landmark_to_np(landmarks)\n",
    "    left_eye_center = np.round(landmarks[[36, 37, 38, 39, 40, 41], :].mean(axis=0)).astype(int)\n",
    "    right_eye_center = np.round(landmarks[[42, 43, 44, 45, 46, 47], :].mean(axis=0)).astype(int)\n",
    "\n",
    "    eyes = (left_eye_center, right_eye_center)\n",
    "    \n",
    "    return eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eyes_angle(eyes):\n",
    "    \"\"\"\n",
    "    Returns the angle between a one of the eyes and a horizontal line crossing the second eye\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eyes : tuple, len (2,)\n",
    "        A tuple consisting of the eyes on the form\n",
    "        >>> (eye_1, eye_2)\n",
    "        Both eye_1 and eye_2 are np.arrays with len 2, \n",
    "        consisting of the x-coordinate and the y-coordinate of the eye\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eyes_angle_deg : float\n",
    "        The angle given in degrees\n",
    "    \"\"\"\n",
    "    # Find the angle between the segment connecting two eyes and horizontal line\n",
    "    eye_1, eye_2 = eyes\n",
    "    eye_1_x_pos = eye_1[1]\n",
    "    eye_2_x_pos = eye_2[1]\n",
    "    \n",
    "    # Identify the left and the rigth eye\n",
    "    if eye_1_x_pos < eye_2_x_pos:\n",
    "        left_eye = np.array(eye_1)\n",
    "        right_eye = np.array(eye_2)\n",
    "    else:\n",
    "        left_eye = np.array(eye_2)\n",
    "        right_eye = np.array(eye_1)\n",
    "        \n",
    "    # Construct the horizontal line\n",
    "    horizontal_form_left_eye = np.array((left_eye[0], right_eye[1]))\n",
    "    \n",
    "    # Calculate the angle in radians\n",
    "    eyes_angle_rad = \\\n",
    "        np.arccos(np.dot(right_eye, horizontal_form_left_eye)/\\\n",
    "                  (np.linalg.norm(right_eye)*np.linalg.norm(horizontal_form_left_eye)))\n",
    "    \n",
    "    # The angle will depend of which eye is on top\n",
    "    # We define the positive angle between the horizontal line and the rigth eye \n",
    "    if left_eye[0] > right_eye[0]:\n",
    "        eyes_angle_rad = -eyes_angle_rad\n",
    "    \n",
    "    # Convert to degrees\n",
    "    eyes_angle_deg = eyes_angle_rad * 180/np.pi\n",
    "    \n",
    "    return eyes_angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_rectangle_to_np(rectangle):\n",
    "    \"\"\"\n",
    "    Convert the rectangle to a numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rectangle : rectangle\n",
    "        Rectangle prediction from dlib\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np_rectangle : np.array, shape(4,)\n",
    "        The rectangle on the form (top, bottom, left, right)\n",
    "    \"\"\"\n",
    "    \n",
    "    np_rectangle = np.array([rectangle.top(), rectangle.bottom(), rectangle.left(), rectangle.right()])\n",
    "    return np_rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img, crop_indices, size=244):\n",
    "    \"\"\"\n",
    "    Cropping and resizing the image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (width, height, 3)\n",
    "        Image to be processed\n",
    "    crop_indices : np.array, shape(4,)\n",
    "        The rectangle on the form (top, bottom, left, right)\n",
    "    size : int\n",
    "        The width and heigth of the output image\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    processed_img : np.array, shape (*size, 3)\n",
    "        The cropped and resized image\n",
    "    \"\"\"\n",
    " \n",
    "    top, bottom, left, right = crop_indices\n",
    "    \n",
    "    # In order to not stretch the image, we use the largest distance from start to end to decide the crop indices\n",
    "    top, bottom, left, right = crop_indices\n",
    "    h_dist = bottom - top\n",
    "    w_dist = right - left\n",
    "    h_w_diff = h_dist - w_dist\n",
    "    \n",
    "    half_dist = np.round(np.abs(h_w_diff)/2).astype(int)\n",
    "\n",
    "    if h_w_diff > 0:\n",
    "        new_left = left - half_dist\n",
    "        new_right = right + half_dist\n",
    "        # NOTE: A corner case exist if right > img.shape[0]\n",
    "        #       This will stretch the image, but not cause any error\n",
    "        if left < 0:\n",
    "            new_left = left\n",
    "            new_right = right + h_w_diff\n",
    "        left = new_left\n",
    "        right = new_right\n",
    "    elif h_w_diff < 0:\n",
    "        new_top = top - half_dist\n",
    "        new_bottom = bottom + half_dist\n",
    "        # NOTE: A corner case exist if bottom > img.shape[1]\n",
    "        #       This will stretch the image, but not cause any error\n",
    "        if bottom < 0:\n",
    "            new_bottom = bottom\n",
    "            new_top = top - h_w_diff\n",
    "        top = new_top\n",
    "        bottom = new_bottom\n",
    "    \n",
    "    # Crop\n",
    "    cropped_img = img[top:bottom, left:right]\n",
    "    \n",
    "    processed_img = resize(cropped_img, (size, size), mode='reflect', anti_aliasing=True)\n",
    "    \n",
    "    return processed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imgs(imgs):\n",
    "    \"\"\"\n",
    "    Preprocesses images by aligning and resize faces\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : list, shape (n, )\n",
    "        A list of images as numpy arrays on with the shape (width, height, 3)\n",
    "        NOTE: The shapes of the images varies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    processed_imgs : np.arrsy, shape (n, 224, 224, 3)\n",
    "        The preprocessed images containing the resized, aligned faces\n",
    "    \"\"\"\n",
    "    \n",
    "    # HOG based face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    \n",
    "    landmark_predictor = dlib.shape_predictor(str(landmark_file))\n",
    "    \n",
    "    processed_imgs = []\n",
    "    for img in imgs:\n",
    "        # Detect the faces, we know apriori that there is one person per image\n",
    "        # NOTE: The 1 in the second argument indicates that we should upsample the image 1 time. \n",
    "        #       This will make everything bigger and allow us to detect more faces.\n",
    "        bounding_rectangle = face_detector(img, 1)[0]\n",
    "        \n",
    "        landmarks = landmark_predictor(img, bounding_rectangle)\n",
    "        eyes = get_eyes(landmarks)\n",
    "        eyes_angle_deg = get_eyes_angle(eyes)\n",
    "    \n",
    "        # Rotate the image\n",
    "        image_center = tuple((np.array(img.shape[1::-1]) / 2).astype(int))\n",
    "        rot_mat = cv2.getRotationMatrix2D(image_center, eyes_angle_deg, 1.0)\n",
    "        rot_img = cv2.warpAffine(img, rot_mat, img.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # If we rotated the bounding_rectangle, it would not be aligned with the abscissa and ordinate\n",
    "        # Therefore, we find the face again\n",
    "        crop_indices = bounding_rectangle_to_np(face_detector(rot_img, 1)[0])\n",
    "        \n",
    "        # Crop the and rescale image\n",
    "        processed_imgs.append(process_image(img, crop_indices))\n",
    "        \n",
    "    processed_imgs = np.array(processed_imgs)\n",
    "    \n",
    "    return processed_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize(x_train, y_train, function = lambda x:preprocess_imgs(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: The skewed faces comes from imperfections in the facial regression algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is already trained on the other face dataset. You should use this network as feature extractor to get descriptors of the faces. You can choose any hidden layer you need (or several layers) to extract features and any classification method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import load_model\n",
    "model = load_model('face_recognition_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of using the network as feature extractor. The shape of input tensor has to be (n_images, 224, 224, 3), so you can input several images simultaneously and get their face descriptors of shape (n_images, n_components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output(images, layer = 'fc6'):\n",
    "    assert len(images.shape)==4, 'Wrong input dimentionality!'\n",
    "    assert images.shape[1:]==(224,224,3), 'Wrong input shape!'\n",
    "    \n",
    "    network_output = model.get_layer(layer).output\n",
    "    feature_extraction_model = Model(model.input, network_output)\n",
    "    \n",
    "    output = feature_extraction_model.predict(images)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(x_train['0.jpg'], (224,224)).reshape(1,224,224,3)\n",
    "out = get_layer_output(img)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier (2 points)\n",
    "\n",
    "\n",
    "You have to implement class $\\tt{Classifier}$ with methods $\\tt{fit}$, $\\tt{classify}$\\_$\\tt{images}$ and $\\tt{classify}$\\_$\\tt{videos}$ in the cell below. \n",
    "The method $\\tt{Classifier.fit}$ gets two dictionaries as input: train images and labels, and trains the classifier to predict the person shown on the image.\n",
    "$\\tt{Classifier.classify}$\\_$\\tt{images}$ gets the dictionary of test images (with filenames as keys) as input and should return the dictionary of the predicted labels.\n",
    "$\\tt{Classifier.classify}$\\_$\\tt{videos}$ is similar to the previous one, but gets the dictionary of test videos (with video as keys) as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify video you can combine the predictions for its frames any way you want (averaging, voting, etc.).\n",
    "If video classification takes too long you can use face detector not in all the frames but every few frames while preprocessing video frames. \n",
    "Besides, sometimes the face is hardly detected on the image and the frame in which the detector works wrong can add noise to the prediction. Hence, the result of the prediction without using such frames may be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "from os.path import join\n",
    "class Classifier():\n",
    "    def __init__(self, nn_model):\n",
    "        \"\"\"Your implementation\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, train_imgs, train_labels):\n",
    "        \"\"\"Your implementation\"\"\"\n",
    "        pass\n",
    "\n",
    "    def classify_images(self, test_imgs):\n",
    "        \"\"\"Your implementation\"\"\"\n",
    "        pass\n",
    "   \n",
    "    def classify_videos(self, test_video):\n",
    "        \"\"\"Your implementation\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the classifier, fit it and use to predict the labels of testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier = Classifier(model)\n",
    "img_classifier.fit(x_train, y_train)\n",
    "y_out = img_classifier.classify_images(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification quality (2 points)\n",
    "\n",
    "Let us check the accuracy of your classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.90, to obtain 2 points — at least 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(output, gt):\n",
    "    correct = 0.\n",
    "    total = len(gt)\n",
    "    for k, v in gt.items():\n",
    "        if output[k] == v:\n",
    "            correct += 1\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return 'Classification accuracy is %.4f' % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print check_test(y_out, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video classification quality (2 points)\n",
    "\n",
    "Let us check the quality of video classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.80, to obtain 2 points — at least 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_classifier = Classifier(model)\n",
    "video_classifier.fit(video_train, train_labels)\n",
    "y_video_out = video_classifier.classify_videos(video_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print check_test(y_video_out, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
