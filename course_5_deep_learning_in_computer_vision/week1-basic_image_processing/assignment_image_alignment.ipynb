{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image alignment\n",
    "\n",
    "In this task, you will have to solve two image alignment problems: channel processing and face alignment. You can get **10 points** implementing all the passed functions (7.5 for the first part and 2.5 for the second one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image channels processing and alignment (7.5 points)\n",
    "\n",
    "## Problem review\n",
    "\n",
    "Sergey Prokudin-Gorsky was the first color photographer in Russia, who made the color portrait of Leo Tolstoy. Each of his photographs is three black-and-white photo plates, corresponding to red, green, and blue color channels. Currently, the collection of his pictures is situated in the U.S. Library of Congress, the altered versions have proliferated online. In this task, you should make a programme which will align the images from the Prokudin-Gorsky plates and learn the basic image processing methods.\n",
    "\n",
    "*The input image and the result of the alignment:*\n",
    "<img src=\"http://cdn1.savepice.ru/uploads/2017/7/31/8e68237bfd49026d137f59283db18b29-full.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    "#### Input image loading\n",
    "\n",
    "The input image is the set of 3 plates, corresponding to B, G, and R channels (top-down). You should implement the function $\\tt{load}$\\_$\\tt{data}$ that reads the data and returns the list of images of plates.\n",
    "$\\tt{dir}$\\_$\\tt{name}$ is the path to the directory with plate images. If this directory is located in the same directory as this notebook, then default arguments can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "def load_data(dir_name = 'plates'):\n",
    "    \"\"\"\n",
    "    Loads the plate data\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    dir_name : str\n",
    "        Path relative to location of this notebook\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : list\n",
    "        The loaded data images. Each image is loaded as a numpy ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort as we would like to load the images in a sorted fashion\n",
    "    files = sorted(Path('.').joinpath(dir_name).glob('*.png'))\n",
    "    \n",
    "    data = [cv2.imread(str(f), cv2.IMREAD_GRAYSCALE) for f in files]\n",
    "\n",
    "    return data\n",
    "\n",
    "plates = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a list of 2-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The auxiliary function `visualize()` displays the images given as argument.\n",
    "def visualize(imgs, format=None):\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    for i, img in enumerate(imgs):\n",
    "        if img.shape[0] == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "        plt_idx = i+1\n",
    "        plt.subplot(3, 3, plt_idx)    \n",
    "        plt.imshow(img, cmap=format)\n",
    "    plt.show()\n",
    "\n",
    "visualize(plates, 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The borders removal (1.5 points)\n",
    "It is worth noting that there is a framing from all sides in most of the images. This framing can appreciably worsen the quality of channels alignment. Here, we suggest that you find the borders on the plates using Canny edge detector, and crop the images according to these edges. The example of using Canny detector implemented in skimage library can be found [here](http://scikit-image.org/docs/dev/auto_examples/edges/plot_canny.html).<br>\n",
    "\n",
    "The borders can be removed in the following way:\n",
    "* Apply Canny edge detector to the image.\n",
    "* Find the rows and columns of the frame pixels. \n",
    "For example, in case of upper bound we will search for the row in the neighborhood of the upper edge of the image (e.g. 5% of its height). For each row let us count the number of edge pixels (obtained using Canny detector) it contains. Having these number let us find two maximums among them. Two rows corresponding to these maximums are edge rows. As there are two color changes in the frame (firstly, from light scanner background to the dark tape and then from the tape to the image), we need the second maximum that is further from the image border. The row corresponding to this maximum is the crop border. In order not to find two neighboring peaks, non-maximum suppression should be implemented: the rows next to the first maximum are set to zero, and after that, the second maximum is searched for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canny detector implementation (2.5 points)\n",
    "You can write your own implementation of Canny edge detector to get extra points. <br>\n",
    "\n",
    "Canny detection algorithm:\n",
    "1. *Noise reduction.* To remove noise, the image is smoothed by Gaussian blur with the kernel of size $5 \\times 5$ and $\\sigma = 1.4$. Since the sum of the elements in the Gaussian kernel equals $1$, the kernel should be normalized before the convolution. <br><br>\n",
    "\n",
    "2. *Calculating gradients.* When the image $I$ is smoothed, the derivatives $I_x$ and $I_y$ w.r.t. $x$ and $y$ are calculated. It can be implemented by convolving $I$ with Sobel kernels $K_x$ and $K_y$, respectively: \n",
    "$$ K_x = \\begin{pmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{pmatrix}, K_y = \\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{pmatrix}. $$ \n",
    "Then, the magnitude $G$ and the slope $\\theta$ of the gradient are calculated:\n",
    "$$ |G| = \\sqrt{I_x^2 + I_y^2}, $$\n",
    "$$ \\theta(x,y) = arctan\\left(\\frac{I_y}{I_x}\\right)$$<br><br>\n",
    "\n",
    "3. *Non-maximum suppression.* For each pixel find two neighbors (in the positive and negative gradient directions, supposing that each neighbor occupies the angle of $\\pi /4$, and $0$ is the direction straight to the right). If the magnitude of the current pixel is greater than the magnitudes of the neighbors, nothing changes, otherwise, the magnitude of the current pixel is set to zero.<br><br>\n",
    "\n",
    "4. *Double threshold.* The gradient magnitudes are compared with two specified threshold values, the first one is less than the second. The gradients that are smaller than the low threshold value are suppressed; the gradients higher than the high threshold value are marked as strong ones and the corresponding pixels are included in the final edge map. All the rest gradients are marked as weak ones and pixels corresponding to these gradients are considered in the next step.<br><br>\n",
    "\n",
    "5. *Edge tracking by hysteresis.* Since a weak edge pixel caused from true edges will be connected to a strong edge pixel, pixel $w$ with weak gradient is marked as edge and included in the final edge map if and only if it is involved in the same blob (connected component) as some pixel $s$ with strong gradient. In other words, there should be a chain of neighbor weak pixels connecting $w$ and $s$ (the neighbors are 8 pixels around the considered one). You are welcome to make up and implement an algorithm that finds all the connected components of the gradient map considering each pixel only once.  After that, you can decide which pixels will be included in the final edge map (this algorithm should be single-pass, as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is the reference canny detection algorithm, see own implementation below\n",
    "\n",
    "from  skimage.feature import canny\n",
    "\n",
    "def Canny_detector(img):\n",
    "    \"\"\" Your implementation instead of skimage \"\"\"     \n",
    "    return canny(img, sigma=1.4)\n",
    "\n",
    "canny_imgs = []\n",
    "for img in plates:\n",
    "    canny_img = Canny_detector(img)\n",
    "    canny_imgs.append(canny_img)\n",
    "    \n",
    "visualize(canny_imgs, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_noise(img, sigma=1.4):\n",
    "    \"\"\"\n",
    "    Input image is smoothed by Gaussian blur with sigma and a filter size of 5x5\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (rows, cols)\n",
    "        The image to reduce the noise of\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    noise_reduce : np.array, shape (rows, cols)\n",
    "        The image with noise reduction\n",
    "    \"\"\"\n",
    "    # The Gaussian is being centered around the center of the mesh\n",
    "    # Therefore, we can make matrices which counts the distances from the center in the respective directions\n",
    "    # We first make the distance array\n",
    "    x_dist = [2, 1, 0, 1, 2]\n",
    "    y_dist = [2, 1, 0, 1, 2]\n",
    "    \n",
    "    # And makes meshgrids out of these\n",
    "    x, y = np.meshgrid(x_dist, y_dist)\n",
    "\n",
    "    gaussian_kernel = (1/(2*np.pi*sigma**2))*np.exp(-(x**2 + y**2)/(2*sigma**2))\n",
    "    # Normalization of the Gaussian kernel\n",
    "    norm_gaussian_kernel = gaussian_kernel/gaussian_kernel.sum()\n",
    "    \n",
    "    # We convolve using mode='same' such that the input dimension equals output dimension\n",
    "    noise_reduced = convolve2d(img, norm_gaussian_kernel, mode='same')\n",
    "    \n",
    "    return noise_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(img):\n",
    "    \"\"\"\n",
    "    Returns the gradients with respect to x and y of the input image using Sobel kernels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (rows, cols)\n",
    "        The image to take the derivative of\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    i_x : np.array, shape (rows, cols)\n",
    "        The derivative of the image with respect to x\n",
    "    i_y : np.array, shape (rows, cols)\n",
    "        The derivative of the image with respect to y\n",
    "    \"\"\"\n",
    "    \n",
    "    k_x = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 2],\n",
    "                    [-1, 0, 1]])\n",
    "    k_y = -k_x.T\n",
    "    \n",
    "    # We convolve using mode='same' such that the input dimension equals output dimension\n",
    "    i_x = convolve2d(img, k_x, mode='same')\n",
    "    i_y = convolve2d(img, k_y, mode='same')\n",
    "    \n",
    "    return i_x, i_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_suppression(g_abs, i_x, i_y):\n",
    "    \"\"\"\n",
    "    Performs non maximum suppresses the image\n",
    "    \n",
    "    That is: \n",
    "    1. Finds the closest points in the positive and negative gradient direction\n",
    "    2. If the point under investigation is less in magnitude than one of its two neighbors:\n",
    "       Set value of point under investigation to zero\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g_abs : np.array, shape (rows, cols)\n",
    "        The magnitude of the gradient to take the non-maximum supression of\n",
    "    i_x : np.array, shape (rows, cols)\n",
    "        The derivative of the image with respect to x\n",
    "    i_y : np.array, shape (rows, cols)\n",
    "        The derivative of the image with respect to y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    non_max_suppressed_g_abs : np.array, shape (rows, cols)\n",
    "        The non-maximum suppressed image\n",
    "    \"\"\"\n",
    "    \n",
    "    non_max_suppressed_g_abs = g_abs.copy()\n",
    "    \n",
    "    row_inds, col_inds = g_abs.shape\n",
    "    \n",
    "    # Numpy is row major, so we loop over the columns first\n",
    "    for x_ind in range(col_inds):\n",
    "        for y_ind in range(row_inds):\n",
    "            # First we find the value of the point under consideration\n",
    "            # NOTE: We check the input g_abs, as the non_max_suppressed_g_abs may have changed\n",
    "            cur_val = g_abs[y_ind, x_ind]\n",
    "            \n",
    "            # Next we find the closest points in positive and negative gradient direction\n",
    "            # NOTE: Clipping values to -1 and 1, as long interactions are of less importance\n",
    "            x_steps = np.round(i_x[y_ind, x_ind]).clip(-1, 1).astype(int)\n",
    "            y_steps = np.round(i_y[y_ind, x_ind]).clip(-1, 1).astype(int)\n",
    "            \n",
    "            pos_x_index = (x_ind + x_steps).clip(0, col_inds-1)\n",
    "            pos_y_index = (y_ind + y_steps).clip(0, row_inds-1)\n",
    "\n",
    "            neg_x_index = (x_ind - x_steps).clip(0, col_inds-1)\n",
    "            neg_y_index = (y_ind - y_steps).clip(0, row_inds-1)\n",
    "            \n",
    "            # Then we find the value at the positive and negative position\n",
    "            # NOTE: We check the input g_abs, as the non_max_suppressed_g_abs may have changed\n",
    "            pos_dir_val = g_abs[pos_y_index, pos_x_index]\n",
    "            neg_dir_val = g_abs[neg_y_index, neg_x_index]\n",
    "            \n",
    "            if cur_val < pos_dir_val or cur_val < neg_dir_val:\n",
    "                non_max_suppressed_g_abs[y_ind, x_ind] = 0\n",
    "\n",
    "    return non_max_suppressed_g_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strong_and_weak_gradients(g_abs, high=90, low=80):\n",
    "    \"\"\"\n",
    "    Returns the strong and weak gradients of an image\n",
    "    \n",
    "    A strong gradient is a gradient value which is above the high treshold.\n",
    "    A weak gradient is a gradient value which is between the low and high treshold.\n",
    "    Gradient values below the low threshold are neither.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The high treshold percentile is set high by default in order to capture the strongest gradients\n",
    "    The low treshold percentile is set close to the high in order to suppress details\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g_abs : np.array, shape (rows, cols)\n",
    "        The magnitude of the gradient\n",
    "    high : float\n",
    "        The high treshold of the gradient (in percentiles)\n",
    "    low : float\n",
    "        The low threshold of the gradient (in percentiles)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    strong_gradients : np.array, shape (rows, cols)\n",
    "        Strong gradients of the image\n",
    "    weak_gradients : np.array, shape (rows, cols)\n",
    "        Weak gradients of the image    \n",
    "    \"\"\"\n",
    "    \n",
    "    high_tresh = np.percentile(g_abs, high)\n",
    "    low_tresh = np.percentile(g_abs, low)\n",
    "    \n",
    "    strong_gradients = g_abs >= high_tresh\n",
    "    weak_gradients = np.logical_and(g_abs >= low_tresh, g_abs < high_tresh)\n",
    "    \n",
    "    return strong_gradients, weak_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_track_image(strong_gradients, weak_gradients):\n",
    "    \"\"\"\n",
    "    Combines strong gradient pixels with weak edge pixels caused from true edges.\n",
    "    \n",
    "    A weak edge pixel is considered to be caused by a true edge if any of the eigth \n",
    "    neighbors contains a strong gradient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    strong_gradients : np.array, shape (rows, cols)\n",
    "        Strong gradients of the image\n",
    "    weak_gradients : np.array, shape (rows, cols)\n",
    "        Weak gradients of the image       \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    canny_img : np.array, shape (rows, cols)\n",
    "        The image containing the canny edges\n",
    "    \"\"\"\n",
    "    \n",
    "    canny_img = strong_gradients.copy().astype(int)\n",
    "    \n",
    "    row_inds, col_inds = weak_gradients.shape\n",
    "\n",
    "    # Numpy is row major, so we loop over the columns first\n",
    "    for x_ind in range(col_inds):\n",
    "        for y_ind in range(row_inds):\n",
    "            # NOTE: A point cannot simultaneously be a strong and weak gradient\n",
    "            if weak_gradients[y_ind, x_ind] and\\\n",
    "               strong_gradients[y_ind-1:y_ind+1, x_ind-1:x_ind+1].any():\n",
    "                canny_img[y_ind, x_ind] = 1\n",
    "                \n",
    "    return canny_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Canny_detector(img):\n",
    "    \"\"\"\n",
    "    Own implementation of the canny detection algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (rows, cols)\n",
    "        The image to find the Canny edges from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    canny_img : np.array, shape (rows, cols)\n",
    "        The image containing the canny edges\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Noise reduction\n",
    "    noise_reduced_img = reduce_noise(img)\n",
    "    \n",
    "    # 2. Calculating the gradients\n",
    "    i_x, i_y = get_gradients(noise_reduced_img)\n",
    "    # NOTE: One could use g_abs and theta for the non-maximum suppression,\n",
    "    #       but using i_x and i_y uses suffices for our purposes\n",
    "    g_abs = np.sqrt(i_x**2 + i_y**2)\n",
    "    \n",
    "    # 3. Non-maximum suppression\n",
    "    suppressed_g = non_maximum_suppression(g_abs, i_x, i_y)\n",
    "    \n",
    "    # 4. Double threshold\n",
    "    strong_gradients, weak_gradients = get_strong_and_weak_gradients(suppressed_g)\n",
    "    \n",
    "    # 5. Edge tracking by hysteresis\n",
    "    canny_img = edge_track_image(strong_gradients, weak_gradients)\n",
    "    \n",
    "    return canny_img\n",
    "\n",
    "canny_imgs = []\n",
    "for img in plates:\n",
    "    canny_img = Canny_detector(img)\n",
    "    canny_imgs.append(canny_img)\n",
    "    \n",
    "visualize(canny_imgs, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTE: This is the reference canny detection algorithm, see own implementation below\n",
    "\n",
    "def remove_borders(img, canny_img):\n",
    "    \"\"\" Your implementation instead of the following one\"\"\"   \n",
    "    dx = int(img.shape[1] * 0.05) \n",
    "    return img[dx : -dx, dx : -dx]\n",
    "\n",
    "\n",
    "cropped_imgs = []\n",
    "#crop borders\n",
    "for i, img in enumerate(plates):\n",
    "    cropped_imgs.append(remove_borders(img, canny_imgs[i]))\n",
    "\n",
    "visualize(cropped_imgs, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counts(top, bottom, left, right, fig_title):\n",
    "    \"\"\"\n",
    "    Plot the counts of the input arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    top : np.array, shape (edge_rows)\n",
    "        The counts of the rows in the top of the image\n",
    "    bottom : np.array, shape (edge_rows)\n",
    "        The counts of the rows in the bottom of the image\n",
    "    left : np.array, shape (edge_columns)\n",
    "        The counts of the rows in the left of the image\n",
    "    right : np.array, shape (edge_columnss)\n",
    "        The counts of the rows in the right of the image\n",
    "    fig_title : str\n",
    "        The title of the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4)\n",
    "    fig.suptitle(fig_title)\n",
    "    ax1.bar(np.arange(len(top)), top)\n",
    "    ax1.set_xlabel('Top rows (counted from the edge)')\n",
    "    ax2.bar(np.arange(len(bottom)), bottom)\n",
    "    ax2.set_xlabel('Bottom rows (counted from the edge)')\n",
    "    ax3.bar(np.arange(len(left)), left)\n",
    "    ax3.set_xlabel('Left rows (counted from the edge)')\n",
    "    ax4.bar(np.arange(len(right)), right)\n",
    "    ax4.set_xlabel('Right rows (counted from the edge)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_1d(array_1d):\n",
    "    \"\"\"\n",
    "    Performs non-maximum suppression on the input array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array_1d : np.array, shape(array_len)\n",
    "        The array to perform non-max suppression on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array_1d_suppr : np.array, shape(array_len)\n",
    "        The non-max suppressed array\n",
    "    \"\"\"\n",
    "    \n",
    "    array_1d_suppr = array_1d.copy()\n",
    "    \n",
    "    array_len = len(array_1d)\n",
    "    \n",
    "    # Non-maximum suppression on the interior points\n",
    "    for i in range(1, array_len-1):\n",
    "        if array_1d[i-1] >= array_1d[i] or array_1d[i+1] >= array_1d[i]:\n",
    "            array_1d_suppr[i] = 0\n",
    "            \n",
    "    # Non-maximum suppression on the edge points\n",
    "    if array_1d[1] >= array_1d[0]:\n",
    "        array_1d_suppr[0] = 0\n",
    "    if array_1d[-2] >= array_1d[-1]:\n",
    "        array_1d_suppr[-1] = 0    \n",
    "    \n",
    "    return array_1d_suppr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_borders(img, canny_img, i):\n",
    "    \"\"\"\n",
    "    Removes the borders of an image depending on the Canny edges\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (rows, cols)\n",
    "        The original image\n",
    "    canny_img : np.array, shape (rows, cols)\n",
    "        The canny edges of the original image\n",
    "    i : int\n",
    "        The plate number\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cropped_img : np.array, shape (rows, cols)\n",
    "        The cropped image\n",
    "    \"\"\"   \n",
    "    \n",
    "    # We will search around the pixels within the 10 percentile of the edges \n",
    "    # (we see from the image above that cropping a exactly 5% leaves some unwanted black lines)\n",
    "    search = 0.1\n",
    "    \n",
    "    top_pixels = np.round(img.shape[0]*search).astype(int)\n",
    "    bottom_pixels = np.round(img.shape[0]*(1-search)).astype(int)\n",
    "    left_pixels = np.round(img.shape[1]*search).astype(int)\n",
    "    right_pixels = np.round(img.shape[1]*(1-search)).astype(int)\n",
    "    \n",
    "    # Take the sum of the respective rows or columns\n",
    "    # NOTE: As we count from the edge (so that we can use negative indexing when cropping), \n",
    "    #       we reverse the bottom_sum and right_sum arrays\n",
    "    top_sum = canny_img[:top_pixels, :].sum(axis=1)\n",
    "    bottom_sum = canny_img[bottom_pixels:, :].sum(axis=1)[::-1]\n",
    "    left_sum = canny_img[:, :left_pixels].sum(axis=0)\n",
    "    right_sum = canny_img[:, right_pixels:].sum(axis=0)[::-1]\n",
    "\n",
    "    # There should be at least two maximums corresponding to the two color changes: Scanner-tape, tape-image\n",
    "    # Thus, we are after the maximum in the Canny edges which corresponds to the tape-image interface\n",
    "    # By using non-maximum suppression we get the local maximas of the Canny image which we can use to\n",
    "    # search for the intersection\n",
    "    # As a sanity check, we plot the counts before and after max suppression\n",
    "    plot_counts(top_sum, \n",
    "                bottom_sum, \n",
    "                left_sum, \n",
    "                right_sum,\n",
    "                f'Before non-max suppression plate {i}')\n",
    "\n",
    "    # Non-max suppression\n",
    "    top_sum_suppr = non_max_suppression_1d(top_sum)\n",
    "    bottom_sum_suppr = non_max_suppression_1d(bottom_sum)\n",
    "    left_sum_suppr = non_max_suppression_1d(left_sum)\n",
    "    right_sum_suppr = non_max_suppression_1d(right_sum)\n",
    "    \n",
    "    plot_counts(top_sum_suppr,\n",
    "                bottom_sum_suppr, \n",
    "                left_sum_suppr, \n",
    "                right_sum_suppr, \n",
    "                f'After non-max suppression plate {i}')\n",
    "   \n",
    "    # NOTE: We observere in the images and the Canny images that there are couple of more lines than the\n",
    "    #       scanner-tape and the tape-image interface, and one could argue that a different maxima could \n",
    "    #       have been used.\n",
    "    #\n",
    "    #       It was checked wheter the following gave better cropping:\n",
    "    #       1. Another local maxima\n",
    "    #       2. The nth highest value of the local maxima, see\n",
    "    #       https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array/27433395    #       \n",
    "    #       3. Tweaking of the Canny lines\n",
    "    #\n",
    "    #       Indeed the overall cropping got better, but on the cost that the later alignment of the images\n",
    "    #       got worse (as cropping just a little part of the origninal image gave a bad alignment)\n",
    "    maxima = 2\n",
    "    \n",
    "    # NOTE: Where returns a tuple, hence the first zero indexing\n",
    "    top = np.where(top_sum_suppr != 0)[0][maxima-1]\n",
    "    bottom = np.where(bottom_sum_suppr != 0)[0][maxima-1]\n",
    "    left = np.where(left_sum_suppr != 0)[0][maxima-1]\n",
    "    right = np.where(right_sum_suppr != 0)[0][maxima-1]\n",
    "     \n",
    "    return img[top:-bottom, left:-right]\n",
    "\n",
    "cropped_imgs = []\n",
    "#crop borders\n",
    "for i, img in enumerate(plates):\n",
    "    cropped_imgs.append(remove_borders(img, canny_imgs[i], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(cropped_imgs, 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels separation  (0.5 points)\n",
    "\n",
    "The next step is to separate the image into three channels (B, G, R) and make one colored picture. To get channels, you can divide each plate into three equal parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impose_components(img):\n",
    "    \"\"\"\n",
    "    Imposes the components of a plate\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (rows, cols)\n",
    "        The image to impose the components of\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rgb_img : np.array, shape (rows/3, cols, 3)\n",
    "        The rgb image\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOTE: We use the floor operator to avoid off-by-one errors\n",
    "    row_split = np.floor(img.shape[0]/3).astype(int)\n",
    "    \n",
    "    # NOTE: We split red by 2*row_split:3*row_split to avoid off-by-one errors\n",
    "    blue = img[0:row_split, :]\n",
    "    green = img[row_split:2*row_split, :]\n",
    "    red = img[2*row_split:3*row_split, :]\n",
    "\n",
    "    rgb_img = np.stack((red, green, blue), axis=-1)\n",
    "    \n",
    "    return rgb_img\n",
    "\n",
    "rgb_imgs = []\n",
    "for cropped_img in cropped_imgs:\n",
    "    rgb_img = impose_components(cropped_img)\n",
    "    rgb_imgs.append(rgb_img)\n",
    "\n",
    "visualize(rgb_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for the best shift for channel alignment (1 point for metrics implementation + 2 points for channel alignment)\n",
    "\n",
    "In order to align two images, we will shift one image relative to another within some limits (e.g. from $-15$ to $15$ pixels). For each shift, we can calculate some metrics in the overlap of the images. Depending on the metrics, the best shift is the one the metrics achieves the greatest or the smallest value for. We suggest that you implement two metrics and choose the one that allows to obtain the better alignment quality:\n",
    "\n",
    "* *Mean squared error (MSE):*<br><br>\n",
    "$$ MSE(I_1, I_2) = \\dfrac{1}{w * h}\\sum_{x,y}(I_1(x,y)-I_2(x,y))^2, $$<br> where *w, h* are width and height of the images, respectively. To find the optimal shift you should find the minimum MSE over all the shift values.\n",
    "    <br><br>\n",
    "* *Normalized cross-correlation (CC):*<br><br>\n",
    "    $$\n",
    "    I_1 \\ast I_2 = \\dfrac{\\sum_{x,y}I_1(x,y)I_2(x,y)}{\\sum_{x,y}I_1(x,y)\\sum_{x,y}I_2(x,y)}.\n",
    "    $$<br>\n",
    "    To find the optimal shift you should find the maximum CC over all the shift values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(i_1, i_2):\n",
    "    \"\"\"\n",
    "    Returns the mean square error of i_1 and i_2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    i_1 : np.array, shape (rows, cols)\n",
    "        The first matrix\n",
    "    i_2 : np.array, shape (rows, cols)\n",
    "        The second matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mse_val : float\n",
    "        The mean squared error between i_1 and i_2\n",
    "    \"\"\"\n",
    "    \n",
    "    w = i_1.shape[1]\n",
    "    h = i_1.shape[0]\n",
    "    \n",
    "    mse_val = (1/(w*h))*((i_1-i_2)**2).sum()\n",
    "    \n",
    "    return mse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor(i_1, i_2):\n",
    "    \"\"\"\n",
    "    Returns the cross correlation of i_1 and i_2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    i_1 : np.array, shape (rows, cols)\n",
    "        The first matrix\n",
    "    i_2 : np.array, shape (rows, cols)\n",
    "        The second matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    cor_val : float\n",
    "        The cross correlation between i_1 and i_2\n",
    "    \"\"\"\n",
    "    \n",
    "    cor_val = (i_1*i_2).sum()/(i_1.sum()*i_2.sum())\n",
    "    \n",
    "    return cor_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_shift(ch_1, ch_2, mode, search_range=15):\n",
    "    \"\"\"    \n",
    "    Finds the optimal shift between two channels by shifting ch_1 with respect to ch_2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ch_1 : np.array, shape (rows, cols)\n",
    "        Channel 1 to shift against channel 2\n",
    "    ch_2 : np.array, shape (rows, cols)\n",
    "        Channel 2 which channel 1 is shifted against\n",
    "    mode : 'mse' or 'cc'\n",
    "        The optimisation mode (either mean squared error or cross correlation)\n",
    "    search_range : int\n",
    "        Number of shifts to try\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_shift_top : int\n",
    "        The best shift of ch_1 to match ch_2 counted from the top of the padded channels\n",
    "    best_shift_left : int\n",
    "        The best shift of ch_1 to match ch_2 counted from the left of the padded channels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Declare the score\n",
    "    score = np.zeros((search_range*2, search_range*2))\n",
    "    \n",
    "    if mode == 'mse':\n",
    "        get_score = mse\n",
    "    elif mode == 'cc':\n",
    "        get_score = cor\n",
    "    else:\n",
    "        raise RuntimeError('mode must be mse or cc')\n",
    "\n",
    "    # We make baground images which we will fill the channels with\n",
    "    ch_1_background = np.zeros((ch_1.shape[0] + 2*search_range, ch_1.shape[1] + 2*search_range))\n",
    "    ch_2_background = np.zeros((ch_2.shape[0] + 2*search_range, ch_2.shape[1] + 2*search_range))\n",
    "    \n",
    "    # We will keep channel 2 steady and shift channel 1\n",
    "    ch_2_steady = ch_2_background.copy()\n",
    "    ch_2_steady[search_range:-search_range, search_range:-search_range] = ch_2\n",
    "\n",
    "    for i in range(search_range*2):\n",
    "        for j in range(search_range*2):\n",
    "            cur_ch_1 = ch_1_background.copy()\n",
    "            cur_ch_1[i:-search_range*2+i, j:-search_range*2+j] = ch_1\n",
    "            score[i, j] = get_score(cur_ch_1, ch_2_steady)\n",
    "\n",
    "    if mode == 'mse':\n",
    "        # NOTE: We negate the mse, so that the max will give the best result\n",
    "        score = -score\n",
    "    \n",
    "    best_shift_top, best_shift_left = np.unravel_index(score.argmax(), score.shape)\n",
    "    \n",
    "    return best_shift_top, best_shift_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_image(rgb_img, mode, search_range=15):\n",
    "    \"\"\"\n",
    "    Generates rgb images based on the best shift between the channels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rgb_img : np.array, shape (rows, cols, 3)\n",
    "        The image to optimize\n",
    "    mode : 'mse' or 'cc'\n",
    "        The optimisation mode (either mean squared error or cross correlation)\n",
    "    search_range : int\n",
    "        Number of shifts to try\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    optimized_img : np.array, shape (rows+rshift, cols, 3)\n",
    "        The image with the optimal amount of shift\n",
    "    \"\"\"\n",
    "    \n",
    "    red = rgb_img[:,:,0]\n",
    "    green = rgb_img[:,:,1]\n",
    "    blue = rgb_img[:,:,2]\n",
    "    n_cols = red.shape[1]\n",
    "    \n",
    "    # In order to have a reference we will shift the channels with respect to the green channel\n",
    "    \n",
    "    # Get the best shift of the blue channel\n",
    "    blue_top_shift, blue_left_shift = get_best_shift(blue, green, mode, search_range)\n",
    "    # Get the best shift of the red channel    \n",
    "    red_top_shift, red_left_shift = get_best_shift(red, green, mode, search_range)\n",
    "\n",
    "    # Create backgrounds which fits the maximum shifted images\n",
    "    best_red = np.zeros((red.shape[0] + 2*search_range, red.shape[1] + 2*search_range))\n",
    "    best_green = np.zeros((green.shape[0] + 2*search_range, green.shape[1] + 2*search_range))\n",
    "    best_blue = np.zeros((blue.shape[0] + 2*search_range, blue.shape[1] + 2*search_range))\n",
    "    \n",
    "    best_red[red_top_shift:-search_range*2+red_top_shift,\n",
    "             red_left_shift:-search_range*2+red_left_shift] = red\n",
    "    best_green[search_range:-search_range,\n",
    "               search_range:-search_range] = green\n",
    "    best_blue[blue_top_shift:-search_range*2+blue_top_shift,\n",
    "              blue_left_shift:-search_range*2+blue_left_shift] = blue\n",
    "    \n",
    "    optimized_img = np.stack([best_red, best_green, best_blue], axis=-1).astype(int)\n",
    "    \n",
    "    return optimized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_imgs_mse = []\n",
    "for img in rgb_imgs:\n",
    "    final_img = get_best_image(img, 'mse')\n",
    "    final_imgs_mse.append(final_img)\n",
    "\n",
    "visualize(final_imgs_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_imgs_cc = []\n",
    "for img in rgb_imgs:\n",
    "    final_img = get_best_image(img, 'cc')\n",
    "    final_imgs_cc.append(final_img)\n",
    "\n",
    "visualize(final_imgs_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one should not compare the numerical values of the error metrics, we do a visual comparison instead.\n",
    "By visual inspection we observe that both the mean square error and the cross correlation gives a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Alignment (2.5 points)\n",
    "\n",
    "In this task, you have to implement face normalization and alignment. Most of the face images deceptively seem to be aligned, but since many face recognition algorithms are very sensitive to shifts and rotations, we need not only to find a face on the image but also normalize it. Besides, the neural networks usually used for recognition have fixed input size, so, the normalized face images should be resized as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are six images of faces you have to normalize. In addition, you have the coordinates of the eyes in each of the pictures. You have to rotate the image so that the eyes are on the same height, crop the square box containing the face and transform it to the size $224\\times 224.$ The eyes should be located symmetrically and in the middle of the image (on the height)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how the transformation should look like.\n",
    "\n",
    "<img src = \"https://cdn1.savepice.ru/uploads/2017/12/13/286e475ef7a4f4e59005bcf7de78742f-full.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data\n",
    "You get the images and corresponding eyes coordinates for each person. You should implement the  function $\\tt{load}$\\_$\\tt{faces}$\\_$\\tt{and}$\\_$\\tt{eyes}$ that reads the data and returns two dictionaries: the dictionary of images and the dictionary of eyes coordinates. Eyes coordinates is a list of two tuples $[(x_1,y_1),(x_2,y_2)]$.\n",
    "Both dictionaries should have filenames as the keys.\n",
    "\n",
    "$\\tt{dir}$\\_$\\tt{name}$ is the path to the directory with face images, $\\tt{eye}$\\_$\\tt{path}$ is the path to .pickle file with eyes coordinates. If these directory and file are located in the same directory as this notebook, then default arguments can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_faces_and_eyes(dir_name = 'faces_imgs', eye_path = 'eyes.pickle'):\n",
    "    \"\"\"\n",
    "    Loads the faces images and the eyes data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_name : str\n",
    "        Path relative to location of this notebook\n",
    "    eye_path : str\n",
    "        File name relative to location of this notebook\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    faces : dict\n",
    "        Dictionary of the faces on the form\n",
    "        >>> {path: np.array}\n",
    "    eyes : dict\n",
    "        Dictionary of the eyes on the form\n",
    "        >>> {path: list}\n",
    "    \"\"\"     \n",
    "    files = sorted(Path('.').joinpath(dir_name).glob('*.jpg'))\n",
    "    # NOTE: cv loads BGR, and not RGB, hence the reverse ordering\n",
    "    faces = {f.name: cv2.imread(str(f), cv2.IMREAD_COLOR)[:,:,::-1] for f in files}\n",
    "    \n",
    "    pickle_path = Path('.').joinpath('eyes.pickle')\n",
    "    with pickle_path.open('rb') as f:\n",
    "        # The protocol version used is detected automatically, so we do not\n",
    "        # have to specify it.\n",
    "        eyes = pickle.load(f)\n",
    "    \n",
    "    return faces, eyes\n",
    "    \n",
    "faces, eyes = load_faces_and_eyes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the input images look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(faces.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You may make the transformation using your own algorithm or by the following steps:\n",
    "1. Find the angle between the segment connecting two eyes and horizontal line;\n",
    "2. Rotate the image;\n",
    "3. Find the coordinates of the eyes on the rotated image\n",
    "4. Find the width and height of the box containing the face depending on the eyes coordinates\n",
    "5. Crop the box and resize it to $224\\times224$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def transform_face(image, eyes):\n",
    "    \"\"\"\n",
    "    Rotates and crops so that eyes are horizontally aligned and in the middle of the image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.array, shape (rows, cols)\n",
    "        The image to transform\n",
    "    eyes : list\n",
    "        List of the eye coordinates\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transfromed_image : np.array, shape(224, 224, 3)\n",
    "        The transformed image\n",
    "    \"\"\" \n",
    "    \n",
    "    # 1. Find the angle between the segment connecting two eyes and horizontal line\n",
    "    eye_1, eye_2 = eyes\n",
    "    eye_1_x_pos = eye_1[1]\n",
    "    eye_2_x_pos = eye_2[1]\n",
    "    if eye_1_x_pos < eye_2_x_pos:\n",
    "        left_eye = np.array(eye_1)\n",
    "        right_eye = np.array(eye_2)\n",
    "    else:\n",
    "        left_eye = np.array(eye_2)\n",
    "        right_eye = np.array(eye_1)\n",
    "        \n",
    "    horizontal_form_left_eye = np.array((left_eye[0], right_eye[1]))\n",
    "    \n",
    "    eyes_angle_deg = \\\n",
    "        np.arccos(np.linalg.norm(horizontal_form_left_eye-left_eye)/\\\n",
    "                  np.linalg.norm(right_eye-left_eye))*\\\n",
    "        180/np.pi\n",
    "    \n",
    "    eyes_angle_rad = \\\n",
    "        np.arccos(np.dot(right_eye, horizontal_form_left_eye)/\\\n",
    "                  (np.linalg.norm(right_eye)*np.linalg.norm(horizontal_form_left_eye)))\n",
    "    \n",
    "    # The method above only finds the angle between the vectors\n",
    "    # We would like to measure the angle with respect to the horizontal line\n",
    "    if left_eye[0] > right_eye[0]:\n",
    "        eyes_angle_rad = -eyes_angle_rad\n",
    "    \n",
    "    # Convert to degrees\n",
    "    eyes_angle_deg = eyes_angle_rad * 180/np.pi\n",
    "    \n",
    "    # 2. Rotate the image\n",
    "    rot_image = rotate(image, eyes_angle_deg, resize=False)\n",
    "    image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(image_center, eyes_angle_deg, 1.0)\n",
    "    rot_image = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # 3. Find the coordinates of the eyes on the rotated image\n",
    "    # We use the rotation matrix to rotate the vectors\n",
    "    # NOTE: We add one (unity) as the rotation matrix from cv is three dimensional\n",
    "    stacked_eyes = np.vstack((left_eye, right_eye))\n",
    "    eyes_w_ones = np.hstack((stacked_eyes, np.ones((stacked_eyes.shape[0], 1))))\n",
    "    rot_eyes = np.round(rot_mat.dot(eyes_w_ones.T)).T.astype(int)\n",
    "\n",
    "    # NOTE: The coordinates has been swapped during the transformation\n",
    "    rot_left_eye = rot_eyes[0, :][::-1]\n",
    "    rot_right_eye = rot_eyes[1, :][::-1]\n",
    "    \n",
    "    # 4. Find the width and height of the box containing the face depending on the eyes coordinates\n",
    "    # Based on the findings from\n",
    "    # https://upload.wikimedia.org/wikipedia/commons/0/06/AvgHeadSizes.png\n",
    "    # We have that head to chin is at the 99 percentile for men is 10 cm\n",
    "    # and that center of eye distance at the 99 percentile for men is 7.4 cm\n",
    "    # This means that the box size should include most of the heads if we multiply the\n",
    "    # center of eye distance of eyes in pixels with the head_eye_ratio\n",
    "    # However, we add some pad as people are usually not in the 99 percentile\n",
    "    pad = 3\n",
    "    head_eye_ratio = pad+10/7.4\n",
    "    eye_distance = np.linalg.norm(rot_right_eye - rot_left_eye)\n",
    "    box_size = np.floor(eye_distance*head_eye_ratio).astype(int)\n",
    "    half_size = (box_size/2).astype(int)\n",
    "    center_eyes = rot_left_eye + ((rot_right_eye - rot_left_eye)/2).astype(int)\n",
    "    \n",
    "    # 5. Crop the box and resize it to 224 x 224\n",
    "    aligned_image = rot_image[center_eyes[0]-half_size:center_eyes[0]+half_size,\n",
    "                              center_eyes[1]-half_size:center_eyes[1]+half_size,\n",
    "                              :]\n",
    "    \n",
    "    transformed_image = resize(aligned_image, (224, 224), mode='reflect', anti_aliasing=True)\n",
    "    \n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformed_imgs = []\n",
    "for i in faces:\n",
    "    img = faces[i]\n",
    "    eye = eyes[i]\n",
    "    transformed = transform_face(img, eye)\n",
    "    transformed_imgs.append(transformed)\n",
    "    \n",
    "visualize(transformed_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
