{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial keypoints detection\n",
    "\n",
    "In this task you will create facial keypoint detector based on CNN regressor.\n",
    "\n",
    "\n",
    "![title](example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script `get_data.py` unpacks data â€” images and labelled points. 6000 images are located in `images` folder and keypoint coordinates are in `gt.csv` file. Run the cell below to unpack data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import unpack\n",
    "from pathlib import Path\n",
    "cur_dir = Path('.').absolute()\n",
    "data_dir = cur_dir.joinpath('data')\n",
    "if not data_dir.is_dir():\n",
    "    unpack('data.tar.xz', cur_dir)\n",
    "else:\n",
    "    print('\"data\" is already unpacked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to read `gt.csv` file and images from `images` dir. File `gt.csv` contains header and ground truth points for every image in `images` folder. It has 29 columns. First column is a filename and next 28 columns are `x` and `y` coordinates for 14 facepoints. We will make following preprocessing:\n",
    "1. Scale all images to resolution $100 \\times 100$ pixels.\n",
    "2. Scale all coordinates to range $[-0.5; 0.5]$. To obtain that, divide all x's by width (or number of columns) of image, and divide all y's by height (or number of rows) of image and subtract 0.5 from all values.\n",
    "\n",
    "Function `load_imgs_and_keypoint` should return a tuple of two numpy arrays: `imgs` of shape `(N, 100, 100, 3)`, where `N` is the number of images and `points` of shape `(N, 28)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm_notebook\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imgs(dirname):\n",
    "    \"\"\"\n",
    "    Obtain images from dirname\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname : Path or str\n",
    "        The directory to obtain the images from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    imgs : np.array, shape (N, 100, 100, 3)\n",
    "        The N images in the dirname directory\n",
    "    orig_width : np.array, shape (N,)\n",
    "        The original widths of the images\n",
    "    orig_height : np.array, shape (N,)\n",
    "        The original heights of the images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sorting to let the filename correspond with points\n",
    "    img_paths = sorted(dirname.glob('**/*.jpg'))\n",
    "    n_imgs = len(img_paths)\n",
    "    size = 100\n",
    "    \n",
    "    # Define the shape of imgs\n",
    "    imgs = np.zeros((n_imgs, size, size, 3))\n",
    "    orig_width = np.zeros(n_imgs)\n",
    "    orig_height = np.zeros(n_imgs)\n",
    "    \n",
    "    for nr, img_path in enumerate(tqdm_notebook(img_paths)):\n",
    "        img = imread(img_path)\n",
    "        # Ensure 3 channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = gray2rgb(img)\n",
    "            \n",
    "        # Store width and height\n",
    "        orig_width[nr] = img.shape[0]\n",
    "        orig_height[nr] = img.shape[1]\n",
    "        \n",
    "        # Resize and store\n",
    "        img = resize(img, (size, size), mode='reflect', anti_aliasing=True)\n",
    "        imgs[nr, :, :, :] = img\n",
    "        \n",
    "    return imgs, orig_width, orig_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(dirname, orig_width, orig_height):\n",
    "    \"\"\"\n",
    "    Obtain points from dirname\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname : Path or str\n",
    "        The directory to obtain the points from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    points : np.array, shape (N, 28)\n",
    "        Keypoints belonging to corresponding images\n",
    "    orig_width : np.array, shape (N,)\n",
    "        The original widths of the images\n",
    "    orig_height : np.array, shape (N,)\n",
    "        The original heights of the images\n",
    "    \"\"\"\n",
    "    \n",
    "    csv_path = data_dir.joinpath('gt.csv')\n",
    "    points_df = pd.read_csv(csv_path)\n",
    "    # NOTE: The filename is sorted in the same manner as imgs are\n",
    "    # NOTE: Casting to float to use the /= operator\n",
    "    points = points_df.loc[:, [col for col in points_df.columns if col != 'filename']].values.astype(float)\n",
    "\n",
    "    # Normalize\n",
    "    for i in tqdm_notebook(range(points.shape[0])):\n",
    "        # NOTE: The columns are arranged like the following\n",
    "        #       x1 y1 x2 x2 ... x14 y14\n",
    "        # Normalize width (the xs) by taking every second column\n",
    "        points[i, ::2] /= orig_width[i]\n",
    "        # Normalize height (the ys) by taking every second column starting from 1\n",
    "        points[i, 1::2] /= orig_height[i]\n",
    "    \n",
    "    # Scale to [-0.5, 0.5]\n",
    "    points -= 0.5\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs_and_keypoints(dirname=cur_dir.joinpath('data')):\n",
    "    \"\"\"\n",
    "    Loads the images and keypoints\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname : Path or str\n",
    "        The directory to obtain the data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    imgs : np.array, shape (N, 100, 100, 3)\n",
    "        The N images in the dirname directory\n",
    "    points : np.array, shape (N, 28)\n",
    "        Keypoints belonging to imgs\n",
    "    \"\"\"\n",
    "    \n",
    "    imgs, orig_width, orig_height = get_imgs(dirname)\n",
    "    points = get_points(dirname, orig_width, orig_height)\n",
    "\n",
    "    return imgs, points\n",
    "\n",
    "imgs, points = load_imgs_and_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max {points.max():.2f} found at image {np.where(np.isclose(points, points.max()))[0][0]}')\n",
    "print(f'Min {points.min():.2f} found at image {np.where(np.isclose(points, points.min()))[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: By inspecting the images with the max and the min, we indeed find that these points are outside the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of output\n",
    "%matplotlib inline\n",
    "from skimage.io import imshow\n",
    "imshow(imgs[0])\n",
    "points[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a function to visualize points on image. Such function obtains two arguments: an image and a vector of points' coordinates and draws points on image (just like first image in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "def visualize_points(img, points):\n",
    "    \"\"\"\n",
    "    Visualizes the points on the image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (cols, rows)\n",
    "        The image\n",
    "    points : np.array, shape (28)\n",
    "        The points given like x1 y1 x2 y2 ... x14 y14\n",
    "    \"\"\"\n",
    "    # Make point pairs\n",
    "    point_pairs = list(zip(points[::2], points[1::2]))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot the image\n",
    "    ax.imshow(img)\n",
    "    # Plot the points\n",
    "    for x, y in point_pairs:\n",
    "        # Backtransform\n",
    "        x = (x+0.5)*100\n",
    "        y = (y+0.5)*100\n",
    "        # Plot\n",
    "        circle = plt.Circle((x, y), radius=1, color='r')\n",
    "        ax.add_artist(circle)\n",
    "    \n",
    "visualize_points(imgs[1], points[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to obtain train/validation split for training neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgs_train, imgs_val, points_train, points_val = train_test_split(imgs, points, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better training we will use simple data augmentation â€” flipping an image and points. Implement function flip_img which flips an image and its' points. Make sure that points are flipped correctly! For instance, points on right eye now should be points on left eye (i.e. you have to mirror coordinates and swap corresponding points on the left and right sides of the face). Visualize an example of original and flipped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img(img, points):\n",
    "    \"\"\"\n",
    "    Flips and image and its points\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (100, 100)\n",
    "        The image to flip\n",
    "    points : np.array, shape (28)\n",
    "        The points given like x1 y1 x2 y2 ... x14 y14\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    f_img : np.array, shape (100, 100)\n",
    "        The flipped image\n",
    "    f_points : np.array, shape (100, 100)\n",
    "        The flipped points given like x1 y1 x2 y2 ... x14 y14\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flip the image\n",
    "    f_img = img.copy()\n",
    "    # Flipping the x-axis will simply be to reverse the column order\n",
    "    # NOTE: We use ellipsis to have arbitrary numbers of first dimensions\n",
    "    # https://stackoverflow.com/questions/772124/what-does-the-python-ellipsis-object-do\n",
    "    f_img = f_img[..., ::-1, :]\n",
    "    \n",
    "    # Flip the points along the x-axis by negating the x coordinate\n",
    "    f_points = points.copy()\n",
    "    # NOTE: We use ellipsis to have arbitrary numbers of first dimensions\n",
    "    f_points[..., ::2] = -f_points[..., ::2] \n",
    "    \n",
    "    return f_img, f_points\n",
    "\n",
    "f_img, f_points = flip_img(imgs[1], points[1])\n",
    "visualize_points(f_img, f_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to augment our training sample. Apply flip to every image in training sample. As a result you should obtain two arrays: `aug_imgs_train` and `aug_points_train` which contain original images and points along with flipped ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_imgs_train, aug_points_train = flip_img(imgs_train, points_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(aug_imgs_train[2], aug_points_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(aug_imgs_train[3], aug_points_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training data and validation data\n",
    "x_train = np.concatenate((imgs_train, aug_imgs_train), axis=0)/255\n",
    "x_val = imgs_val/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((points_train, aug_points_train), axis=0)\n",
    "y_val = points_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture and training\n",
    "\n",
    "Now let's define neural network regressor. It will have 28 outputs, 2 numbers per point. The precise architecture is up to you. We recommend to add 2-3 (`Conv2D` + `MaxPooling2D`) pairs, then `Flatten` and 2-3 `Dense` layers. Don't forget about ReLU activations. We also recommend to add `Dropout` to every `Dense` layer (with p from 0.2 to 0.5) to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, loss='mse', optimizer='adadelta'):\n",
    "    \"\"\"\n",
    "    Compiles the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    loss : str or function\n",
    "        The loss function\n",
    "    optimizer : str or keras.optimizer\n",
    "        The optimizer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The compiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "def fit_model(model,\n",
    "              name,\n",
    "              x_train_=x_train,\n",
    "              y_train_=y_train,\n",
    "              x_val_=x_val,\n",
    "              y_val_=y_val,\n",
    "              patience=7,\n",
    "              batch_size=64,\n",
    "              epochs=100):\n",
    "    \"\"\"\n",
    "    Fits the model\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    x_train, y_train, x_val and y_val must be global variables\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sequential\n",
    "        The compiled model\n",
    "    name : str\n",
    "        Name of the model\n",
    "    x_train_ : array-like\n",
    "        The training data\n",
    "    y_train_ :array-like\n",
    "        The target data for training\n",
    "    x_val_ : array-like\n",
    "        The validation data\n",
    "    y_val_ :array-like\n",
    "        The target data for validation\n",
    "    patience : int\n",
    "        How long to wait for non-imporving validation error\n",
    "    batch_size : int\n",
    "        The batch size\n",
    "    epochs : int\n",
    "        Number of epochs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The fitted model\n",
    "    history : dict\n",
    "        The history of the fit\n",
    "    \"\"\"\n",
    "    \n",
    "    model_dir = Path('.').absolute().joinpath('model')\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    model_path = model_dir.joinpath(f'{name}.h5')\n",
    "    history_path = model_dir.joinpath(f'{name}.pickle')\n",
    "    \n",
    "    if not model_path.is_file():\n",
    "        stopper = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='auto', baseline=None)\n",
    "        checkpointer = ModelCheckpoint(filepath=str(model_path),\n",
    "                                       verbose=1, \n",
    "                                       save_best_only=True)\n",
    "        history = model.fit(x_train_,\n",
    "                            y_train_,\n",
    "                            validation_data=(x_val_, y_val_),\n",
    "                            batch_size=64,\n",
    "                            epochs=100,\n",
    "                            shuffle=True,\n",
    "                            callbacks=[checkpointer, stopper])\n",
    "        \n",
    "        history = history.history\n",
    "        with history_path.open('wb') as f:\n",
    "            pickle.dump(history, f, pickle.HIGHEST_PROTOCOL)  \n",
    "    else:\n",
    "        model = load_model(str(model_path))\n",
    "        with history_path.open('rb') as f:\n",
    "            history = pickle.load(f)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plots the history\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    history : history\n",
    "        The history of the fit\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_val_mse : float\n",
    "        The best validation mse\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # NOTE: When regularizers are used, the loss is no longer the same as mse\n",
    "    ax.plot(history['mean_squared_error'], label='train_mse')\n",
    "    ax.plot(history['val_mean_squared_error'], label='val_mse')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Mean squared error')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True)\n",
    "    _ = ax.legend(loc='lower left')\n",
    "    \n",
    "    best_val_mse = np.array(history['val_mean_squared_error']).min()\n",
    "    \n",
    "    return best_val_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train! Since we are training a regressor, make sure that you use mean squared error (mse) as loss. Feel free to experiment with optimization method (SGD, Adam, etc.) and its' parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet-like model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled alexnet-like model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=96,\n",
    "                     kernel_size=11,\n",
    "                     strides=4,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=3, \n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=256,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=3,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=384,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(filters=384,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(filters=256,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=3,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(9216, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_double(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet-like with more pooling and more fully connected layers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=120,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_double_batch_normalization(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_double, but with batch normalization\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=120,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_double_bn_leaky(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_double_batch_normalization, but with leaky ReLU activation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=120,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(500))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_d_bn_l_more_filters(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_double_bn_leaky, but with more filters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=256,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(500))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_d_bn_l_more_filters_reg(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_d_bn_l_more_filters, but with regularization in the last layer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=256,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(500))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, \n",
    "                    activation='linear',\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    activity_regularizer=regularizers.l1(0.01)))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_d_bn_l_more_filters_less_fc(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_d_bn_l_more_filters, but with less nodes in the second last fc layer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=256,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(400))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(500))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_d_bn_l_mf_2(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_d_bn_l_more_filters, but with more filters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=512,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(500))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_pow_2(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_d_bn_l_more_filters, but nodes increasing by the power of 2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_pow_2_k_4(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_pow_2, but with kernel size of 4\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_pow_2_k_4_a_1(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_pow_2_k_4, but with alpha=0.1 in the leaky ReLU activations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_pow_2_k_4_double_one(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_pow_2_k_4, but with kernel size of 4\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_pow_2_k_4_double_all(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_pow_2_k_4, but with kernel size of 4\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_pow_2_k_4_large_fc(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet_pow_2_k_4, but with large fully connectors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=4,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1000))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_model = lenet(imgs_train.shape[1:])\n",
    "lenet_model = compile_model(lenet_model)\n",
    "lenet_model, lenet_history = fit_model(lenet_model, 'lenet')\n",
    "lenet_mse = plot_history(lenet_history)\n",
    "del(lenet_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alexnet_model = alexnet(imgs_train.shape[1:])\n",
    "alexnet_model = compile_model(alexnet_model)\n",
    "alexnet_model, alexnet_history = fit_model(alexnet_model, 'alexnet')\n",
    "alexnet_mse = plot_history(alexnet_history)\n",
    "del(alexnet_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_double_model = lenet_double(imgs_train.shape[1:])\n",
    "lenet_double_model = compile_model(lenet_double_model)\n",
    "lenet_double_model, lenet_double_history = fit_model(lenet_double_model, 'lenet_double')\n",
    "lenet_double_mse = plot_history(lenet_double_history)\n",
    "del(lenet_double_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_double_batch_normalization_model = lenet_double_batch_normalization(imgs_train.shape[1:])\n",
    "lenet_double_batch_normalization_model = compile_model(lenet_double_batch_normalization_model)\n",
    "lenet_double_batch_normalization_model, lenet_double_batch_normalization_history = fit_model(lenet_double_batch_normalization_model, 'lenet_double_batch_normalization')\n",
    "lenet_double_batch_normalization_mse = plot_history(lenet_double_batch_normalization_history)\n",
    "del(lenet_double_batch_normalization_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_double_batch_norm_high_dropout_model = lenet_double_batch_normalization(imgs_train.shape[1:], dropout=0.5)\n",
    "lenet_double_batch_norm_high_dropout_model = compile_model(lenet_double_batch_norm_high_dropout_model)\n",
    "lenet_double_batch_norm_high_dropout_model, lenet_double_batch_norm_high_dropout_history = fit_model(lenet_double_batch_norm_high_dropout_model, 'lenet_double_batch_norm_high_dropout')\n",
    "lenet_double_batch_norm_high_dropout_mse = plot_history(lenet_double_batch_norm_high_dropout_history)\n",
    "del(lenet_double_batch_norm_high_dropout_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_double_bn_leaky_high_do_model = lenet_double_bn_leaky(imgs_train.shape[1:])\n",
    "lenet_double_bn_leaky_high_do_model = compile_model(lenet_double_bn_leaky_high_do_model)\n",
    "lenet_double_bn_leaky_high_do_model, lenet_double_bn_leaky_high_do_history = fit_model(lenet_double_bn_leaky_high_do_model, 'lenet_bn_leaky_high_do')\n",
    "lenet_double_bn_leaky_high_do_mse = plot_history(lenet_double_bn_leaky_high_do_history)\n",
    "del(lenet_double_bn_leaky_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_d_bn_l_more_filters_high_do_model = lenet_d_bn_l_more_filters(imgs_train.shape[1:])\n",
    "lenet_d_bn_l_more_filters_high_do_model = compile_model(lenet_d_bn_l_more_filters_high_do_model)\n",
    "lenet_d_bn_l_more_filters_high_do_model, lenet_d_bn_l_more_filters_high_do_history = fit_model(lenet_d_bn_l_more_filters_high_do_model, 'lenet_d_bn_l_more_filters_high_do')\n",
    "lenet_d_bn_l_more_filters_high_do_mse = plot_history(lenet_d_bn_l_more_filters_high_do_history)\n",
    "del(lenet_d_bn_l_more_filters_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_d_bn_l_more_filters_high_do_high_batch_model = lenet_d_bn_l_more_filters(imgs_train.shape[1:])\n",
    "lenet_d_bn_l_more_filters_high_do_high_batch_model = compile_model(lenet_d_bn_l_more_filters_high_do_high_batch_model)\n",
    "lenet_d_bn_l_more_filters_high_do_high_batch_model, lenet_d_bn_l_more_filters_high_do_high_batch_history = fit_model(lenet_d_bn_l_more_filters_high_do_high_batch_model, 'lenet_d_bn_l_more_filters_high_do_high_batch', batch_size=128)\n",
    "lenet_d_bn_l_more_filters_high_do_high_batch_mse = plot_history(lenet_d_bn_l_more_filters_high_do_high_batch_history)\n",
    "del(lenet_d_bn_l_more_filters_high_do_high_batch_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_d_bn_l_more_filters_reg_high_do_model = lenet_d_bn_l_more_filters_reg(imgs_train.shape[1:])\n",
    "lenet_d_bn_l_more_filters_reg_high_do_model = compile_model(lenet_d_bn_l_more_filters_reg_high_do_model)\n",
    "lenet_d_bn_l_more_filters_reg_high_do_model, lenet_d_bn_l_more_filters_reg_high_do_history = fit_model(lenet_d_bn_l_more_filters_reg_high_do_model, 'lenet_d_bn_l_more_filters_reg_high_do')\n",
    "lenet_d_bn_l_more_filters_reg_high_do_mse = plot_history(lenet_d_bn_l_more_filters_reg_high_do_history)\n",
    "del(lenet_d_bn_l_more_filters_reg_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_d_bn_l_more_filters_less_fc_high_do_model = lenet_d_bn_l_more_filters_less_fc(imgs_train.shape[1:])\n",
    "lenet_d_bn_l_more_filters_less_fc_high_do_model = compile_model(lenet_d_bn_l_more_filters_less_fc_high_do_model)\n",
    "lenet_d_bn_l_more_filters_less_fc_high_do_model, lenet_d_bn_l_more_filters_less_fc_high_do_history = fit_model(lenet_d_bn_l_more_filters_less_fc_high_do_model, 'lenet_d_bn_l_more_filters_less_fc_high_do')\n",
    "lenet_d_bn_l_more_filters_less_fc_high_do_mse = plot_history(lenet_d_bn_l_more_filters_less_fc_high_do_history)\n",
    "del(lenet_d_bn_l_more_filters_less_fc_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_d_bn_l_mf_2_high_do_model = lenet_d_bn_l_mf_2(imgs_train.shape[1:])\n",
    "lenet_d_bn_l_mf_2_high_do_model = compile_model(lenet_d_bn_l_mf_2_high_do_model)\n",
    "lenet_d_bn_l_mf_2_high_do_model, lenet_d_bn_l_mf_2_high_do_history = fit_model(lenet_d_bn_l_mf_2_high_do_model, 'lenet_d_bn_l_mf_2_high_do')\n",
    "lenet_d_bn_l_mf_2_high_do_mse = plot_history(lenet_d_bn_l_mf_2_high_do_history)\n",
    "del(lenet_d_bn_l_mf_2_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_pow_2_high_do_model = lenet_pow_2(imgs_train.shape[1:])\n",
    "lenet_pow_2_high_do_model = compile_model(lenet_pow_2_high_do_model)\n",
    "lenet_pow_2_high_do_model, lenet_pow_2_high_do_history = fit_model(lenet_pow_2_high_do_model, 'lenet_pow_2_high_do')\n",
    "lenet_pow_2_high_do_mse = plot_history(lenet_pow_2_high_do_history)\n",
    "del(lenet_pow_2_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_pow_2_k_4_high_do_model = lenet_pow_2_k_4(imgs_train.shape[1:])\n",
    "lenet_pow_2_k_4_high_do_model = compile_model(lenet_pow_2_k_4_high_do_model)\n",
    "lenet_pow_2_k_4_high_do_model, lenet_pow_2_k_4_high_do_history = fit_model(lenet_pow_2_k_4_high_do_model, 'lenet_pow_2_k_4_high_do')\n",
    "lenet_pow_2_k_4_high_do_mse = plot_history(lenet_pow_2_k_4_high_do_history)\n",
    "del(lenet_pow_2_k_4_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_pow_2_k_4_a_1_high_do_model = lenet_pow_2_k_4_a_1(imgs_train.shape[1:])\n",
    "lenet_pow_2_k_4_a_1_high_do_model = compile_model(lenet_pow_2_k_4_a_1_high_do_model)\n",
    "lenet_pow_2_k_4_a_1_high_do_model, lenet_pow_2_k_4_a_1_high_do_history = fit_model(lenet_pow_2_k_4_a_1_high_do_model, 'lenet_pow_2_k_4_a_1_high_do')\n",
    "lenet_pow_2_k_4_a_1_high_do_mse = plot_history(lenet_pow_2_k_4_a_1_high_do_history)\n",
    "del(lenet_pow_2_k_4_a_1_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_pow_2_k_4_double_one_high_do_model = lenet_pow_2_k_4_double_one(imgs_train.shape[1:])\n",
    "lenet_pow_2_k_4_double_one_high_do_model = compile_model(lenet_pow_2_k_4_double_one_high_do_model)\n",
    "lenet_pow_2_k_4_double_one_high_do_model, lenet_pow_2_k_4_double_one_high_do_history = fit_model(lenet_pow_2_k_4_double_one_high_do_model, 'lenet_pow_2_k_4_double_one_high_do', patience=10)\n",
    "lenet_pow_2_k_4_double_one_high_do_mse = plot_history(lenet_pow_2_k_4_double_one_high_do_history)\n",
    "del(lenet_pow_2_k_4_double_one_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_pow_2_k_4_double_all_high_do_model = lenet_pow_2_k_4_double_all(imgs_train.shape[1:])\n",
    "lenet_pow_2_k_4_double_all_high_do_model = compile_model(lenet_pow_2_k_4_double_all_high_do_model)\n",
    "lenet_pow_2_k_4_double_all_high_do_model, lenet_pow_2_k_4_double_all_high_do_history = fit_model(lenet_pow_2_k_4_double_all_high_do_model, 'lenet_pow_2_k_4_double_all_high_do', patience=10)\n",
    "lenet_pow_2_k_4_double_all_high_do_mse = plot_history(lenet_pow_2_k_4_double_all_high_do_history)\n",
    "del(lenet_pow_2_k_4_double_all_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gray = rgb2gray(x_train)[..., np.newaxis]\n",
    "x_val_gray = rgb2gray(x_val)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_pow_2_k_4_high_do_gray_model = lenet_pow_2_k_4(x_train_gray.shape[1:])\n",
    "lenet_pow_2_k_4_high_do_gray_model = compile_model(lenet_pow_2_k_4_high_do_gray_model)\n",
    "lenet_pow_2_k_4_high_do_gray_model, lenet_pow_2_k_4_high_do_gray_history = \\\n",
    "    fit_model(lenet_pow_2_k_4_high_do_gray_model, \n",
    "              'lenet_pow_2_k_4_high_do_gray',\n",
    "              x_train_=x_train_gray,\n",
    "              x_val_=x_val_gray, \n",
    "              patience=10)\n",
    "lenet_pow_2_k_4_high_do_gray_mse = plot_history(lenet_pow_2_k_4_high_do_gray_history)\n",
    "del(lenet_pow_2_k_4_high_do_gray_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lenet_pow_2_k_4_large_fc_high_do_model = lenet_pow_2_k_4_large_fc(imgs_train.shape[1:])\n",
    "lenet_pow_2_k_4_large_fc_high_do_model = compile_model(lenet_pow_2_k_4_large_fc_high_do_model)\n",
    "lenet_pow_2_k_4_large_fc_high_do_model, lenet_pow_2_k_4_large_fc_high_do_history = fit_model(lenet_pow_2_k_4_large_fc_high_do_model, 'lenet_pow_2_k_4_large_fc_high_do')\n",
    "lenet_pow_2_k_4_large_fc_high_do_mse = plot_history(lenet_pow_2_k_4_large_fc_high_do_history)\n",
    "del(lenet_pow_2_k_4_large_fc_high_do_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'lenet_mse': lenet_mse,\n",
    "           'alexnet_mse': alexnet_mse,\n",
    "           'lenet_double_mse': lenet_double_mse,\n",
    "           'lenet_double_batch_normalization_mse': lenet_double_batch_normalization_mse,\n",
    "           'lenet_double_batch_norm_high_dropout_mse': lenet_double_batch_norm_high_dropout_mse,\n",
    "           'lenet_double_bn_leaky_high_do_mse': lenet_double_bn_leaky_high_do_mse,\n",
    "           'lenet_d_bn_l_more_filters_high_do_mse': lenet_d_bn_l_more_filters_high_do_mse,\n",
    "           'lenet_d_bn_l_more_filters_high_do_high_batch_mse': lenet_d_bn_l_more_filters_high_do_high_batch_mse,\n",
    "           'lenet_d_bn_l_more_filters_reg_high_do_mse': lenet_d_bn_l_more_filters_reg_high_do_mse,\n",
    "           'lenet_d_bn_l_more_filters_less_fc_high_do_mse': lenet_d_bn_l_more_filters_less_fc_high_do_mse,\n",
    "           'lenet_d_bn_l_mf_2_high_do_mse': lenet_d_bn_l_mf_2_high_do_mse,\n",
    "           'lenet_pow_2_high_do_mse': lenet_pow_2_high_do_mse,\n",
    "           'lenet_pow_2_k_4_high_do_mse': lenet_pow_2_k_4_high_do_mse,\n",
    "           'lenet_pow_2_k_4_a_1_high_do_mse': lenet_pow_2_k_4_a_1_high_do_mse,\n",
    "           'lenet_pow_2_k_4_double_one_high_do_mse': lenet_pow_2_k_4_double_one_high_do_mse,\n",
    "           'lenet_pow_2_k_4_double_all_high_do_mse': lenet_pow_2_k_4_double_all_high_do_mse,\n",
    "           'lenet_pow_2_k_4_high_do_gray_mse': lenet_pow_2_k_4_high_do_gray_mse,\n",
    "           'lenet_pow_2_k_4_large_fc_high_do_mse': lenet_pow_2_k_4_large_fc_high_do_mse\n",
    "          }\n",
    "\n",
    "results = dict(sorted(results.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(results.keys(), results.values())\n",
    "ax.set_ylabel('Mean squared error')\n",
    "ax.set_xlabel('Model')\n",
    "ax.grid(True)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize neural network results on several images from validation sample. Make sure that your network outputs different points for images (i.e. it doesn't output some constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: It would be most appropriate to visualize on a test set as we can overfit to the validation set by trying out different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_pow_2_k_4_high_do_model = lenet_pow_2_k_4(imgs_train.shape[1:])\n",
    "lenet_pow_2_k_4_high_do_model = compile_model(lenet_pow_2_k_4_high_do_model)\n",
    "model, _ = fit_model(lenet_pow_2_k_4_high_do_model, 'lenet_pow_2_k_4_high_do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take 10 random images from the validation set\n",
    "indices = random.sample(range(x_val.shape[0]), 10)\n",
    "\n",
    "test = []\n",
    "for ind in indices:\n",
    "    # NOTE: We are keeping the original dimension with ind:ind+1\n",
    "    predictions = model.predict(x_val[ind:ind+1, ...])[0]\n",
    "    test.append(predictions)\n",
    "    visualize_points(x_val[ind, ...]*255, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
