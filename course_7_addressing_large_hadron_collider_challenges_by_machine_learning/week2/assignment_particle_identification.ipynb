{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "In this programming assignment you will train a classifier to identify type of a particle. There are six particle types: electron, proton, muon, kaon, pion and ghost. Ghost is a particle with other type than the first five or a detector noise. \n",
    "\n",
    "Different particle types remain different responses in the detector systems or subdetectors. Thre are five systems: tracking system, ring imaging Cherenkov detector (RICH), electromagnetic and hadron calorimeters, and muon system.\n",
    "\n",
    "![pid](pic/pid.jpg)\n",
    "\n",
    "You task is to identify a particle type using the responses in the detector systems. \n",
    "\n",
    "# Attention\n",
    "\n",
    "Data files you should download from https://github.com/hse-aml/hadron-collider-machine-learning/releases/tag/Week_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/hse-aml/hadron-collider-machine-learning/releases/download/Week_2/test.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/hse-aml/hadron-collider-machine-learning/releases/download/Week_2/training.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns\n",
    "pandas.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Download data used to train classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('training.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of columns in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **Spd** stands for Scintillating Pad Detector, **Prs** - Preshower, **Ecal** - electromagnetic calorimeter, **Hcal** - hadronic calorimeter, **Brem** denotes traces of the particles that were deflected by detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ID - id value for tracks (presents only in the test file for the submitting purposes)\n",
    "- Label - string valued observable denoting particle types. Can take values \"Electron\", \"Muon\", \"Kaon\", \"Proton\", \"Pion\" and \"Ghost\". This column is absent in the test file.\n",
    "- FlagSpd - flag (0 or 1), if reconstructed track passes through Spd\n",
    "- FlagPrs - flag (0 or 1), if reconstructed track passes through Prs\n",
    "- FlagBrem - flag (0 or 1), if reconstructed track passes through Brem\n",
    "- FlagEcal - flag (0 or 1), if reconstructed track passes through Ecal\n",
    "- FlagHcal - flag (0 or 1), if reconstructed track passes through Hcal\n",
    "- FlagRICH1 - flag (0 or 1), if reconstructed track passes through the first RICH detector\n",
    "- FlagRICH2 - flag (0 or 1), if reconstructed track passes through the second RICH detector\n",
    "- FlagMuon - flag (0 or 1), if reconstructed track passes through muon stations (Muon)\n",
    "- SpdE - energy deposit associated to the track in the Spd\n",
    "- PrsE - energy deposit associated to the track in the Prs\n",
    "- EcalE - energy deposit associated to the track in the Hcal\n",
    "- HcalE - energy deposit associated to the track in the Hcal\n",
    "- PrsDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Prs\n",
    "- BremDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Brem\n",
    "- TrackP - particle momentum\n",
    "- TrackPt - particle transverse momentum\n",
    "- TrackNDoFSubdetector1  - number of degrees of freedom for track fit using hits in the tracking sub-detector1\n",
    "- TrackQualitySubdetector1 - chi2 quality of the track fit using hits in the tracking sub-detector1\n",
    "- TrackNDoFSubdetector2 - number of degrees of freedom for track fit using hits in the tracking sub-detector2\n",
    "- TrackQualitySubdetector2 - chi2 quality of the track fit using hits in the  tracking sub-detector2\n",
    "- TrackNDoF - number of degrees of freedom for track fit using hits in all tracking sub-detectors\n",
    "- TrackQualityPerNDoF - chi2 quality of the track fit per degree of freedom\n",
    "- TrackDistanceToZ - distance between track and z-axis (beam axis)\n",
    "- Calo2dFitQuality - quality of the 2d fit of the clusters in the calorimeter \n",
    "- Calo3dFitQuality - quality of the 3d fit in the calorimeter with assumption that particle was electron\n",
    "- EcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Ecal\n",
    "- EcalDLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from Ecal\n",
    "- EcalShowerLongitudinalParameter - longitudinal parameter of Ecal shower\n",
    "- HcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Hcal\n",
    "- HcalDLLbeMuon - delta log-likelihood for a particle candidate to be using information from Hcal\n",
    "- RICHpFlagElectron - flag (0 or 1) if momentum is greater than threshold for electrons to produce Cherenkov light\n",
    "- RICHpFlagProton - flag (0 or 1) if momentum is greater than threshold for protons to produce Cherenkov light\n",
    "- RICHpFlagPion - flag (0 or 1) if momentum is greater than threshold for pions to produce Cherenkov light\n",
    "- RICHpFlagKaon - flag (0 or 1) if momentum is greater than threshold for kaons to produce Cherenkov light\n",
    "- RICHpFlagMuon - flag (0 or 1) if momentum is greater than threshold for muons to produce Cherenkov light\n",
    "- RICH_DLLbeBCK  - delta log-likelihood for a particle candidate to be background using information from RICH\n",
    "- RICH_DLLbeKaon - delta log-likelihood for a particle candidate to be kaon using information from RICH\n",
    "- RICH_DLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from RICH\n",
    "- RICH_DLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from RICH\n",
    "- RICH_DLLbeProton - delta log-likelihood for a particle candidate to be proton using information from RICH\n",
    "- MuonFlag - muon flag (is this track muon) which is determined from muon stations\n",
    "- MuonLooseFlag muon flag (is this track muon) which is determined from muon stations using looser criteria\n",
    "- MuonLLbeBCK - log-likelihood for a particle candidate to be not muon using information from muon stations\n",
    "- MuonLLbeMuon - log-likelihood for a particle candidate to be muon using information from muon stations\n",
    "- DLLelectron - delta log-likelihood for a particle candidate to be electron using information from all subdetectors\n",
    "- DLLmuon - delta log-likelihood for a particle candidate to be muon using information from all subdetectors\n",
    "- DLLkaon - delta log-likelihood for a particle candidate to be kaon using information from all subdetectors\n",
    "- DLLproton - delta log-likelihood for a particle candidate to be proton using information from all subdetectors\n",
    "- GhostProbability - probability for a particle candidate to be ghost track. This variable is an output of classification model used in the tracking algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta log-likelihood in the features descriptions means the difference between log-likelihood for the mass hypothesis that a given track is left by some particle (for example, electron) and log-likelihood for the mass hypothesis that a given track is left by a pion (so, DLLpion = 0 and thus we don't have these columns). This is done since most tracks (~80%) are left by pions and in practice we actually need to discriminate other particles from pions. In other words, the null hypothesis is that particle is a pion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the labels set\n",
    "\n",
    "The training data contains six classes. Each class corresponds to a particle type. Your task is to predict type of a particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the particle types into class numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'] = utils.get_class_ids(data.Label.values)\n",
    "set(data.Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training features\n",
    "\n",
    "The following set of features describe particle responses in the detector systems:\n",
    "\n",
    "![features](pic/features.jpeg)\n",
    "\n",
    "Also there are several combined features. The full list is following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(data.columns) - {'Label', 'Class'})\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide training data into 2 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split(data, random_state=11, train_size=0.90, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data), len(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn classifier\n",
    "\n",
    "On this step your task is to train **Sklearn** classifier to provide lower **log loss** value.\n",
    "\n",
    "\n",
    "TASK: your task is to tune the classifier parameters to achieve the lowest **log loss** value on the validation sample you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                n_estimators=50, \n",
    "                                subsample=0.3, \n",
    "                                random_state=13,\n",
    "                                min_samples_leaf=100,\n",
    "                                max_depth=3,\n",
    "                                verbose=3)\n",
    "gb.fit(training_data[features].values, training_data.Class.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss on the cross validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict each track\n",
    "proba_gb = gb.predict_proba(validation_data[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(validation_data.Class.values, proba_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'learning_rate': [0.05, 0.1, 0.2], \n",
    "              'n_estimators': [25, 50, 75], \n",
    "              'subsample': [0.1, 0.3, 0.6], \n",
    "              'random_state': [13],\n",
    "              'min_samples_leaf': [50, 100, 200],\n",
    "              'max_depth': [1, 3, 6],\n",
    "              'verbose': [3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search:\n",
    "    random_search = RandomizedSearchCV(GradientBoostingClassifier(),\n",
    "                                       param_distributions=param_dist,\n",
    "                                       # See \n",
    "                                       # https://stackoverflow.com/questions/43081251/sklearn-metrics-log-loss-is-positive-vs-scoring-neg-log-loss-is-negative\n",
    "                                       # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "                                       scoring='neg_log_loss', \n",
    "                                       n_iter=10,\n",
    "                                       n_jobs=-1,  # NOTE: You should monitor the memory so it doesn't get saturated\n",
    "                                       refit=True,  # Refit with the best parameters on the whole data set (default)\n",
    "                                       return_train_score=True,\n",
    "                                       cv=3)\n",
    "    \n",
    "    # NOTE: Here we fit on the data from training.csv.gz and let the RandomizedSearchCV do the splitting\n",
    "    random_search.fit(data.loc[:, features].values,\n",
    "                      data.loc[:, 'Class'].values)\n",
    "    \n",
    "    print(random_search.best_params_)\n",
    "    print(random_search.best_score_)\n",
    "    pandas.DataFrame(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random search gives the following best parameters\n",
    "\n",
    "```\n",
    "{'verbose': 3,\n",
    " 'subsample': 0.3,\n",
    " 'random_state': 13,\n",
    " 'n_estimators': 50,\n",
    " 'min_samples_leaf': 50,\n",
    " 'max_depth': 6,\n",
    " 'learning_rate': 0.2}\n",
    "```\n",
    "\n",
    "with the score\n",
    "\n",
    "```\n",
    "0.5808503911824091\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras neural network\n",
    "\n",
    "On this step your task is to train **Keras** NN classifier to provide lower **log loss** value.\n",
    "\n",
    "\n",
    "TASK: your task is to tune the classifier parameters to achieve the lowest **log loss** value on the validation sample you can. Data preprocessing may help you to improve your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=input_dim))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = nn_model(len(features))\n",
    "nn.fit(training_data[features].values, \n",
    "       np_utils.to_categorical(training_data.Class.values), \n",
    "       verbose=1, \n",
    "       epochs=5, \n",
    "       batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss on the cross validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict each track\n",
    "proba_nn = nn.predict_proba(validation_data[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(validation_data.Class.values, proba_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    epochs_range = range(len(history.history['val_loss']))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_range, history.history['val_loss'], label='val loss')\n",
    "    ax.plot(epochs_range, history.history['loss'], label='train loss')\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0, \n",
    "                              patience=10, \n",
    "                              verbose=1, \n",
    "                              mode='auto',\n",
    "                              restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model_v2(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(180, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(rate=.30))\n",
    "    \n",
    "    model.add(Dense(180))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(rate=.30))\n",
    "    \n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='model_v2.h5',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "nn_v2 = nn_model_v2(len(features))\n",
    "\n",
    "history_v2 = nn_v2.fit(training_data[features].values, \n",
    "                       np_utils.to_categorical(training_data.Class.values), \n",
    "                       validation_data=(validation_data[features].values,\n",
    "                                        np_utils.to_categorical(validation_data.Class.values)),\n",
    "                       verbose=1, \n",
    "                       epochs=100, \n",
    "                       callbacks=[checkpointer, early_stopper],\n",
    "                       batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality metrics\n",
    "\n",
    "Plot ROC curves and signal efficiency dependece from particle mometum and transverse momentum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = proba_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_roc_curves(proba, validation_data.Class.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_signal_efficiency_on_p(proba, validation_data.Class.values, validation_data.TrackP.values, 60, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_signal_efficiency_on_pt(proba, validation_data.Class.values, validation_data.TrackPt.values, 60, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare submission\n",
    "\n",
    "Select your best classifier and prepare submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pandas.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test sample\n",
    "submit_proba = best_model.predict_proba(test[features])\n",
    "submit_ids = test.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "utils.create_solution(submit_ids, submit_proba, filename='submission_file.csv.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
