{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will derive and implement formulas for Gaussian Mixture Model â€” one of the most commonly used methods for performing soft clustering of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "We will need ```numpy```, ```scikit-learn```, ```matplotlib``` libraries for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from grader import Grader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instance below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to the platform only after running submitting function in the last part of this assignment. If you want to make a partial submission, you can run that cell anytime you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing EM for GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging we will use samples from gaussian mixture model with unknown mean, variance and priors. We also added inital values of parameters for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.load('samples.npz')\n",
    "X = samples['data']\n",
    "pi0 = samples['pi0']\n",
    "mu0 = samples['mu0']\n",
    "sigma0 = samples['sigma0']\n",
    "plt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that EM algorithm is a coordinate descent optimization of variational lower bound $\\mathcal{L}(\\theta, q) = \\int q(T) \\log\\frac{P(X, T|\\theta)}{q(T)}dT\\to \\max$.\n",
    "\n",
    "<b>E-step</b>:<br>\n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow \\mathcal{KL} [q(T) \\,\\|\\, p(T|X, \\theta)] \\to \\min \\limits_{q\\in Q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
    "<b>M-step</b>:<br> \n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$\n",
    "\n",
    "For GMM, $\\theta$ is a set of parameters that consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
    "\n",
    "Latent variables $T$ are indices of components to which each data point is assigned. $T_i$ (cluster index for object $i$) is a binary vector with only one active bit in position corresponding to the true component. For example, if we have $C=3$ components and object $i$ lies in first component, $T_i = [1, 0, 0]$.\n",
    "\n",
    "The joint distribution can be written as follows: $p(T, X \\mid \\theta) =  \\prod\\limits_{i=1}^N p(T_i, X_i \\mid \\theta) = \\prod\\limits_{i=1}^N \\prod\\limits_{c=1}^C [\\pi_c \\mathcal{N}(X_i \\mid \\mu_c, \\Sigma_c)]^{T_{ic}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step\n",
    "In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q(T) = p(T|X, \\theta)$. We will assume that $T_i$ (cluster index for object $i$) is a binary vector with only one '1' in position corresponding to the true component. To do so we need to compute $\\gamma_{ic} = P(T_{ic} = 1 \\mid X, \\theta)$. Note that $\\sum\\limits_{c=1}^C\\gamma_{ic}=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Important trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$. When you compute exponents of large numbers, you get huge numerical errors (some numbers will simply become infinity). You can avoid this by dividing numerator and denominator by $e^{\\max(x)}$: $\\frac{e^{x_i-\\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. This trick is called log-sum-exp. So, to compute desired formula you first subtract maximum value from each component in vector $X$ and then compute everything else as before.\n",
    "\n",
    "<b>Important trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to solve the equation $Ay = x$. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by Gaussian elimination procedure. You can use ```np.linalg.solve``` for this.\n",
    "\n",
    "<b>Other usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task 1:</b> Implement E-step for GMM using template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian(X, mu, sigma):\n",
    "    \"\"\"\n",
    "    Returns a Gaussian of X, given mu and sigma\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape (N, d)\n",
    "        Data points \n",
    "    mu : np.array, shape (d, )\n",
    "        The means\n",
    "    sigma: np.array, shape (d, d)\n",
    "        The covariance matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    gaussian : np.array, shape (N,)\n",
    "        The Gaussian\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    # NOTE: The Gaussian is defined as 1/((2pi)^N |Sigma|)exp(-(1/2)(x-mu)^T Sigma^-1(x-mu))\n",
    "    # We used signed log determinant for robustness\n",
    "    sign, logdet = np.linalg.slogdet(sigma)\n",
    "    det = sign * np.exp(logdet)\n",
    "        \n",
    "    # NOTE: Subtract all columns in the matrix with the vector\n",
    "    #       Equivalent to A - xI\n",
    "    x_minus_mu = X - mu\n",
    "    \n",
    "    # Let's define y = Sigma^-1(x-mu)\n",
    "    # NOTE: We transpose here in order for the dimensions to match \n",
    "    #       (we are taking the dot product of the dimensionality of x-mu, not of the number of points)\n",
    "    y = np.linalg.solve(sigma, x_minus_mu.transpose())\n",
    "\n",
    "    # NOTE: The Gaussian maps from R^d to R^1\n",
    "    #       y has the shape (d, N)\n",
    "    #       x_minus_mu has the shape (N, d)\n",
    "    #       The gaussian is to output something of shape (N,)\n",
    "    #       As we take a vector with dimension d in and want a scalar, we would like to take the\n",
    "    #       dot product \"element-wise\"\n",
    "    x_minus_mu_dot_y = np.empty(N)\n",
    "    for i in range(N):\n",
    "        x_minus_mu_dot_y[i] = x_minus_mu[i, :]@y[:, i]\n",
    "        \n",
    "    gaussian = (1/(det*(2*np.pi)**N)**0.5)*np.exp(-(1/2)*x_minus_mu_dot_y)\n",
    "    \n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Performs E-step on GMM model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X : np.array, shape (N, d)\n",
    "        Data points\n",
    "    pi : np.array, shape (C,)\n",
    "        Mixture component weights \n",
    "    mu : np.array, shape (C, d)\n",
    "        Mixture component means\n",
    "    sigma : np.array, shape (C, d, d)\n",
    "        Mixture component covariance matrices\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gamma : np.array, shape (N, C)\n",
    "        Probabilities of clusters for objects\n",
    "    \"\"\"\n",
    "    \n",
    "    N = X.shape[0] # number of objects\n",
    "    C = pi.shape[0] # number of clusters\n",
    "    d = mu.shape[1] # dimension of each object\n",
    "    gamma = np.zeros((N, C)) # distribution q(T)\n",
    "\n",
    "    # NOTE: Before introducing the latent variable we have p(x, theta)\n",
    "    #       Then we introduce p(t|theta) = pi_c and p(x|t, theta) = N(x|mu_c, Sigma_c)\n",
    "    #       We are asked to find p(t|x, theta) \n",
    "    #       (which we set equal to q as this maximize the E step [i.e. minimize the the gap] due to KL)\n",
    "    #       Bayes rule states that\n",
    "    #       p(t|x, theta) = p(x|t, theta) p(t|theta) / p(x|theta)\n",
    "    #       gamma_c = p(t|x, theta)\n",
    "    #       Although we don't know p(x|theta), we know that sum gamma_c = 1, and we can use that to normalize\n",
    "    #       as p(x|theta) is independent of c\n",
    "        \n",
    "    for c in range(C):\n",
    "        gaussian = get_gaussian(X, mu[c, ...], sigma[c, ...])\n",
    "\n",
    "        # NOTE: This is the un-normalized gamma\n",
    "        gamma[:, c] = gaussian * pi[c]\n",
    "    \n",
    "    # NOTE: To normalize we realize that a point must belong to one cluster\n",
    "    #       Thus the gamma summed over c should equal to one\n",
    "    # We divide all columns in gamma with the same normalization constant\n",
    "    # https://stackoverflow.com/questions/1550130/cloning-row-or-column-vectors\n",
    "    norm = np.tile(gamma.sum(axis=1)[..., np.newaxis], (1,3))\n",
    "    norm = gamma.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Trick to avoid large numerical errors\n",
    "    max_exp = np.exp(-X.max())\n",
    "    gamma = (gamma*max_exp)/(norm*max_exp)\n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "grader.submit_e_step(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "In M-step we need to maximize $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)$ with respect to $\\theta$. In our model this means that we need to find optimal values of $\\pi$, $\\mu$, $\\Sigma$. To do so, you need to compute the derivatives and \n",
    "set them to zero. You should start by deriving formulas for $\\mu$ as it is the easiest part. Then move on to $\\Sigma$. Here it is crucial to optimize function w.r.t. to $\\Lambda = \\Sigma^{-1}$ and then inverse obtained result. Finaly, to compute $\\pi$, you will need <a href=\"https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf\">Lagrange Multipliers technique</a> to satisfy constraint $\\sum\\limits_{i=1}^{n}\\pi_i = 1$.\n",
    "\n",
    "<br>\n",
    "<b>Important note:</b> You will need to compute derivatives of scalars with respect to matrices. To refresh this technique from previous courses, see <a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\">Chapter 2 of The Matrix Cookbook</a>. For example, there you may find that $\\frac{\\partial}{\\partial A}\\log |A| = A^{-T}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task 2:</b> Implement M-step for GMM using template below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation\n",
    "\n",
    "We now want to maximize\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "= \n",
    "\\sum_{i=1}^N \\sum_{c=1}^C q(t_i = c) \\log p(t_i=c, x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "(see [derivation of the EM algorithm](https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/Fm3mY/expectation-maximization-algorithm) on how to obtain the $\\mathcal{L}$ term and [derivation of the M algorithm](https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/46DgL/m-step-details) on how to obatain the $\\mathbb{E}_{q(T)}$ term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that\n",
    "\n",
    "$$\n",
    "p(T, X \\mid \\theta) \n",
    "= \\prod\\limits_{i=1}^N p(t_i, x_i \\mid \\theta)\n",
    "= \\prod\\limits_{i=1}^N \\prod\\limits_{c=1}^C [\\pi_c \\mathcal{N}(x_i \\mid \\mu_c, \\Sigma_c)]^{T_{ic}}\n",
    "= \\prod\\limits_{i=1}^N \\prod\\limits_{c=1}^C \\left[\\pi_c\n",
    "\\frac{\\exp \n",
    "      \\left(-\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right)\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma }_c|}}\n",
    "      \\right]^{T_{ic}}\n",
    "$$\n",
    "\n",
    "so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log[p(T, X \\mid \\theta)]\n",
    "= \\log\\left[\\prod\\limits_{i=1}^N \\prod\\limits_{c=1}^C \\left[\n",
    "\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\n",
    "      \\exp \n",
    "      \\left(-\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right)\n",
    "      \\right]^{T_{ic}}\\right]\\\\\n",
    "= \\sum_{i=1}^N \\sum_{c=1}^C \\log\\left[\\left[\n",
    "\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\n",
    "      \\exp \n",
    "      \\left(-\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right)\n",
    "      \\right]^{T_{ic}}\\right]\\\\\n",
    "= \\sum_{i=1}^N \\sum_{c=1}^C \\log\\left[\\left[\n",
    "\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\n",
    "      \\exp \n",
    "      \\left(-\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right)\n",
    "      \\right]^{T_{ic}}\\right]\\\\\n",
    "= \\sum_{i=1}^N \\sum_{c=1}^C T_{ic}\\log\\left[\n",
    "\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\n",
    "      \\exp \n",
    "      \\left(-\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right)\n",
    "      \\right]\\\\\n",
    "= \\sum_{i=1}^N \\sum_{c=1}^C T_{ic}\\left[\n",
    "\\log\\left[\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\\right]\n",
    "      +\n",
    "      \\log\\left[\n",
    "      \\exp \n",
    "      \\left(-\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right)\n",
    "      \\right]\n",
    "      \\right]\\\\\n",
    "= \\sum_{i=1}^N \\sum_{c=1}^C T_{ic}\\left[\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "= \n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative w.r.t mean\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\mu}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "= \n",
    "\\nabla_{\\boldsymbol {\\mu}}\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "\\\\\n",
    "= \n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "      -\\frac{1}{2}\n",
    "            \\nabla_{\\boldsymbol {\\mu}}(\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use that\n",
    "\n",
    "$$\\partial (\\mathbf{A}\\mathbf{B}) = (\\partial \\mathbf{A})\\mathbf{B} + \\mathbf{A}(\\partial \\mathbf{B})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\mu}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "=\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "      -\\frac{1}{2}\n",
    "      \\left[\n",
    "            \\left[\n",
    "            \\nabla_{\\boldsymbol {\\mu}}(\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "            \\right]\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "            +\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "            \\left[\n",
    "            \\nabla_{\\boldsymbol {\\mu}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "            \\right]\n",
    "      \\right]      \n",
    "      \\right]\\\\\n",
    "=\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "      -\\frac{1}{2}\n",
    "      \\left[\n",
    "            \\left[\n",
    "            \\nabla_{\\boldsymbol {\\mu}}(\\mathbf{x} - \\boldsymbol{\\mu}_c)\n",
    "            \\right]^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "            +\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "            \\left(\n",
    "            \\left[\n",
    "            \\nabla_{\\boldsymbol {\\mu}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            \\right]\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "            +\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            \\left[\n",
    "            \\nabla_{\\boldsymbol {\\mu}}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)            \n",
    "            \\right)\n",
    "            \\right]\n",
    "      \\right]      \n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that $\\partial_{\\boldsymbol {\\mu}_{c=i}} \\boldsymbol{\\mu}_{c=j} = 1 \\iff i=j$, and $0$ elsewhere, and that $T_{ic}=1$ only if a point belongs to $c$, meaning that the sum over $C$ disappears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol {\\mu}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "=\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    "\\left[\n",
    "      -\\frac{1}{2}\n",
    "      \\left[\n",
    "            \\mathbb{I}^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "            +\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "            \\left(\n",
    "            \\mathbf {0}\n",
    "            +\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            \\mathbb{I}           \n",
    "            \\right)\n",
    "      \\right]      \n",
    "      \\right]\\\\\n",
    "=\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    "\\left[\n",
    "      -\\frac{1}{2}\n",
    "      \\left[\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "            +\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "      \\right]      \n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use that $\\boldsymbol{\\Sigma}_c$ is symmetric, which means that $(A\\mathbf{x})^T = \\mathbf{x}^TA^T = \\mathbf{x}^TA$. As the dot product between a matrix and a vector is a vector, we can add the row and the column vector together, and we get\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\mu}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "=\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "      -\\frac{1}{2}\n",
    "             2\\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\\\\\n",
    "=\n",
    "-\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)T_{ic}\n",
    "      \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the extremes if we set the derivative equal to $0$\n",
    "\n",
    "$$\n",
    "\\mathbf{0} = \\nabla_{\\boldsymbol {\\mu}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "\\\\\n",
    "-\\boldsymbol{\\Sigma}_c\\mathbf{0}\n",
    "=\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    "      \\boldsymbol{\\Sigma}_c\\boldsymbol{\\Sigma}_c^{-1}\n",
    "      (\\mathbf {x} - \\boldsymbol {\\mu}_c)\\\\\n",
    "\\mathbf{0}\n",
    "=\n",
    "\\sum_{i=1}^N\n",
    "q(t_i=c)\n",
    "      (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "\\\\\n",
    "\\sum_{i=1}^N\n",
    "q(t_i=c)\n",
    "\\boldsymbol {\\mu}_c\n",
    "=\n",
    "\\sum_{i=1}^N\n",
    "q(t_i=c)\n",
    "      \\mathbf {x}\n",
    "\\\\\n",
    "\\boldsymbol {\\mu}_c\n",
    "=\n",
    "\\frac{\n",
    "\\sum_{i=1}^N\n",
    "q(t_i=c)\n",
    "      \\mathbf {x}\n",
    "}{\n",
    "\\sum_{i=1}^N\n",
    "q(t_i=c)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative w.r.t probability\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\pi}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "= \n",
    "\\nabla_{\\boldsymbol {\\pi}}\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "\\\\\n",
    "= \n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\nabla_{\\boldsymbol {\\pi}}\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\\right)\n",
    "      -\\mathbf{0}\n",
    "      \\right]\n",
    "      \\\\\n",
    "= \n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\nabla_{\\boldsymbol {\\pi}}\n",
    "\\log\\pi_c\n",
    "      -\n",
    "\\nabla_{\\boldsymbol {\\pi}}\n",
    "\\log\\left(\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}\\right)\n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same arguments as when maximizing the mean, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol {\\pi}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "= \n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    "\\frac{\\mathbf{1}}{\\pi_c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize this under the constraint $\\sum_{c=1}^C \\pi_c = 1$. We can reformulate this to the Lagrange multiplier $\\sum_{c=1}^C \\pi_c - 1 = 0$. We have that\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\pi}} \\left(\\sum_{c=1}^C \\pi_c - 1\\right) = \\mathbf{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the maximum is obtained by solving\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    "\\frac{\\mathbf{1}}{\\pi_c}\n",
    "- \\sum_{i=1}^N = 0\n",
    "\\\\\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    "\\frac{\\mathbf{1}}{\\pi_c}\n",
    " = \\sum_{i=1}^N\n",
    "\\\\\n",
    "\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    " = N\\pi_c\n",
    "\\\\\n",
    "\\pi_c\n",
    "=\n",
    "\\frac{\\sum_{i=1}^N \n",
    "q(t_i=c)}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative w.r.t variance\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\Sigma^{-1}}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "= \n",
    "\\nabla_{\\boldsymbol {\\Sigma^{-1}}}\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} | \\boldsymbol {\\Sigma}_c|}}\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\\\\\n",
    "= \n",
    "\\nabla_{\\boldsymbol {\\Sigma^{-1}}}\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} }}\\right)\n",
    "      + \\log\\left(\\frac{1}{\\sqrt{|\\boldsymbol {\\Sigma}_c|}}\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use that $|A|^{-1} = |A^{-1}|$, we get\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\Sigma^{-1}}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "=\n",
    "\\nabla_{\\boldsymbol {\\Sigma^{-1}}}\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\n",
    "\\log\\left(\\frac{\\pi_c\n",
    "      }{\\sqrt{(2\\pi)^{k} }}\\right)\n",
    "      + \\frac{1}{2}\\log\\left(\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\\\\\n",
    "=\n",
    "\\sum_{i=1}^N \\sum_{c=1}^C \n",
    "q(t_i=c)T_{ic}\n",
    "\\left[\\mathbf{0}\n",
    "      + \\frac{1}{2}\\nabla_{\\boldsymbol {\\Sigma^{-1}}}\\log\\left(\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|\\right)\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "             \\nabla_{\\boldsymbol {\\Sigma^{-1}}}\\boldsymbol{\\Sigma}_c^{-1}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log\\left(\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|\\right)}{\\partial{\\Sigma}_c^{-1}}\n",
    "=\n",
    "\\frac{\\partial \\log\\left(\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|\\right)}{\\partial\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|}\n",
    "\\frac{\\partial \\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|}{\\partial{\\Sigma}_c^{-1}}\n",
    "$$\n",
    "\n",
    "Since $\\frac{\\partial \\left|\\mathbf{A}\\right|}{\\partial \\mathbf{A}} = \\left|\\mathbf{A}\\right|(\\mathbf{A}^{-1})^T$, this yields\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log\\left(\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|\\right)}{\\partial{\\Sigma}_c^{-1}}\n",
    "=\n",
    "\\frac{1}{\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|}\n",
    "\\left|\\boldsymbol {\\Sigma}_c^{-1}\\right|\\left(\\boldsymbol{\\Sigma}_c\\right)^T\n",
    "=\n",
    "\\boldsymbol{\\Sigma}_c\n",
    "$$\n",
    "\n",
    "as $\\boldsymbol{\\Sigma}_c$ is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the arguments from above we get\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol {\\Sigma^{-1}}}\n",
    "\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)\n",
    "=\\sum_{i=1}^N \n",
    "q(t_i=c)\n",
    "\\left[\\frac{1}{2}\\boldsymbol{\\Sigma}_c\n",
    "      -\\frac{1}{2}\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "\\\\\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\left[\\sum_{i=1}^N q(t_i=c)\\boldsymbol{\\Sigma}_c\n",
    "      -\n",
    "      \\sum_{i=1}^N q(t_i=c)\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equating this to $\\mathbf{0}$ in order to find the maximum gives\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\n",
    "\\left[\\sum_{i=1}^N q(t_i=c)\\boldsymbol{\\Sigma}_c\n",
    "      -\n",
    "      \\sum_{i=1}^N q(t_i=c)\n",
    "            (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "            (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "      \\right]\n",
    "      = \\mathbf{0}\n",
    "\\\\\n",
    "\\sum_{i=1}^N q(t_i=c)\\boldsymbol{\\Sigma}_c\n",
    "=\n",
    "\\sum_{i=1}^N q(t_i=c)\n",
    "      (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "      (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "\\\\\n",
    "\\boldsymbol{\\Sigma}_c\n",
    "=\n",
    "\\frac{\n",
    "\\sum_{i=1}^N q(t_i=c)\n",
    "      (\\mathbf{x} - \\boldsymbol{\\mu}_c)^{\\mathrm{T}}\n",
    "      (\\mathbf {x} - \\boldsymbol {\\mu}_c)\n",
    "}{\\sum_{i=1}^N q(t_i=c)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    \"\"\"\n",
    "    Performs M-step on GMM model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape (N, d)\n",
    "        Data points\n",
    "    gamma : np.array, shape (N, C)\n",
    "        The q(T) distribution\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pi : np.array, shape (C,)\n",
    "        Mixture component weights \n",
    "    mu : np.array, shape (C, d)\n",
    "        Mixture component means\n",
    "    sigma: np.array, shape (C, d, d)\n",
    "        Mixture component covariance matrices\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "\n",
    "    # Initialize\n",
    "    pi = np.zeros(C)\n",
    "    mu = np.zeros((C, d))\n",
    "    sigma = np.zeros((C, d, d))\n",
    "    \n",
    "    for c in range(C):\n",
    "        # Calculate the scalar datapoint sum of q for the current c\n",
    "        sum_N_q = gamma[:, c].sum()\n",
    "        # q contains one value per point\n",
    "        # We want to multiply these values elementwise to each column of X\n",
    "        # Therefore we copy the values of q to as many columns there is in X\n",
    "        gamma_c_mat = np.tile(gamma[:, c, np.newaxis], (1, d))\n",
    "        # We sum over the datapoints at axis 0\n",
    "        mu[c,:] = (gamma_c_mat*X).sum(axis=0)/ sum_N_q \n",
    "        # Note that sum_N = N\n",
    "        pi[c] = sum_N_q / N\n",
    "        # As both X and q has a value per point, we calculate the numinator using list comprehension\n",
    "        sigma[c,:] = np.sum([gamma[i, c] * np.outer(X[i] - mu[c], X[i] - mu[c]) for i in range(N)], axis=0) /\\\n",
    "                     sum_N_q\n",
    "\n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "grader.submit_m_step(pi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need some function to track convergence. We will use variational lower bound $\\mathcal{L}$ for this purpose. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code.\n",
    "\n",
    "<b>Task 3:</b> Implement a function that will compute $\\mathcal{L}$ using template below.\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{E}[z_{n, k}] (\\log \\pi_k + \\log \\mathcal{N}(x_n | \\mu_k, \\sigma_k)) - \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{E}[z_{n, k}] \\log \\mathbb{E}[z_{n, k}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: The new notation:\n",
    "$z_{n,k}$ is the binary random latent variable. For each \n",
    "$x_n$ we have \n",
    "$z_n$, and \n",
    "$z_n \\in \\{0,1\\}^K$  where \n",
    "$K$ is number of mixture components and \n",
    "$\\sum_{k=1}^K z_{n,k} = 1$. So \n",
    "$z_{n,k}=1$ if \n",
    "$x_n$ is from the\n",
    "$k$-th component of mixture. This means that $\\mathbb{E}[z_{n, k}]=q(t_i=c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def compute_vlb(X, pi, mu, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Computes the variational lower bound\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape (N, d)\n",
    "        Data points\n",
    "    gamma : np.array (N, C)\n",
    "        The q(T) distribution\n",
    "    pi : np.array, shape (C,)\n",
    "        Mixture component weights \n",
    "    mu : np.array, shape (C, d)\n",
    "        Mixture component means\n",
    "    sigma : np.array, shape (C, d, d)\n",
    "        Mixture component covariance matrices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        The loss\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "\n",
    "    eps = np.finfo(float).eps\n",
    "    loss = 0\n",
    "    for c in range(C):\n",
    "        # NOTE: The numerical precision of get_gaussian is too low when calculating for the whole X\n",
    "        #       Therefore we will use the scipy package instead\n",
    "        gaussian = multivariate_normal(mu[c, ...], sigma[c, ...])\n",
    "        for i in range(N):\n",
    "            loss += gamma[i, c]*(np.log(pi[c] + eps) + gaussian.logpdf(X[i,:]) - np.log(gamma[i, c] + eps))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu, sigma = pi0, mu0, sigma0\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "grader.submit_VLB(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have E step, M step and VLB, we can implement training loop. We will start at random values of $\\pi$, $\\mu$ and $\\Sigma$, train until $\\mathcal{L}$ stops changing and return the resulting points. We also know that EM algorithm sometimes stops at local optima. To avoid this we should restart algorithm multiple times from different starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L}_i-\\mathcal{L}_{i-1}}{\\mathcal{L}_{i-1}}| \\le \\text{rtol}$).\n",
    "\n",
    "Remember, that values of $\\pi$ that you generate must be non-negative and sum up to 1. Also, $\\Sigma$ matrices must be symmetric and positive semi-definite. If you don't know how to generate those matrices, you can use $\\Sigma=I$ as initialization.\n",
    "\n",
    "You will also sometimes get numerical errors because of component collapsing. The easiest way to deal with this problems is to simply restart the procedure.\n",
    "\n",
    "<b>Task 4:</b> Implement training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_EM(X, C, rtol=1e-3, max_iter=100, restarts=10):\n",
    "    \"\"\"\n",
    "    Starts with random initialization *restarts* times\n",
    "    Runs optimization until saturation with *rtol* reached\n",
    "    or *max_iter* iterations were made.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array, shape (N, d)\n",
    "        Data points\n",
    "    C : int\n",
    "        Number of clusters\n",
    "    rtol : float\n",
    "        The relative tolerance\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    restarts : int\n",
    "        Number of restarts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_loss : float\n",
    "        The best obtained loss\n",
    "    best_pi : np.array, shape (C,)\n",
    "        The corresponding best mixture component weights\n",
    "    best_mu : np.array, shape (C, d)\n",
    "        The corresponding best mixture component means\n",
    "    best_sigma : (C, d, d)\n",
    "        The corresponding best mixture component covariance matrices\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    best_loss = -np.inf\n",
    "    best_pi = None\n",
    "    best_mu = None\n",
    "    best_sigma = None\n",
    "    \n",
    "    for _ in tqdm_notebook(range(restarts), desc='restart'):\n",
    "        # Initialization\n",
    "        pi = np.array([1.0/C]*C)\n",
    "        mu = np.random.rand(C, d)\n",
    "        sigma_random = np.random.rand(C, d, d)\n",
    "        sigma = np.array([np.dot(mat, mat.transpose()) for mat in sigma_random])\n",
    "\n",
    "        try:\n",
    "            for _ in tqdm_notebook(range(max_iter), desc='iteration', leave=False):\n",
    "                gamma = E_step(X, pi, mu, sigma)\n",
    "                pi, mu, sigma = M_step(X, gamma)\n",
    "                try:\n",
    "                    loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "                except ValueError as e:\n",
    "                    print(f'Value error \"{e}\" encountered in calulation of loss')\n",
    "                    break\n",
    "                \n",
    "                if loss > best_loss:\n",
    "                    cur_rtol = np.abs(best_loss - loss) / np.min([np.abs(best_loss), np.abs(loss)])\n",
    "                        \n",
    "                    best_loss = loss\n",
    "                    best_pi = pi\n",
    "                    best_mu = mu\n",
    "                    best_sigma = sigma\n",
    "\n",
    "                    if cur_rtol < rtol:\n",
    "                        print(f'cur_rtol < rtol with loss={loss}')\n",
    "                        break\n",
    "                        \n",
    "            print(f'Reached max iterations with loss={loss}')\n",
    "                        \n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Singular matrix: components collapsed\")\n",
    "            pass\n",
    "\n",
    "    return best_loss, best_pi, best_mu, best_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3)\n",
    "grader.submit_EM(best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented all the steps correctly, your algorithm should converge in about 20 iterations. Let's plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using matrix $\\gamma$ computed on last E-step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and your token into variables below. You can generate the token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_EMAIL = ''\n",
    "STUDENT_TOKEN = ''\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
