{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This seminar:__ after you're done coding your own recurrent cells, it's time you learn how to train recurrent networks easily with Keras. We'll also learn some tricks on how to use keras layers and model. We also want you to note that this is a non-graded assignment, meaning you are not required to pass it for a certificate.\n",
    "\n",
    "Enough beatin' around the bush, let's get to the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part Of Speech Tagging\n",
    "\n",
    "<img src=dogatecat.png width=320>\n",
    "\n",
    "Unlike our previous experience with language modelling, this time around we learn the mapping between two different kinds of elements.\n",
    "\n",
    "This setting is common for a range of useful problems:\n",
    "* Speech Recognition - processing human voice into text\n",
    "* Part Of Speech Tagging - for morphology-aware search and as an auxuliary task for most NLP problems\n",
    "* Named Entity Recognition - for chat bots and web crawlers\n",
    "* Protein structure prediction - for bioinformatics\n",
    "\n",
    "Our current guest is part-of-speech tagging. As the name suggests, it's all about converting a sequence of words into a sequence of part-of-speech tags. We'll use a reduced tag set for simplicity:\n",
    "\n",
    "### POS-tags\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition\t(on, of, at, ...)\n",
    "- ADV - adverb\t(really, already, still, ...)\n",
    "- CONJ\t- conjunction\t(and, or, but, ...)\n",
    "- DET - determiner, article\t(the, a, some, ...)\n",
    "- NOUN\t- noun\t(year, home, costs, ...)\n",
    "- NUM - numeral\t(twenty-four, fourth, 1991, ...)\n",
    "- PRT -\tparticle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- .\t- punctuation marks\t(. , ;)\n",
    "- X\t- other\t(ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/michael/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/michael/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
    "\n",
    "data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data = train_test_split(data,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "def draw(sentence):\n",
    "    words,tags = zip(*sentence)\n",
    "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
    "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
    "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
    "    \n",
    "    \n",
    "draw(data[11])\n",
    "draw(data[10])\n",
    "draw(data[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vocabularies\n",
    "\n",
    "Just like before, we have to build a mapping from tokens to integer ids. This time around, our model operates on a word level, processing one word per RNN step. This means we'll have to deal with far larger vocabulary.\n",
    "\n",
    "Luckily for us, we only receive those words as input i.e. we don't have to predict them. This means we can have a large vocabulary for free by using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage = 0.92876\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for sentence in data:\n",
    "    words,tags = zip(*sentence)\n",
    "    word_counts.update(words)\n",
    "\n",
    "# NOTE: A list of the most common words\n",
    "all_words = ['#EOS#','#UNK#']+list(list(zip(*word_counts.most_common(10000)))[0])\n",
    "\n",
    "#let's measure what fraction of data words are in the dictionary\n",
    "print(\"Coverage = %.5f\"%(float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# NOTE: Looks like a normal dict could have done the work here\n",
    "word_to_id = defaultdict(lambda:1,{word:i for i,word in enumerate(all_words)})\n",
    "tag_to_id = {tag:i for i,tag in enumerate(all_tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert words and tags into fixed-size matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(lines,token_to_id,max_len=None,pad=0,dtype='int32',time_major=False):\n",
    "    \"\"\"Converts a list of names into rnn-digestable matrix with paddings added after the end\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len,lines))\n",
    "    matrix = np.empty([len(lines),max_len],dtype)\n",
    "    matrix.fill(pad)\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n",
    "        matrix[i,:len(line_ix)] = line_ix\n",
    "\n",
    "    return matrix.T if time_major else matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ids:\n",
      "[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n",
      "     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n",
      "     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n",
      "   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n",
      "     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n",
      "    12   10    2  861 5240   12    8 8936  121    1    4]\n",
      " [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n",
      "     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Tag ids:\n",
      "[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n",
      "   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n",
      "   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n",
      "   6  3  2 13  7]\n",
      " [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: zip(*sentence) splits the words and the tags (word1, word2), (tag1, tag2)\n",
    "#       The outermost zip concatenates the words and the tags for each sentence \n",
    "#       ((word1s1, word2s1), (word1s2, word2s2), ...), ((tag1s1, tag2s1), (tag1s2, tag2s2), ...)\n",
    "batch_words,batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n",
    "\n",
    "print(\"Word ids:\")\n",
    "print(to_matrix(batch_words,word_to_id))\n",
    "print(\"Tag ids:\")\n",
    "print(to_matrix(batch_tags,tag_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build model\n",
    "\n",
    "Unlike our previous lab, this time we'll focus on a high-level keras interface to recurrent neural networks. It is as simple as you can get with RNN, allbeit somewhat constraining for complex tasks like seq2seq.\n",
    "\n",
    "By default, all keras RNNs apply to a whole sequence of inputs and produce a sequence of hidden states `(return_sequences=True` or just the last hidden state `(return_sequences=False)`. All the recurrence is happening under the hood.\n",
    "\n",
    "At the top of our model we need to apply a Dense layer to each time-step independently. As of now, by default keras.layers.Dense would apply once to all time-steps concatenated. We use __keras.layers.TimeDistributed__ to modify Dense layer so that it would apply across both batch and time axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "\n",
    "# NOTE: The number of recurrences depends on the lenght of the max length of input sequence \n",
    "#      (the other elements are padded to 0 in the sequence)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.SimpleRNN(64,return_sequences=True))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training:__ in this case we don't want to prepare the whole training dataset in advance. The main cause is that the length of every batch depends on the maximum sentence length within the batch. This leaves us two options: use custom training code as in previous seminar or use generators.\n",
    "\n",
    "Keras models have a __`model.fit_generator`__ method that accepts a python generator yielding one batch at a time. But first we need to implement such generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "BATCH_SIZE=32\n",
    "def generate_batches(sentences,batch_size=BATCH_SIZE,max_len=None,pad=0):\n",
    "    assert isinstance(sentences,np.ndarray),\"Make sure sentences is q numpy array\"\n",
    "    \n",
    "    while True:\n",
    "        indices = np.random.permutation(np.arange(len(sentences)))\n",
    "        for start in range(0,len(indices)-1,batch_size):\n",
    "            batch_indices = indices[start:start+batch_size]\n",
    "            batch_words,batch_tags = [],[]\n",
    "            for sent in sentences[batch_indices]:\n",
    "                words,tags = zip(*sent)\n",
    "                batch_words.append(words)\n",
    "                batch_tags.append(tags)\n",
    "\n",
    "            batch_words = to_matrix(batch_words,word_to_id,max_len,pad)\n",
    "            batch_tags = to_matrix(batch_tags,tag_to_id,max_len,pad)\n",
    "\n",
    "            batch_tags_1hot = to_categorical(batch_tags,len(all_tags)).reshape(batch_tags.shape+(-1,))\n",
    "            yield batch_words,batch_tags_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Callbacks:__ Another thing we need is to measure model performance. The tricky part is not to count accuracy after sentence ends (on padding) and making sure we count all the validation data exactly once.\n",
    "\n",
    "While it isn't impossible to persuade Keras to do all of that, we may as well write our own callback that does that.\n",
    "Keras callbacks allow you to write a custom code to be ran once every epoch or every minibatch. We'll define one via LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_accuracy(model):\n",
    "    # NOTE: test_data is globally available\n",
    "    test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])\n",
    "    test_words,test_tags = to_matrix(test_words,word_to_id),to_matrix(test_tags,tag_to_id)\n",
    "\n",
    "    #predict tag probabilities of shape [batch,time,n_tags]\n",
    "    predicted_tag_probabilities = model.predict(test_words,verbose=1)\n",
    "    predicted_tags = predicted_tag_probabilities.argmax(axis=-1)\n",
    "\n",
    "    #compute accurary excluding padding\n",
    "    numerator = np.sum(np.logical_and((predicted_tags == test_tags),(test_words != 0)))\n",
    "    denominator = np.sum(test_words != 0)\n",
    "    return float(numerator)/denominator\n",
    "\n",
    "\n",
    "class EvaluateAccuracy(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\nMeasuring validation accuracy...\")\n",
    "        acc = compute_test_accuracy(self.model)\n",
    "        print(\"\\nValidation accuracy: %.5f\\n\"%acc)\n",
    "        sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1344/1343 [==============================] - 31s 23ms/step - loss: 0.2384\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 7s 455us/step\n",
      "\n",
      "Validation accuracy: 0.93892\n",
      "\n",
      "Epoch 2/5\n",
      "1344/1343 [==============================] - 30s 22ms/step - loss: 0.0579\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 7s 464us/step\n",
      "\n",
      "Validation accuracy: 0.94406\n",
      "\n",
      "Epoch 3/5\n",
      "1344/1343 [==============================] - 29s 22ms/step - loss: 0.0512\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 7s 454us/step\n",
      "\n",
      "Validation accuracy: 0.94618\n",
      "\n",
      "Epoch 4/5\n",
      "1344/1343 [==============================] - 30s 22ms/step - loss: 0.0463\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 7s 455us/step\n",
      "\n",
      "Validation accuracy: 0.94542\n",
      "\n",
      "Epoch 5/5\n",
      "1344/1343 [==============================] - 30s 22ms/step - loss: 0.0427\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 6s 451us/step\n",
      "\n",
      "Validation accuracy: 0.94538\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1ac447710>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure final accuracy on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14335/14335 [==============================] - 7s 460us/step\n",
      "Final accuracy: 0.94538\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"Final accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.94, \"Keras has gone on a rampage again, please contact course staff.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task I: getting all bidirectional\n",
    "\n",
    "Since we're analyzing a full sequence, it's legal for us to look into future data.\n",
    "\n",
    "A simple way to achieve that is to go both directions at once, making a __bidirectional RNN__.\n",
    "\n",
    "In Keras you can achieve that both manually (using two LSTMs and Concatenate) and by using __`keras.layers.Bidirectional`__. \n",
    "\n",
    "This one works just as `TimeDistributed` we saw before: you wrap it around a recurrent layer (SimpleRNN now and LSTM/GRU later) and it actually creates two layers under the hood.\n",
    "\n",
    "Your first task is to use such a layer our POS-tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a model that utilizes bidirectional SimpleRNN\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.SimpleRNN(64,return_sequences=True), merge_mode='concat'))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1344/1343 [==============================] - 45s 34ms/step - loss: 0.1869\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 722us/step\n",
      "\n",
      "Validation accuracy: 0.95643\n",
      "\n",
      "Epoch 2/5\n",
      "1344/1343 [==============================] - 45s 34ms/step - loss: 0.0418\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 694us/step\n",
      "\n",
      "Validation accuracy: 0.96082\n",
      "\n",
      "Epoch 3/5\n",
      "1344/1343 [==============================] - 45s 34ms/step - loss: 0.0347\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 11s 734us/step\n",
      "\n",
      "Validation accuracy: 0.96248\n",
      "\n",
      "Epoch 4/5\n",
      "1344/1343 [==============================] - 45s 34ms/step - loss: 0.0297\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 698us/step\n",
      "\n",
      "Validation accuracy: 0.96278\n",
      "\n",
      "Epoch 5/5\n",
      "1344/1343 [==============================] - 45s 34ms/step - loss: 0.0253\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 725us/step\n",
      "\n",
      "Validation accuracy: 0.96202\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1340f26a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14335/14335 [==============================] - 11s 750us/step\n",
      "\n",
      "Final accuracy: 0.96202\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task II: now go and improve it\n",
    "\n",
    "You guesses it. We're now gonna ask you to come up with a better network.\n",
    "\n",
    "Here's a few tips:\n",
    "\n",
    "* __Go beyond SimpleRNN__: there's `keras.layers.LSTM` and `keras.layers.GRU`\n",
    "  * If you want to use a custom recurrent Cell, read [this](https://keras.io/layers/recurrent/#rnn)\n",
    "  * You can also use 1D Convolutions (`keras.layers.Conv1D`). They are often as good as recurrent layers but with less overfitting.\n",
    "* __Stack more layers__: if there is a common motif to this course it's about stacking layers\n",
    "  * You can just add recurrent and 1dconv layers on top of one another and keras will understand it\n",
    "  * Just remember that bigger networks may need more epochs to train\n",
    "* __Gradient clipping__: If your training isn't as stable as you'd like, set `clipnorm` in your optimizer.\n",
    "  * Which is to say, it's a good idea to watch over your loss curve at each minibatch. Try tensorboard callback or something similar.\n",
    "* __Regularization__: you can apply dropouts as usuall but also in an RNN-specific way\n",
    "  * `keras.layers.Dropout` works inbetween RNN layers\n",
    "  * Recurrent layers also have `recurrent_dropout` parameter\n",
    "* __More words!__: You can obtain greater performance by expanding your model's input dictionary from 5000 to up to every single word!\n",
    "  * Just make sure your model doesn't overfit due to so many parameters.\n",
    "  * Combined with regularizers or pre-trained word-vectors this could be really good cuz right now our model is blind to >5% of words.\n",
    "* __The most important advice__: don't cram in everything at once!\n",
    "  * If you stuff in a lot of modiffications, some of them almost inevitably gonna be detrimental and you'll never know which of them are.\n",
    "  * Try to instead go in small iterations and record experiment results to guide further search.\n",
    "  \n",
    "There's some advanced stuff waiting at the end of the notebook.\n",
    "  \n",
    "Good hunting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "168/167 [==============================] - 8s 49ms/step - loss: 0.6709\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 670us/step\n",
      "\n",
      "Validation accuracy: 0.71215\n",
      "\n",
      "Epoch 2/5\n",
      "168/167 [==============================] - 8s 46ms/step - loss: 0.1608\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 715us/step\n",
      "\n",
      "Validation accuracy: 0.92591\n",
      "\n",
      "Epoch 3/5\n",
      "168/167 [==============================] - 8s 46ms/step - loss: 0.0573\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 731us/step\n",
      "\n",
      "Validation accuracy: 0.94957\n",
      "\n",
      "Epoch 4/5\n",
      "168/167 [==============================] - 8s 46ms/step - loss: 0.0398\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 695us/step\n",
      "\n",
      "Validation accuracy: 0.95398\n",
      "\n",
      "Epoch 5/5\n",
      "168/167 [==============================] - 8s 47ms/step - loss: 0.0341\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 10s 723us/step\n",
      "\n",
      "Validation accuracy: 0.95621\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff12c0b7898>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playing with batch size\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.SimpleRNN(64,return_sequences=True), merge_mode='concat'))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "batch_size = 256\n",
    "model.fit_generator(generate_batches(train_data, batch_size=batch_size),len(train_data)/batch_size,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** With an accuracy of 0.95621 it appeared that the model suffered from a higher batch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1344/1343 [==============================] - 142s 105ms/step - loss: 0.2493\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 28s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.95607\n",
      "\n",
      "Epoch 2/5\n",
      "1344/1343 [==============================] - 141s 105ms/step - loss: 0.0427\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 28s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96146\n",
      "\n",
      "Epoch 3/5\n",
      "1344/1343 [==============================] - 140s 104ms/step - loss: 0.0353\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 28s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96442\n",
      "\n",
      "Epoch 4/5\n",
      "1344/1343 [==============================] - 141s 105ms/step - loss: 0.0308\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 28s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96582\n",
      "\n",
      "Epoch 5/5\n",
      "1344/1343 [==============================] - 140s 105ms/step - loss: 0.0272\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 28s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96562\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1900d8860>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing LTSM\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.LSTM(64,return_sequences=True), merge_mode='concat'))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** This is marginally better with an accuracy of 0.96562, however, it was timeconsuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1344/1343 [==============================] - 118s 87ms/step - loss: 0.1955\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.95750\n",
      "\n",
      "Epoch 2/5\n",
      "1344/1343 [==============================] - 116s 87ms/step - loss: 0.0410\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96221\n",
      "\n",
      "Epoch 3/5\n",
      "1344/1343 [==============================] - 116s 86ms/step - loss: 0.0345\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96482\n",
      "\n",
      "Epoch 4/5\n",
      "1344/1343 [==============================] - 116s 86ms/step - loss: 0.0301\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96592\n",
      "\n",
      "Epoch 5/5\n",
      "1344/1343 [==============================] - 116s 87ms/step - loss: 0.0268\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96601\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee78109518>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GRU\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(64,return_sequences=True), merge_mode='concat'))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** This is marginally better with an accuracy of 0.96601, however, and was slightly faster than LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1344/1343 [==============================] - 117s 87ms/step - loss: 0.1745\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.95857\n",
      "\n",
      "Epoch 2/10\n",
      "1344/1343 [==============================] - 116s 86ms/step - loss: 0.0406\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96194\n",
      "\n",
      "Epoch 3/10\n",
      "1344/1343 [==============================] - 117s 87ms/step - loss: 0.0334\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 24s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96485\n",
      "\n",
      "Epoch 4/10\n",
      "1344/1343 [==============================] - 117s 87ms/step - loss: 0.0293\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 24s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96590\n",
      "\n",
      "Epoch 5/10\n",
      "1344/1343 [==============================] - 116s 87ms/step - loss: 0.0253\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96512\n",
      "\n",
      "Epoch 6/10\n",
      "1344/1343 [==============================] - 116s 86ms/step - loss: 0.0224\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96523\n",
      "\n",
      "Epoch 7/10\n",
      "1344/1343 [==============================] - 117s 87ms/step - loss: 0.0192\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96418\n",
      "\n",
      "Epoch 8/10\n",
      "1344/1343 [==============================] - 116s 86ms/step - loss: 0.0163\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96258\n",
      "\n",
      "Epoch 9/10\n",
      "1344/1343 [==============================] - 117s 87ms/step - loss: 0.0137\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96242\n",
      "\n",
      "Epoch 10/10\n",
      "1344/1343 [==============================] - 116s 86ms/step - loss: 0.0112\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 23s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96084\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7febc9641668>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing stereoided GRU\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(128,return_sequences=True), merge_mode='concat'))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Oh dear...this was both time-consuming, and it overfitted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1344/1343 [==============================] - 128s 95ms/step - loss: 0.2008\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.95653\n",
      "\n",
      "Epoch 2/10\n",
      "1344/1343 [==============================] - 126s 94ms/step - loss: 0.0438\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96239\n",
      "\n",
      "Epoch 3/10\n",
      "1344/1343 [==============================] - 126s 94ms/step - loss: 0.0371\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96457\n",
      "\n",
      "Epoch 4/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0329\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 25s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96583\n",
      "\n",
      "Epoch 5/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0297\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96645\n",
      "\n",
      "Epoch 6/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0271\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96639\n",
      "\n",
      "Epoch 7/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0252\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96651\n",
      "\n",
      "Epoch 8/10\n",
      "1344/1343 [==============================] - 126s 94ms/step - loss: 0.0235\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96648\n",
      "\n",
      "Epoch 9/10\n",
      "1344/1343 [==============================] - 126s 94ms/step - loss: 0.0222\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96624\n",
      "\n",
      "Epoch 10/10\n",
      "1344/1343 [==============================] - 128s 95ms/step - loss: 0.0206\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7febadc2c0b8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GRU with recurrence dropout\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Reaches highest validation, but there seem to be some overfitting towards the end (a bit hard to tell due to the dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1344/1343 [==============================] - 128s 95ms/step - loss: 0.2075\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.95699\n",
      "\n",
      "Epoch 2/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0450\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96238\n",
      "\n",
      "Epoch 3/10\n",
      "1344/1343 [==============================] - 127s 95ms/step - loss: 0.0385\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96427\n",
      "\n",
      "Epoch 4/10\n",
      "1344/1343 [==============================] - 127s 95ms/step - loss: 0.0347\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96598\n",
      "\n",
      "Epoch 5/10\n",
      "1344/1343 [==============================] - 128s 95ms/step - loss: 0.0315\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96654\n",
      "\n",
      "Epoch 6/10\n",
      "1344/1343 [==============================] - 127s 95ms/step - loss: 0.0293\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 25s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96659\n",
      "\n",
      "Epoch 7/10\n",
      "1344/1343 [==============================] - 127s 95ms/step - loss: 0.0273\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96693\n",
      "\n",
      "Epoch 8/10\n",
      "1344/1343 [==============================] - 128s 95ms/step - loss: 0.0258\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96629\n",
      "\n",
      "Epoch 9/10\n",
      "1344/1343 [==============================] - 128s 95ms/step - loss: 0.0241\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96694\n",
      "\n",
      "Epoch 10/10\n",
      "1344/1343 [==============================] - 127s 95ms/step - loss: 0.0228\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96638\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7febac122d68>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GRU with recurrence dropout and dense dropout\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.2))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Reaches highest validation of 0.96694, but drops towards the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1344/1343 [==============================] - 128s 95ms/step - loss: 0.2149\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.95460\n",
      "\n",
      "Epoch 2/10\n",
      "1344/1343 [==============================] - 126s 93ms/step - loss: 0.0505\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.95994\n",
      "\n",
      "Epoch 3/10\n",
      "1344/1343 [==============================] - 126s 94ms/step - loss: 0.0437\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96243\n",
      "\n",
      "Epoch 4/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0398\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96426\n",
      "\n",
      "Epoch 5/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0370\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96491\n",
      "\n",
      "Epoch 6/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0346\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96574\n",
      "\n",
      "Epoch 7/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0328\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96618\n",
      "\n",
      "Epoch 8/10\n",
      "1344/1343 [==============================] - 127s 94ms/step - loss: 0.0314\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96628\n",
      "\n",
      "Epoch 9/10\n",
      "1344/1343 [==============================] - 126s 94ms/step - loss: 0.0300\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96672\n",
      "\n",
      "Epoch 10/10\n",
      "1344/1343 [==============================] - 126s 94ms/step - loss: 0.0288\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 26s 2ms/step\n",
      "\n",
      "Validation accuracy: 0.96664\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feba7b1dcf8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GRU with more recurrence dropout and dense dropout\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.4,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.4))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Accuracy almost the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1344/1343 [==============================] - 240s 178ms/step - loss: 0.1797\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.95726\n",
      "\n",
      "Epoch 2/10\n",
      "1344/1343 [==============================] - 237s 176ms/step - loss: 0.0464\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96234\n",
      "\n",
      "Epoch 3/10\n",
      "1344/1343 [==============================] - 238s 177ms/step - loss: 0.0398\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96541\n",
      "\n",
      "Epoch 4/10\n",
      "1344/1343 [==============================] - 237s 177ms/step - loss: 0.0351\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96670\n",
      "\n",
      "Epoch 5/10\n",
      "1344/1343 [==============================] - 239s 178ms/step - loss: 0.0318\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96769\n",
      "\n",
      "Epoch 6/10\n",
      "1344/1343 [==============================] - 239s 178ms/step - loss: 0.0291\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96785\n",
      "\n",
      "Epoch 7/10\n",
      "1344/1343 [==============================] - 241s 179ms/step - loss: 0.0267\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96748\n",
      "\n",
      "Epoch 8/10\n",
      "1344/1343 [==============================] - 240s 179ms/step - loss: 0.0250\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96766\n",
      "\n",
      "Epoch 9/10\n",
      "1344/1343 [==============================] - 240s 179ms/step - loss: 0.0231\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96756\n",
      "\n",
      "Epoch 10/10\n",
      "1344/1343 [==============================] - 239s 178ms/step - loss: 0.0216\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 45s 3ms/step\n",
      "\n",
      "Validation accuracy: 0.96703\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe997b37400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack more GRU\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.2))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.2))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Best so far with accuracy of 0.967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1344/1343 [==============================] - 354s 263ms/step - loss: 0.1874\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.95765\n",
      "\n",
      "Epoch 2/10\n",
      "1344/1343 [==============================] - 350s 261ms/step - loss: 0.0497\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96304\n",
      "\n",
      "Epoch 3/10\n",
      "1344/1343 [==============================] - 350s 261ms/step - loss: 0.0423\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96493\n",
      "\n",
      "Epoch 4/10\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0374\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96537\n",
      "\n",
      "Epoch 5/10\n",
      "1344/1343 [==============================] - 353s 263ms/step - loss: 0.0336\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96738\n",
      "\n",
      "Epoch 6/10\n",
      "1344/1343 [==============================] - 351s 261ms/step - loss: 0.0308\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96729\n",
      "\n",
      "Epoch 7/10\n",
      "1344/1343 [==============================] - 350s 261ms/step - loss: 0.0287\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96762\n",
      "\n",
      "Epoch 8/10\n",
      "1344/1343 [==============================] - 351s 261ms/step - loss: 0.0267\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96740\n",
      "\n",
      "Epoch 9/10\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0248\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96716\n",
      "\n",
      "Epoch 10/10\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0235\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96734\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9940d7eb8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack more GRU\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.2))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.2))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.2))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Just a bit better than previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1344/1343 [==============================] - 357s 265ms/step - loss: 0.1779\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 65s 5ms/step\n",
      "\n",
      "Validation accuracy: 0.95476\n",
      "\n",
      "Epoch 2/20\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0540\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96151\n",
      "\n",
      "Epoch 3/20\n",
      "1344/1343 [==============================] - 351s 261ms/step - loss: 0.0455\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96383\n",
      "\n",
      "Epoch 4/20\n",
      "1344/1343 [==============================] - 353s 263ms/step - loss: 0.0400\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96579\n",
      "\n",
      "Epoch 5/20\n",
      "1344/1343 [==============================] - 353s 262ms/step - loss: 0.0367\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96680\n",
      "\n",
      "Epoch 6/20\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0341\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96724\n",
      "\n",
      "Epoch 7/20\n",
      "1344/1343 [==============================] - 353s 262ms/step - loss: 0.0319\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96798\n",
      "\n",
      "Epoch 8/20\n",
      "1344/1343 [==============================] - 353s 263ms/step - loss: 0.0298\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96737\n",
      "\n",
      "Epoch 9/20\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0282\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96723\n",
      "\n",
      "Epoch 10/20\n",
      "1344/1343 [==============================] - 351s 261ms/step - loss: 0.0269\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96772\n",
      "\n",
      "Epoch 11/20\n",
      "1344/1343 [==============================] - 354s 263ms/step - loss: 0.0253\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96687\n",
      "\n",
      "Epoch 12/20\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0242\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96703\n",
      "\n",
      "Epoch 13/20\n",
      "1344/1343 [==============================] - 354s 263ms/step - loss: 0.0229\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96668\n",
      "\n",
      "Epoch 14/20\n",
      "1344/1343 [==============================] - 353s 263ms/step - loss: 0.0220\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96684\n",
      "\n",
      "Epoch 15/20\n",
      "1344/1343 [==============================] - 353s 263ms/step - loss: 0.0210\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96696\n",
      "\n",
      "Epoch 16/20\n",
      "1344/1343 [==============================] - 354s 264ms/step - loss: 0.0200\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96634\n",
      "\n",
      "Epoch 17/20\n",
      "1344/1343 [==============================] - 353s 263ms/step - loss: 0.0194\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96661\n",
      "\n",
      "Epoch 18/20\n",
      "1344/1343 [==============================] - 352s 262ms/step - loss: 0.0184\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 65s 5ms/step\n",
      "\n",
      "Validation accuracy: 0.96663\n",
      "\n",
      "Epoch 19/20\n",
      "1344/1343 [==============================] - 353s 262ms/step - loss: 0.0176\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 65s 5ms/step\n",
      "\n",
      "Validation accuracy: 0.96607\n",
      "\n",
      "Epoch 20/20\n",
      "1344/1343 [==============================] - 354s 263ms/step - loss: 0.0170\n",
      "\n",
      "Measuring validation accuracy...\n",
      "14335/14335 [==============================] - 64s 4ms/step\n",
      "\n",
      "Validation accuracy: 0.96675\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe37bcebd68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last try: More nodes, harder dropout\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.Bidirectional(L.GRU(128,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.5))\n",
    "model.add(L.Bidirectional(L.GRU(128,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.5))\n",
    "model.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\n",
    "model.add(L.Dropout(0.2))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-fc8aad68f205>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-fc8aad68f205>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    **Conclusion**: Not that much of an improvement\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**Conclusion**: Not that much of an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14335/14335 [==============================] - 63s 4ms/step\n",
      "\n",
      "Final accuracy: 0.96675\n",
      "Just a few more iterations!\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
    "\n",
    "if acc >= 0.99:\n",
    "    print(\"Awesome! Sky was the limit and yet you scored even higher!\")\n",
    "elif acc >= 0.98:\n",
    "    print(\"Excellent! Whatever dark magic you used, it certainly did it's trick.\")\n",
    "elif acc >= 0.97:\n",
    "    print(\"Well done! If this was a graded assignment, you would have gotten a 100% score.\")\n",
    "elif acc > 0.96:\n",
    "    print(\"Just a few more iterations!\")\n",
    "else:\n",
    "    print(\"There seems to be something broken in the model. Unless you know what you're doing, try taking bidirectional RNN and adding one enhancement at a time to see where's the problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some advanced stuff\n",
    "Here there are a few more tips on how to improve training that are a bit trickier to impliment. We strongly suggest that you try them _after_ you've got a good initial model.\n",
    "* __Use pre-trained embeddings__: you can use pre-trained weights from [there](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) to kickstart your Embedding layer.\n",
    "  * Embedding layer has a matrix W (layer.W) which contains word embeddings for each word in the dictionary. You can just overwrite them with tf.assign.\n",
    "  * When using pre-trained embeddings, pay attention to the fact that model's dictionary is different from your own.\n",
    "  * You may want to switch trainable=False for embedding layer in first few epochs as in regular fine-tuning.  \n",
    "* __More efficient baching__: right now TF spends a lot of time iterating over \"0\"s\n",
    "  * This happens because batch is always padded to the length of a longest sentence\n",
    "  * You can speed things up by pre-generating batches of similar lengths and feeding it with randomly chosen pre-generated batch.\n",
    "  * This technically breaks the i.i.d. assumption, but it works unless you come up with some insane rnn architectures.\n",
    "* __Structured loss functions__: since we're tagging the whole sequence at once, we might as well train our network to do so.\n",
    "  * There's more than one way to do so, but we'd recommend starting with [Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n",
    "  * You could plug CRF as a loss function and still train by backprop. There's even some neat tensorflow [implementation](https://www.tensorflow.org/api_guides/python/contrib.crf) for you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
