{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to my first NN\n",
    "\n",
    "We are here following the pipeline suggested from the [assignment](https://www.coursera.org/learn/intro-to-deep-learning/peer/0AgYP/my1stnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from preprocessed_mnist import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Begin with logistic regression from the previous assignment to classify some number against others (e.g. zero vs nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `preprocessed_mnist` the data has already been:\n",
    "\n",
    "1. Been normalized (note that the images only have one channel)\n",
    "2. Split into train, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression separating zeros from non-zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "small_number = 1e-3\n",
    "n_iter = 10\n",
    "# Maybe not needed\n",
    "batch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaper(var):\n",
    "    \"\"\"\n",
    "    Reshapes a 3-d array to a 2-d array, collapsing the two last dimensions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var : array, shape (samples, image-rows, image-colums)\n",
    "        The variable to reshape\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    reshaped : array, shape (samples, image-rows, image-colums)\n",
    "        The reshaped variable    \n",
    "    \"\"\"\n",
    "    \n",
    "    reshaped = var.reshape(var.shape[0], var.shape[1]*var.shape[2])\n",
    "    \n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r = reshaper(X_train)\n",
    "X_val_r = reshaper(X_val)\n",
    "X_test_r = reshaper(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_ex = X_train_r.shape[0]\n",
    "n_features = X_train_r.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first dimension is None, as we would like to vary the number of input examples\n",
    "\n",
    "# NOTE: How can a network take in all training examples at once?\n",
    "#       When predicting one example, we are essential sending in a row-vector (1 x n-matrix)\n",
    "#       When training several examples, we are sending in several one-vectors (m x n-matix)\n",
    "#       The loss will still be a scalar as the input_y will be m x 1-dimensional, where we will take an inner product\n",
    "#       with predicted_y, which is also m x 1 dimensional\n",
    "\n",
    "input_X = tf.placeholder(\"float32\", shape=(None, n_features), name=\"input_x\")\n",
    "input_y = tf.placeholder(\"float32\", shape=(None, 1), name=\"input_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_and_b(rows, cols):\n",
    "    \"\"\"\n",
    "    Returns weights and biases based on the input dimensions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rows : int\n",
    "        Number of rows in the weights matrix and the bias matrix\n",
    "        This corresponds to training examples in the input layer\n",
    "    cols : int\n",
    "        Number of columns in the weights matrix\n",
    "        This corresponds to features in the input layer and number of nodes in the previous layer for hidden layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    W : Variable, shape (rows, cols)\n",
    "        The weights variable\n",
    "    b : Variable, shape (rows, 1)\n",
    "        The bias variable\n",
    "    \"\"\"\n",
    "     \n",
    "    # We initialize with random weights to break symmetry\n",
    "    W = tf.Variable(initial_value=np.random.randn(rows, cols)*small_number,\n",
    "                    name=\"weights\",\n",
    "                    dtype='float32')\n",
    "\n",
    "    b = tf.Variable(initial_value=np.random.randn(rows, 1)*small_number,\n",
    "                    name=\"bias\",\n",
    "                    dtype='float32')\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = get_w_and_b(n_training_ex, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model code\n",
    "\n",
    "# Compute a vector of predictions, resulting shape should be [input_X.shape[0],]\n",
    "# This is 1D, if you have extra dimensions, you can  get rid of them with tf.squeeze .\n",
    "# Don't forget the sigmoid.\n",
    "\n",
    "# predicted_y = <predicted probabilities for input_X>\n",
    "# NOTE: Predicted y will have the same number of rows as the number of input examples\n",
    "# NOTE: Squeezing gets rid of the extra \"bracket\" (that is the 1 in (dim, 1)). This is needed for the scaffold\n",
    "predicted_y = tf.squeeze(tf.nn.sigmoid(tf.matmul(input_X, weights) + b))\n",
    "\n",
    "# Loss. Should be a scalar number - average loss over all the objects\n",
    "# tf.reduce_mean is your friend here\n",
    "# loss = <logistic loss (scalar, mean over sample)>\n",
    "# NOTE: We are not using tf.matmul(input_y , tf.log(predicted_y)) as matmul requires tensors with rank > 1\n",
    "# NOTE: When optimizing, the 1/m factor when taking reduce_mean contra taking matmul will not change the \n",
    "#       location of the minima \n",
    "loss = tf.reduce_mean(- input_y * tf.log(predicted_y) - (1-input_y) * tf.log(1 - predicted_y))\n",
    "\n",
    "# See above for an example. tf.train.*Optimizer\n",
    "# optimizer = <optimizer that minimizes loss>\n",
    "# NOTE: No var_list here as we are optimizing predicted_y, which is a placeholder\n",
    "\n",
    "\n",
    "# TODO: Maybe need a var list\n",
    "optimizer = tf.train.MomentumOptimizer(0.01, 0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generalize it to multiclass logistic regression. Either try to remember the week 1 lectures or google it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of a weights vector you'll have to use a matrix with `shape=(features, classes)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* softmax (exp over sum of exps) can implemented manually or as `tf.nn.softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* probably better to use STOCHASTIC gradient descent (minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in which case sample should probably be shuffled (or use random subsamples on each iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (sigmoid) instead of softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You need to train both layers, not just output layer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do not initialize layers with zeros (due to symmetry effects). A gaussian noize with small sigma will do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In an ideal case this totals to 2 .dot's, 1 softmax and 1 sigmoid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
